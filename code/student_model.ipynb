{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Roberta-Base\n",
    "\n",
    "[[2459  146   96  271   19]\n",
    " [ 109  159   11   94   35]\n",
    " [  18    4  111    3    1]\n",
    " [ 228   92   21 1621  138]\n",
    " [  19   27    3   91  560]]\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.87      0.82      0.84      2991\n",
    "           1       0.37      0.39      0.38       408\n",
    "           2       0.46      0.81      0.59       137\n",
    "           3       0.78      0.77      0.78      2100\n",
    "           4       0.74      0.80      0.77       700\n",
    "\n",
    "   micro avg       0.77      0.77      0.77      6336\n",
    "   macro avg       0.64      0.72      0.67      6336\n",
    "weighted avg       0.78      0.77      0.78      6336\n",
    "\n",
    "0.660511585369704\n",
    "0.7158294392523363 0.636353124559319\n",
    "\n",
    "Bert - BASE\n",
    "[[2463  157   90  252   29]\n",
    " [ 105  167   12   87   37]\n",
    " [  20    1  111    4    1]\n",
    " [ 268   87   17 1594  134]\n",
    " [  24   24    5   97  550]]\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.86      0.82      0.84      2991\n",
    "           1       0.38      0.41      0.40       408\n",
    "           2       0.47      0.81      0.60       137\n",
    "           3       0.78      0.76      0.77      2100\n",
    "           4       0.73      0.79      0.76       700\n",
    "\n",
    "   micro avg       0.77      0.77      0.77      6336\n",
    "   macro avg       0.65      0.72      0.67      6336\n",
    "weighted avg       0.78      0.77      0.77      6336\n",
    "\n",
    "0.6538214991499939\n",
    "0.712248198794295 0.6382049692979974\n",
    "\n",
    "Electra-BASE\n",
    "\n",
    "[[2456  104   62  334   35]\n",
    " [ 134  105   12  106   51]\n",
    " [  50    1   75    9    2]\n",
    " [ 295   62   27 1564  152]\n",
    " [  23   10    8  107  552]]\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.83      0.82      0.83      2991\n",
    "           1       0.37      0.26      0.30       408\n",
    "           2       0.41      0.55      0.47       137\n",
    "           3       0.74      0.74      0.74      2100\n",
    "           4       0.70      0.79      0.74       700\n",
    "\n",
    "   micro avg       0.75      0.75      0.75      6336\n",
    "   macro avg       0.61      0.63      0.62      6336\n",
    "weighted avg       0.75      0.75      0.75      6336\n",
    "\n",
    "0.6165488812009696\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score, matthews_corrcoef\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, train_test_split\n",
    "import sys\n",
    "import logging\n",
    "import os\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions  - Evaluating the model\n",
    "def save_confusion_matrix(y_true, y_pred, output_dir, file_name):\n",
    "    \"\"\"\n",
    "    Stores the confusion matrix in the file path\n",
    "    \"\"\"\n",
    "    labels = [\n",
    "        'None', 'KpEverTgthr', 'GetStudRelate', 'Restat',\n",
    "        'Revoic', 'PrsAcc', 'PrsRsn'\n",
    "    ]\n",
    "    conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "    svm = sns.heatmap(\n",
    "        conf_matrix/conf_matrix.sum(axis=1)[:, None],\n",
    "        cmap='Blues',\n",
    "        xticklabels=labels,\n",
    "        yticklabels=labels,\n",
    "        annot=True,\n",
    "    )\n",
    "    figure = svm.get_figure()\n",
    "    figure.savefig(output_dir + \"/\" + file_name + \".png\", dpi=600)\n",
    "    figure.clear()\n",
    "\n",
    "\n",
    "def custom_micro_f1_score(true_labels, pred_labels, label_count=7):\n",
    "    \"\"\"\n",
    "    This computes a custom micro f1 score ignoring\n",
    "    the predictions associated with label None (0)\n",
    "\n",
    "    This code still has to undergo some testing\n",
    "    \"\"\"\n",
    "\n",
    "    none_label = 0\n",
    "\n",
    "    ref_count = sum([1 for true in true_labels if true != none_label])\n",
    "    pred_count = sum([1 for pred in pred_labels if pred != none_label])\n",
    "    match_count = sum([1 for true, pred in zip(true_labels, pred_labels)\n",
    "                       if true == pred and true != none_label])\n",
    "\n",
    "    micro_precision = match_count / pred_count if pred_count != 0 else 0\n",
    "    micro_recall = match_count / ref_count if ref_count != 0 else 0\n",
    "    micro_f1 = 2/(1/micro_precision + 1/micro_recall) \\\n",
    "        if micro_precision != 0 and micro_recall != 0 else 0\n",
    "\n",
    "    return(micro_precision, micro_recall, micro_f1)\n",
    "\n",
    "\n",
    "def custom_macro_f1_score(true_labels, pred_labels, label_count=5):\n",
    "    \"\"\"\n",
    "    This computes a custom macro f1 score ignoring\n",
    "    in its entirety, the predictions associated\n",
    "    with the label None (0)\n",
    "\n",
    "\n",
    "    This code still has to undergo some testing\n",
    "    \"\"\"\n",
    "\n",
    "    none_label = 0\n",
    "\n",
    "    stats = defaultdict(Counter)\n",
    "\n",
    "    for true_label, pred_label in zip(true_labels, pred_labels):\n",
    "\n",
    "        stats[true_label]['tp+fn'] += 1\n",
    "\n",
    "        if true_label == pred_label:\n",
    "            stats[true_label]['tp'] += 1\n",
    "\n",
    "        stats[pred_label]['tp+fp'] += 1\n",
    "\n",
    "    for label in set(pred_labels):\n",
    "        if stats[label]['tp+fp'] != 0:\n",
    "            stats[label]['precision'] = \\\n",
    "                stats[label]['tp']/stats[label]['tp+fp']\n",
    "        if stats[label]['tp+fn'] != 0:\n",
    "            stats[label]['recall'] = stats[label]['tp']/stats[label]['tp+fn']\n",
    "        if stats[label]['precision'] != 0 and stats[label]['recall'] != 0:\n",
    "            stats[label]['f1'] = \\\n",
    "                2.0/(1/stats[label]['precision'] + 1/stats[label]['recall'])\n",
    "\n",
    "    macro_precision = sum(\n",
    "        [stats[lbl]['precision'] for lbl in stats.keys() if lbl != none_label]\n",
    "    )/(label_count - 1)\n",
    "\n",
    "    macro_recall = sum(\n",
    "        [stats[lbl]['recall'] for lbl in stats.keys() if lbl != none_label]\n",
    "    )/(label_count - 1)\n",
    "\n",
    "    macro_f1 = 2/(1/macro_precision + 1/macro_recall) \\\n",
    "        if macro_precision != 0 and macro_recall != 0 else 0\n",
    "\n",
    "    return macro_f1, stats\n",
    "\n",
    "\n",
    "class TalkBackDataset(torch.utils.data.Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data,\n",
    "        tokenizer,\n",
    "        max_seq_length\n",
    "    ):\n",
    "\n",
    "        input_examples = [\n",
    "            (\n",
    "                '' if datum[1].text_a is np.nan else datum[1].text_a.lower(),\n",
    "                '' if datum[1].text_b is np.nan else datum[1].text_b.lower()\n",
    "            ) for datum in data.iterrows()\n",
    "        ]\n",
    "        self.examples = tokenizer.batch_encode_plus(\n",
    "            batch_text_or_text_pairs=input_examples,\n",
    "            max_length=max_seq_length,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        self.labels = [\n",
    "            torch.tensor(int(label), dtype=torch.long) for label in data.labels\n",
    "        ]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples['input_ids'])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "            Use one if the other fails\n",
    "        \"\"\"\n",
    "        answer = {}\n",
    "        for key in self.examples:\n",
    "            answer[key] = self.examples[key][index]\n",
    "\n",
    "        answer['label'] = self.labels[index]\n",
    "\n",
    "        return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache() \n",
    "import gc\n",
    "gc.collect() \n",
    "from keras import backend as K\n",
    "K.clear_session()\n",
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_train_valid_testsets(tokenizer, max_seq_length=256, seed=1010):\n",
    "    '''\n",
    "    Arguments:\n",
    "        tokenizer - BERT tokenizer\n",
    "        max_seq_length - Maximum sequence length\n",
    "        seed - Random seed\n",
    "\n",
    "    Returns:\n",
    "        train_df - Training set (Pandas dataframe)\n",
    "        valid_df - Validation set (Pandas dataframe)\n",
    "        test_df - Testing set (Pandas dataframe)\n",
    "    '''\n",
    "    '''\n",
    "    the data in TSV format.\n",
    "    Contains three colums:\n",
    "        text_a (previous student sentence);\n",
    "        text_b (teacher sentence);\n",
    "        labels (category or TalkMove label)\n",
    "    '''\n",
    "    train_data = pd.read_csv(\n",
    "        '../data/train_student.tsv', sep='\\t'\n",
    "    )\n",
    "    \n",
    "    valid_data = pd.read_csv(\n",
    "        '../data/valid_student.tsv', sep='\\t'\n",
    "    )\n",
    "    \n",
    "    test_data = pd.read_csv(\n",
    "        '../data/test_student.tsv', sep='\\t'\n",
    "    )\n",
    "\n",
    "\n",
    "    X_train, X_test, y_train, y_test =  train_data, test_data, train_data.labels, test_data.labels\n",
    "    X_valid, y_valid = valid_data, valid_data.labels\n",
    "    \n",
    "    train_df = X_train.replace(np.nan, '', regex=True)\n",
    "    train_df = TalkBackDataset(train_df, tokenizer, max_seq_length)\n",
    "    \n",
    "    valid_df = X_valid.replace(np.nan, '', regex=True)\n",
    "    valid_df = TalkBackDataset(valid_df, tokenizer, max_seq_length)\n",
    "    \n",
    "    test_df = X_test.replace(np.nan, '', regex=True)\n",
    "    test_df = TalkBackDataset(test_df, tokenizer, max_seq_length)\n",
    "\n",
    "    return(train_df, valid_df, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    '''\n",
    "    Arguments:\n",
    "        pred - predictions\n",
    "\n",
    "    Returns:\n",
    "        accuracy, Micro F1, Macro F1 and MCC scoresoutput_attentions=True,  \n",
    "\n",
    "    '''\n",
    "    true_labels = pred.label_ids\n",
    "    #print(type(pred), pred.predictions.shape, pred.predictions)\n",
    "    pred_labels = pred.predictions.argmax(-1)\n",
    "    _, _, mif1 = custom_micro_f1_score(true_labels, pred_labels)\n",
    "    maf1, _ = custom_macro_f1_score(true_labels, pred_labels)\n",
    "    acc = accuracy_score(true_labels, pred_labels)\n",
    "    matthew_corr = matthews_corrcoef(true_labels, pred_labels)\n",
    "\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'micro_f1': mif1,\n",
    "        'macro_f1': maf1,\n",
    "        'matthew_corr_coeff': matthew_corr\n",
    "    }\n",
    "\n",
    "\n",
    "def test_compute_metrics(pred, model_store):\n",
    "    _, _, mif1 = custom_micro_f1_score(true_labels, pred_labels)\n",
    "    maf1, _ = custom_macro_f1_score(true_labels, pred_labels)\n",
    "\n",
    "    print(confusion_matrix(true_labels, pred_labels))\n",
    "    #print(save_confusion_matrix(true_labels, pred_labels, model_store, \"unseen\"))\n",
    "    print(classification_report(true_labels, pred_labels))\n",
    "    print(matthews_corrcoef(true_labels, pred_labels))\n",
    "    return mif1, maf1 \n",
    "\n",
    "\n",
    "#Command line agrument - Folder name where the model outputs can be stored\n",
    "folder_name =  '../../../baseline_public/'\n",
    "#Command line argument  - Random seed\n",
    "seed = 1022\n",
    "model_store = folder_name + \"/model_storage/\"\n",
    "\n",
    "#if the model storage folder does not exist, create it\n",
    "if not os.path.exists(model_store):\n",
    "    os.makedirs(model_store)\n",
    "\n",
    "#Log the process to help with debugging. All the logs will be saved in the corresponding folder\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "transformers_logger = logging.getLogger(\"transformers-\" + str(seed))\n",
    "transformers_logger.setLevel(logging.INFO)\n",
    "\n",
    "'''\n",
    "Arguments for model training Including:\n",
    "    output_dir - Store the outputs of the checkpoints as well as the best model\n",
    "    learning_rate - learning learning_rate\n",
    "    num_train_epochs - Number of training epochs (Tuning)\n",
    "    per_device_train_batch_size - Training batch size\n",
    "    per_device_eval_batch_size - validation batch size\n",
    "    warmup_steps - Number of warmup steps\n",
    "        Warmup steps are just a few updates with low learning rate before / at the beginning of training.\n",
    "        After this warmup, you use the regular learning rate (schedule) to train your model to convergence.\n",
    "        The idea that this helps your network to slowly adapt to the data intuitively makes sense.\n",
    "        However, theoretically, the main reason for warmup steps is to allow adaptive optimisers (e.g. Adam, RMSProp, ...)\n",
    "        to compute correct statistics of the gradients. Therefore, a warmup period makes little sense when training with plain SGD.\n",
    "    overwrite_output_dir - Overwrite the output directory\n",
    "    fp16 - 16 point precision - Speed up the process\n",
    "    save_steps - save checkpoints to the model output folder\n",
    "    evaluation_strategy - Evaluation is done at the end of each epoch.\n",
    "    logging_dir - Folder to save the logs\n",
    "'''\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=model_store,\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=4,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    warmup_steps=1000,\n",
    "    overwrite_output_dir=True,\n",
    "    fp16=False,\n",
    "    seed=seed,\n",
    "    save_steps=30000,\n",
    "    evaluation_strategy='epoch',\n",
    "    logging_dir='./logs',\n",
    "    eval_accumulation_steps=1\n",
    ")\n",
    "\n",
    "\n",
    "#Pretrained model to download and use including the number of labels\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    'google/electra-small-discriminator', num_labels=5, output_hidden_states=True\n",
    ")\n",
    "\n",
    "#Select the tokenizer to use\n",
    "tokenizer = AutoTokenizer.from_pretrained('google/electra-small-discriminator')\n",
    "\n",
    "#Fetch the data\n",
    "train_df, valid_df, test_df = fetch_train_valid_testsets(tokenizer)\n",
    "print(len(train_df), len(valid_df), len(test_df))\n",
    "\n",
    "#Set up the DNN model trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_df,\n",
    "    eval_dataset=valid_df,\n",
    "    compute_metrics=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train and evaluate the model\n",
    "trainer.train()\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('../../baseline_public/'+'student_electra_base.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_split = np.array_split(test_df, 20)\n",
    "pred_labels = []\n",
    "true_labels = []\n",
    "for item in val_split:\n",
    "    val = trainer.predict(test_dataset=item)\n",
    "    true_labels.extend([id for id in val.label_ids])\n",
    "    pred_labels.extend([id for id in val.predictions[0].argmax(-1)])\n",
    "mif1, maf1 = test_compute_metrics(pred_labels, true_labels)\n",
    "print(mif1, maf1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
