{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score, matthews_corrcoef\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, train_test_split\n",
    "import sys\n",
    "import logging\n",
    "import os\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions  - Evaluating the model\n",
    "\n",
    "def save_confusion_matrix(y_true, y_pred, output_dir, file_name):\n",
    "    \"\"\"\n",
    "    Stores the confusion matrix in the file path\n",
    "    \"\"\"\n",
    "    labels = [\n",
    "        'None', 'KpEverTgthr', 'GetStudRelate', 'Restat',\n",
    "        'Revoic', 'PrsAcc', 'PrsRsn'\n",
    "    ]\n",
    "    conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "    svm = sns.heatmap(\n",
    "        conf_matrix/conf_matrix.sum(axis=1)[:, None],\n",
    "        cmap='Blues',\n",
    "        xticklabels=labels,\n",
    "        yticklabels=labels,\n",
    "        annot=True,\n",
    "    )\n",
    "    figure = svm.get_figure()\n",
    "    figure.savefig(output_dir + \"/\" + file_name + \".png\", dpi=600)\n",
    "    figure.clear()\n",
    "\n",
    "\n",
    "def custom_micro_f1_score(true_labels, pred_labels, label_count=7):\n",
    "    \"\"\"\n",
    "    This computes a custom micro f1 score ignoring\n",
    "    the predictions associated with label None (0)\n",
    "\n",
    "    This code still has to undergo some testing\n",
    "    \"\"\"\n",
    "\n",
    "    none_label = 0\n",
    "\n",
    "    ref_count = sum([1 for true in true_labels if true != none_label])\n",
    "    pred_count = sum([1 for pred in pred_labels if pred != none_label])\n",
    "    match_count = sum([1 for true, pred in zip(true_labels, pred_labels)\n",
    "                       if true == pred and true != none_label])\n",
    "\n",
    "    micro_precision = match_count / pred_count if pred_count != 0 else 0\n",
    "    micro_recall = match_count / ref_count if ref_count != 0 else 0\n",
    "    micro_f1 = 2/(1/micro_precision + 1/micro_recall) \\\n",
    "        if micro_precision != 0 and micro_recall != 0 else 0\n",
    "\n",
    "    return(micro_precision, micro_recall, micro_f1)\n",
    "\n",
    "\n",
    "def custom_macro_f1_score(true_labels, pred_labels, label_count=7):\n",
    "    \"\"\"\n",
    "    This computes a custom macro f1 score ignoring\n",
    "    in its entirety, the predictions associated\n",
    "    with the label None (0)\n",
    "\n",
    "\n",
    "    This code still has to undergo some testing\n",
    "    \"\"\"\n",
    "\n",
    "    none_label = 0\n",
    "\n",
    "    stats = defaultdict(Counter)\n",
    "\n",
    "    for true_label, pred_label in zip(true_labels, pred_labels):\n",
    "\n",
    "        stats[true_label]['tp+fn'] += 1\n",
    "\n",
    "        if true_label == pred_label:\n",
    "            stats[true_label]['tp'] += 1\n",
    "\n",
    "        stats[pred_label]['tp+fp'] += 1\n",
    "\n",
    "    for label in set(pred_labels):\n",
    "        if stats[label]['tp+fp'] != 0:\n",
    "            stats[label]['precision'] = \\\n",
    "                stats[label]['tp']/stats[label]['tp+fp']\n",
    "        if stats[label]['tp+fn'] != 0:\n",
    "            stats[label]['recall'] = stats[label]['tp']/stats[label]['tp+fn']\n",
    "        if stats[label]['precision'] != 0 and stats[label]['recall'] != 0:\n",
    "            stats[label]['f1'] = \\\n",
    "                2.0/(1/stats[label]['precision'] + 1/stats[label]['recall'])\n",
    "\n",
    "    macro_precision = sum(\n",
    "        [stats[lbl]['precision'] for lbl in stats.keys() if lbl != none_label]\n",
    "    )/(label_count - 1)roberta-large\n",
    "\n",
    "    macro_recall = sum(\n",
    "        [stats[lbl]['recall'] for lbl in stats.keys() if lbl != none_label]\n",
    "    )/(label_count - 1)\n",
    "\n",
    "    macro_f1 = 2/(1/macro_precision + 1/macro_recall) \\\n",
    "        if macro_precision != 0 and macro_recall != 0 else 0\n",
    "\n",
    "    return macro_f1, stats\n",
    "\n",
    "\n",
    "class TalkBackDataset(torch.utils.data.Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data,\n",
    "        tokenizer,\n",
    "        max_seq_length\n",
    "    ):\n",
    "\n",
    "        input_examples = [\n",
    "            (\n",
    "                '' if datum[1].text_a is np.nan else datum[1].text_a.lower(),\n",
    "                '' if datum[1].text_b is np.nan else datum[1].text_b.lower()\n",
    "            ) for datum in data.iterrows()\n",
    "        ]\n",
    "        self.examples = tokenizer.batch_encode_plus(\n",
    "            batch_text_or_text_pairs=input_examples,\n",
    "            max_length=max_seq_length,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        self.labels = [\n",
    "            torch.tensor(int(label), dtype=torch.long) for label in data.labels\n",
    "        ]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples['input_ids'])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "            Use one if the other fails\n",
    "        \"\"\"\n",
    "        answer = {}\n",
    "        for key in self.examples:\n",
    "            answer[key] = self.examples[key][index]\n",
    "\n",
    "        answer['label'] = self.labels[index]\n",
    "\n",
    "        return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache() \n",
    "import gc\n",
    "gc.collect() \n",
    "from keras import backend as K\n",
    "K.clear_session()\n",
    "\n",
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_train_valid_testsets(tokenizer, max_seq_length=256, seed=1010):\n",
    "    '''\n",
    "    Arguments:\n",
    "        tokenizer - BERT tokenizer\n",
    "        max_seq_length - Maximum sequence length\n",
    "        seed - Random seed\n",
    "\n",
    "    Returns:\n",
    "        train_df - Training set (Pandas dataframe)\n",
    "        valid_df - Validation set (Pandas dataframe)\n",
    "        test_df - Testing set (Pandas dataframe)\n",
    "    '''\n",
    "    '''\n",
    "    the data in TSV format.\n",
    "    Contains three colums:\n",
    "        text_a (previous student sentence);\n",
    "        text_b (teacher sentence);\n",
    "        labels (category or TalkMove label)\n",
    "    '''\n",
    "    train_data = pd.read_csv(\n",
    "        '../data/train_teacher.tsv', sep='\\t'\n",
    "    )\n",
    "    \n",
    "    valid_data = pd.read_csv(\n",
    "        '../data/valid_teacher.tsv', sep='\\t'\n",
    "    )\n",
    "    \n",
    "    test_data = pd.read_csv(\n",
    "        '../data/test_teacher.tsv', sep='\\t'\n",
    "    )\n",
    "\n",
    "\n",
    "    X_train, X_test, y_train, y_test =  train_data, test_data, train_data.labels, test_data.labels\n",
    "    X_valid, y_valid = valid_data, valid_data.labels\n",
    "    \n",
    "    train_df = X_train.replace(np.nan, '', regex=True)\n",
    "    train_df = TalkBackDataset(train_df, tokenizer, max_seq_length)\n",
    "    \n",
    "    valid_df = X_valid.replace(np.nan, '', regex=True)\n",
    "    valid_df = TalkBackDataset(valid_df, tokenizer, max_seq_length)\n",
    "    \n",
    "    test_df = X_test.replace(np.nan, '', regex=True)\n",
    "    test_df = TalkBackDataset(test_df, tokenizer, max_seq_length)\n",
    "\n",
    "    return(train_df, valid_df, test_df)\n",
    "\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    '''\n",
    "    Arguments:\n",
    "        pred - predictions\n",
    "\n",
    "    Returns:\n",
    "        accuracy, Micro F1, Macro F1 and MCC scoresoutput_attentions=True,  \n",
    "\n",
    "    '''\n",
    "    true_labels = pred.label_ids\n",
    "    #print(type(pred), pred.predictions.shape, pred.predictions)\n",
    "    pred_labels = pred.predictions.argmax(-1)\n",
    "    _, _, mif1 = custom_micro_f1_score(true_labels, pred_labels)\n",
    "    maf1, _ = custom_macro_f1_score(true_labels, pred_labels)\n",
    "    acc = accuracy_score(true_labels, pred_labels)\n",
    "    matthew_corr = matthews_corrcoef(true_labels, pred_labels)\n",
    "\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'micro_f1': mif1,\n",
    "        'macro_f1': maf1,\n",
    "        'matthew_corr_coeff': matthew_corr\n",
    "    }\n",
    "\n",
    "\n",
    "def test_compute_metrics(pred, model_store):\n",
    "    _, _, mif1 = custom_micro_f1_score(true_labels, pred_labels)\n",
    "    maf1, _ = custom_macro_f1_score(true_labels, pred_labels)\n",
    "\n",
    "    print(confusion_matrix(true_labels, pred_labels))\n",
    "    #print(save_confusion_matrix(true_labels, pred_labels, model_store, \"unseen\"))\n",
    "    print(classification_report(true_labels, pred_labels))\n",
    "    print(matthews_corrcoef(true_labels, pred_labels))\n",
    "    return mif1, maf1\n",
    "\n",
    "\n",
    "#Command line agrument - Folder name where the model outputs can be stored\n",
    "folder_name = '../../../baseline_public/'\n",
    "\n",
    "#Command line argument  - Random seed\n",
    "seed = 1022\n",
    "model_store = folder_name + \"/model_storage/\"\n",
    "\n",
    "#if the model storage folder does not exist, create it\n",
    "if not os.path.exists(model_store):\n",
    "    os.makedirs(model_store)\n",
    "\n",
    "#Log the process to help with debugging. All the logs will be saved in the corresponding folder\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "transformers_logger = logging.getLogger(\"transformers-\" + str(seed))\n",
    "transformers_logger.setLevel(logging.INFO)\n",
    "\n",
    "'''\n",
    "Arguments for model training Including:\n",
    "    output_dir - Store the outputs of the checkpoints as well as the best model\n",
    "    learning_rate - learning learning_rate\n",
    "    num_train_epochs - Number of training epochs (Tuning)\n",
    "    per_device_train_batch_size - Training batch size\n",
    "    per_device_eval_batch_size - validation batch size\n",
    "    warmup_steps - Number of warmup steps\n",
    "        Warmup steps are just a few updates with low learning rate before / at the beginning of training.\n",
    "        After this warmup, you use the regular learning rate (schedule) to train your model to convergence.\n",
    "        The idea that this helps your network to slowly adapt to the data intuitively makes sense.\n",
    "        However, theoretically, the main reason for warmup steps is to allow adaptive optimisers (e.g. Adam, RMSProp, ...)\n",
    "        to compute correct statistics of the gradients. Therefore, a warmup period makes little sense when training with plain SGD.\n",
    "    overwrite_output_dir - Overwrite the output directory\n",
    "    fp16 - 16 point precision - Speed up the process\n",
    "    save_steps - save checkpoints to the model output folder\n",
    "    evaluation_strategy - Evaluation is done at the end of each epoch.\n",
    "    logging_dir - Folder to save the logs\n",
    "'''\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=model_store,\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=4,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    warmup_steps=1000,\n",
    "    overwrite_output_dir=True,\n",
    "    fp16=False,\n",
    "    seed=seed,\n",
    "    save_steps=30000,\n",
    "    evaluation_strategy='epoch',\n",
    "    logging_dir='./logs',\n",
    "    eval_accumulation_steps=1\n",
    ")\n",
    "\n",
    "\n",
    "#Pretrained model to download and use including the number of labels\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    'google/electra-small-discriminator', num_labels=7, output_hidden_states=True\n",
    ")\n",
    "\n",
    "#Select the tokenizer to use\n",
    "tokenizer = AutoTokenizer.from_pretrained('google/electra-small-discriminator')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fetch the data\n",
    "train_df, valid_df, test_df = fetch_train_valid_testsets(tokenizer)\n",
    "print(len(train_df), len(valid_df), len(test_df))\n",
    "\n",
    "#Set up the DNN model trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset= train_df,\n",
    "    eval_dataset = valid_df,\n",
    "    compute_metrics=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train and evaluate the model\n",
    "trainer.train()\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('../../baseline_public/'+'teacher_electra_base.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_split = np.array_split(test_df, 20)\n",
    "pred_labels = []\n",
    "true_labels = []\n",
    "for item in val_split:\n",
    "    val = trainer.predict(test_dataset=item)\n",
    "    true_labels.extend([id for id in val.label_ids])\n",
    "    pred_labels.extend([id for id in val.predictions[0].argmax(-1)])\n",
    "mif1, maf1 = test_compute_metrics(pred_labels, true_labels)\n",
    "print(mif1, maf1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "model = AutoModelForSequenceClassification.from_pretrained('../../hf_teacher/'+'teacher_roberta_base.pth', local_files_only=True)\n",
    "model= nn.DataParallel(model)\n",
    "model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=model_store,\n",
    "    learning_rate=3e-5,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_steps=100,\n",
    "    overwrite_output_dir=True,\n",
    "    fp16=False,\n",
    "    seed=seed,\n",
    "    save_steps=30000,\n",
    "    evaluation_strategy='epoch',\n",
    "    logging_dir='./logs',\n",
    "    eval_accumulation_steps=1\n",
    ")\n",
    "\n",
    "\n",
    "#Select the tokenizer to use\n",
    "tokenizer = AutoTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "#Fetch the data\n",
    "train_df, valid_df, test_df = fetch_train_valid_testsets(tokenizer)\n",
    "val_split = np.array_split(test_df, 3)\n",
    "\n",
    "#Set up the DNN model trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_df,\n",
    "    eval_dataset=valid_df,\n",
    "    compute_metrics=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Getting the embeddings of size - 768\n",
    "Get the one-hot encoding vector - 7\n",
    "\n",
    "'''\n",
    "\n",
    "df_split = np.array_split(embed_df, 10)\n",
    "i = 1\n",
    "for item in df_split:\n",
    "    temp = trainer.predict(test_dataset=item)\n",
    "    with h5py.File('../data/'+str(i)+'_embeddings.h5', 'w') as hf:\n",
    "        hf.create_dataset(\"raw_transcripts\",  data=temp.predictions[1][12])\n",
    "    i += 1\n",
    "    print(temp.predictions[1][12].shape)\n",
    "\n",
    "'''\n",
    "import numpy as np\n",
    "\n",
    "target = np.array(['dog', 'dog', 'cat', 'cat', 'cat', 'dog', 'dog', \n",
    "    'cat', 'cat', 'hamster', 'hamster'])\n",
    "\n",
    "def one_hot(array):\n",
    "    unique, inverse = np.unique(array, return_inverse=True)\n",
    "    onehot = np.eye(unique.shape[0])[inverse]\n",
    "    return onehot\n",
    "\n",
    "print(one_hot(target))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the new model\n",
    "\n",
    "class Feedforward(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(Feedforward, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size  = hidden_size\n",
    "        self.fc1 = torch.nn.Linear(self.input_size, self.hidden_size)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.fc2 = torch.nn.Linear(self.hidden_size, 7)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        hidden = self.fc1(x)\n",
    "        relu = self.relu(hidden)\n",
    "        output = self.fc2(relu)\n",
    "        output = F.log_softmax(output, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Feedforward(810, 1620)\n",
    "criterion = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "epoch = 20\n",
    "for epoch in range(epoch):\n",
    "    optimizer.zero_grad()\n",
    "    # Forward pass\n",
    "    y_pred = model(x_train)\n",
    "    # Compute Loss\n",
    "    loss = criterion(y_pred.squeeze(), y_train)\n",
    "   \n",
    "    print('Epoch {}: train loss: {}'.format(epoch, loss.item()))\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_split = np.array_split(test_df, 20)\n",
    "pred_labels = []\n",
    "true_labels = []\n",
    "for item in val_split:\n",
    "    val = trainer.predict(test_dataset=item)\n",
    "    true_labels.extend([id for id in val.label_ids])\n",
    "    pred_labels.extend([id for id in val.predictions[0].argmax(-1)])\n",
    "m1f1, maf1 = test_compute_metrics(pred_labels, true_labels)\n",
    "print(mif1, maf1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
