{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "This notebook is stil work in progress\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score, matthews_corrcoef\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, train_test_split\n",
    "import sys\n",
    "import logging\n",
    "import os\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "import pickle\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions  - Evaluating the model\n",
    "\n",
    "def save_confusion_matrix(y_true, y_pred, output_dir, file_name):\n",
    "    \"\"\"\n",
    "    Stores the confusion matrix in the file path\n",
    "    \"\"\"\n",
    "    labels = [\n",
    "        'None', 'KpEverTgthr', 'GetStudRelate', 'Restat',\n",
    "        'Revoic', 'PrsAcc', 'PrsRsn'\n",
    "    ]\n",
    "    conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "    svm = sns.heatmap(\n",
    "        conf_matrix/conf_matrix.sum(axis=1)[:, None],\n",
    "        cmap='Blues',\n",
    "        xticklabels=labels,\n",
    "        yticklabels=labels,\n",
    "        annot=True,\n",
    "    )\n",
    "    figure = svm.get_figure()\n",
    "    figure.savefig(output_dir + \"/\" + file_name + \".png\", dpi=600)\n",
    "    figure.clear()\n",
    "\n",
    "\n",
    "def custom_micro_f1_score(true_labels, pred_labels, label_count=7):\n",
    "    \"\"\"\n",
    "    This computes a custom micro f1 score ignoring\n",
    "    the predictions associated with label None (0)\n",
    "\n",
    "    This code still has to undergo some testing\n",
    "    \"\"\"\n",
    "\n",
    "    none_label = 0\n",
    "\n",
    "    ref_count = sum([1 for true in true_labels if true != none_label])\n",
    "    pred_count = sum([1 for pred in pred_labels if pred != none_label])\n",
    "    match_count = sum([1 for true, pred in zip(true_labels, pred_labels)\n",
    "                       if true == pred and true != none_label])\n",
    "\n",
    "    micro_precision = match_count / pred_count if pred_count != 0 else 0\n",
    "    micro_recall = match_count / ref_count if ref_count != 0 else 0\n",
    "    micro_f1 = 2/(1/micro_precision + 1/micro_recall) \\\n",
    "        if micro_precision != 0 and micro_recall != 0 else 0\n",
    "\n",
    "    return(micro_precision, micro_recall, micro_f1)\n",
    "\n",
    "\n",
    "def custom_macro_f1_score(true_labels, pred_labels, label_count=7):\n",
    "    \"\"\"\n",
    "    This computes a custom macro f1 score ignoring\n",
    "    in its entirety, the predictions associated\n",
    "    with the label None (0)\n",
    "\n",
    "\n",
    "    This code still has to undergo some testing\n",
    "    \"\"\"\n",
    "\n",
    "    none_label = 0\n",
    "\n",
    "    stats = defaultdict(Counter)\n",
    "\n",
    "    for true_label, pred_label in zip(true_labels, pred_labels):\n",
    "\n",
    "        stats[true_label]['tp+fn'] += 1\n",
    "\n",
    "        if true_label == pred_label:\n",
    "            stats[true_label]['tp'] += 1\n",
    "\n",
    "        stats[pred_label]['tp+fp'] += 1\n",
    "\n",
    "    for label in set(pred_labels):\n",
    "        if stats[label]['tp+fp'] != 0:\n",
    "            stats[label]['precision'] = \\\n",
    "                stats[label]['tp']/stats[label]['tp+fp']\n",
    "        if stats[label]['tp+fn'] != 0:\n",
    "            stats[label]['recall'] = stats[label]['tp']/stats[label]['tp+fn']\n",
    "        if stats[label]['precision'] != 0 and stats[label]['recall'] != 0:\n",
    "            stats[label]['f1'] = \\\n",
    "                2.0/(1/stats[label]['precision'] + 1/stats[label]['recall'])\n",
    "\n",
    "    macro_precision = sum(\n",
    "        [stats[lbl]['precision'] for lbl in stats.keys() if lbl != none_label]\n",
    "    )/(label_count - 1)\n",
    "\n",
    "    macro_recall = sum(\n",
    "        [stats[lbl]['recall'] for lbl in stats.keys() if lbl != none_label]\n",
    "    )/(label_count - 1)\n",
    "\n",
    "    macro_f1 = 2/(1/macro_precision + 1/macro_recall) \\\n",
    "        if macro_precision != 0 and macro_recall != 0 else 0\n",
    "\n",
    "    return macro_f1, stats\n",
    "\n",
    "\n",
    "class TalkBackDataset(torch.utils.data.Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data,\n",
    "        tokenizer,\n",
    "        max_seq_length\n",
    "    ):\n",
    "\n",
    "        input_examples = [\n",
    "            (\n",
    "                '' if datum[1].text_a is np.nan else datum[1].text_a.lower(),\n",
    "                '' if datum[1].text_b is np.nan else datum[1].text_b.lower()\n",
    "            ) for datum in data.iterrows()\n",
    "        ]\n",
    "        self.examples = tokenizer.batch_encode_plus(\n",
    "            batch_text_or_text_pairs=input_examples,\n",
    "            max_length=max_seq_length,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        self.labels = [\n",
    "            torch.tensor(int(label), dtype=torch.long) for label in data.labels\n",
    "        ]\n",
    "        \n",
    "        self.damsl_labels = [\n",
    "            str(label) for label in data.damsl_labels\n",
    "        ]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples['input_ids'])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "            Use one if the other fails\n",
    "        \"\"\"\n",
    "        answer = {}\n",
    "        for key in self.examples:\n",
    "            answer[key] = self.examples[key][index]\n",
    "\n",
    "        answer['label'] = self.labels[index]\n",
    "        answer['damsl_label'] = self.damsl_labels[index]\n",
    "        return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/ec2-user/anaconda3/envs/p37/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/ec2-user/anaconda3/envs/p37/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/ec2-user/anaconda3/envs/p37/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/ec2-user/anaconda3/envs/p37/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/ec2-user/anaconda3/envs/p37/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/ec2-user/anaconda3/envs/p37/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache() \n",
    "import gc\n",
    "gc.collect()\n",
    "from keras import backend as K\n",
    "K.clear_session()\n",
    "\n",
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def fetch_train_valid_testsets(tokenizer, max_seq_length=128, seed=1010):\n",
    "    '''\n",
    "    Arguments:\n",
    "        tokenizer - BERT tokenizer\n",
    "        max_seq_length - Maximum sequence length\n",
    "        seed - Random seed\n",
    "\n",
    "    Returns:\n",
    "        train_df - Training set (Pandas dataframe)\n",
    "        valid_df - Validation set (Pandas dataframe)\n",
    "        test_df - Testing set (Pandas dataframe)\n",
    "    '''\n",
    "    '''\n",
    "    the data in TSV format.\n",
    "    Contains three colums:\n",
    "        text_a (previous student sentence);\n",
    "        text_b (teacher sentence);\n",
    "        labels (category or TalkMove label)\n",
    "    '''\n",
    "    train_data = pd.read_csv(\n",
    "        '../data/train_teacher_dact.tsv', sep='\\t'\n",
    "    )\n",
    "    \n",
    "    valid_data = pd.read_csv(\n",
    "        '../data/test_teacher_dact.tsv', sep='\\t'\n",
    "    )\n",
    "    \n",
    "    test_data = pd.read_csv(\n",
    "        '../data/test_teacher_dact.tsv', sep='\\t'\n",
    "    )\n",
    "\n",
    "\n",
    "    X_train, X_test, y_train, y_test =  train_data, test_data, train_data.labels, test_data.labels\n",
    "    X_valid, y_valid = valid_data, valid_data.labels\n",
    "    \n",
    "    train_df = X_train.replace(np.nan, '', regex=True)\n",
    "    train_df = TalkBackDataset(train_df, tokenizer, max_seq_length)\n",
    "    \n",
    "    valid_df = X_valid.replace(np.nan, '', regex=True)\n",
    "    valid_df = TalkBackDataset(valid_df, tokenizer, max_seq_length)\n",
    "    \n",
    "    test_df = X_test.replace(np.nan, '', regex=True)\n",
    "    test_df = TalkBackDataset(test_df, tokenizer, max_seq_length)\n",
    "\n",
    "    return(train_df, valid_df, test_df)\n",
    "\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    '''\n",
    "    Arguments:\n",
    "        pred - predictions\n",
    "\n",
    "    Returns:\n",
    "        accuracy, Micro F1, Macro F1 and MCC scoresoutput_attentions=True,  \n",
    "\n",
    "    '''\n",
    "    true_labels = pred.label_ids\n",
    "    #print(type(pred), pred.predictions.shape, pred.predictions)\n",
    "    pred_labels = pred.predictions.argmax(-1)\n",
    "    _, _, mif1 = custom_micro_f1_score(true_labels, pred_labels)\n",
    "    maf1, _ = custom_macro_f1_score(true_labels, pred_labels)\n",
    "    acc = accuracy_score(true_labels, pred_labels)\n",
    "    matthew_corr = matthews_corrcoef(true_labels, pred_labels)\n",
    "\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'micro_f1': mif1,\n",
    "        'macro_f1': maf1,\n",
    "        'matthew_corr_coeff': matthew_corr\n",
    "    }\n",
    "\n",
    "\n",
    "def test_compute_metrics(pred, model_store):\n",
    "    _, _, mif1 = custom_micro_f1_score(true_labels, pred_labels)\n",
    "    maf1, _ = custom_macro_f1_score(true_labels, pred_labels)\n",
    "\n",
    "    print(confusion_matrix(true_labels, pred_labels))\n",
    "    #print(save_confusion_matrix(true_labels, pred_labels, model_store, \"unseen\"))\n",
    "    print(classification_report(true_labels, pred_labels))\n",
    "    print(matthews_corrcoef(true_labels, pred_labels))\n",
    "    return mif1, maf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Command line agrument - Folder name where the model outputs can be stored\n",
    "folder_name = '../../../baseline_public/'\n",
    "\n",
    "#Command line argument  - Random seed\n",
    "seed = 1022\n",
    "model_store = folder_name + \"/model_storage/\"\n",
    "\n",
    "#if the model storage folder does not exist, create it\n",
    "if not os.path.exists(model_store):\n",
    "    os.makedirs(model_store)\n",
    "\n",
    "#Log the process to help with debugging. All the logs will be saved in the corresponding folder\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "transformers_logger = logging.getLogger(\"transformers-\" + str(seed))\n",
    "transformers_logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Arguments for model training Including:\n",
    "    output_dir - Store the outputs of the checkpoints as well as the best model\n",
    "    learning_rate - learning learning_rate\n",
    "    num_train_epochs - Number of training epochs (Tuning)\n",
    "    per_device_train_batch_size - Training batch size\n",
    "    per_device_eval_batch_size - validation batch size\n",
    "    warmup_steps - Number of warmup steps\n",
    "        Warmup steps are just a few updates with low learning rate before / at the beginning of training.\n",
    "        After this warmup, you use the regular learning rate (schedule) to train your model to convergence.\n",
    "        The idea that this helps your network to slowly adapt to the data intuitively makes sense.\n",
    "        However, theoretically, the main reason for warmup steps is to allow adaptive optimisers (e.g. Adam, RMSProp, ...)\n",
    "        to compute correct statistics of the gradients. Therefore, a warmup period makes little sense when training with plain SGD.\n",
    "    overwrite_output_dir - Overwrite the output directory\n",
    "    fp16 - 16 point precision - Speed up the process\n",
    "    save_steps - save checkpoints to the model output folder\n",
    "    evaluation_strategy - Evaluation is done at the end of each epoch.\n",
    "    logging_dir - Folder to save the logs\n",
    "'''\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=model_store,\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=4,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    warmup_steps=1000,\n",
    "    overwrite_output_dir=True,\n",
    "    fp16=False,\n",
    "    seed=seed,\n",
    "    save_steps=30000,\n",
    "    evaluation_strategy='epoch',\n",
    "    logging_dir='./logs',\n",
    "    eval_accumulation_steps=1\n",
    ")\n",
    "\n",
    "\n",
    "#Pretrained model to download and use including the number of labels\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    'roberta-base', num_labels=7, output_hidden_states=True\n",
    ")\n",
    "\n",
    "#Select the tokenizer to use\n",
    "tokenizer = AutoTokenizer.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Fetch the data\n",
    "train_df, valid_df, test_df = fetch_train_valid_testsets(tokenizer)\n",
    "print(len(train_df), len(valid_df), len(test_df))\n",
    "\n",
    "#Set up the DNN model trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset= train_df,\n",
    "    eval_dataset = valid_df,\n",
    "    compute_metrics=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Train and evaluate the model\n",
    "trainer.train()\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('../../baseline_public/'+'teacher_dact_roberta_base.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/p37/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5890' max='31' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [31/31 42:41]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[12457   429    30   101    96   290    23]\n",
      " [  587  1649    33    17     7   110     7]\n",
      " [   31    13   142     0     0    29     3]\n",
      " [  131    21     1    96     2     5     0]\n",
      " [  304    23     0    22    90     9     0]\n",
      " [  240    84    13    27     5  2183    32]\n",
      " [    8     1     1     0     0    25   183]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.93      0.92     13426\n",
      "           1       0.74      0.68      0.71      2410\n",
      "           2       0.65      0.65      0.65       218\n",
      "           3       0.37      0.38      0.37       256\n",
      "           4       0.45      0.20      0.28       448\n",
      "           5       0.82      0.84      0.83      2584\n",
      "           6       0.74      0.84      0.79       218\n",
      "\n",
      "   micro avg       0.86      0.86      0.86     19560\n",
      "   macro avg       0.67      0.65      0.65     19560\n",
      "weighted avg       0.85      0.86      0.85     19560\n",
      "\n",
      "0.70943406230943\n",
      "0.7277144772117963 0.6130436086110573\n"
     ]
    }
   ],
   "source": [
    "val_split = np.array_split(test_df, 20)\n",
    "pred_labels = []\n",
    "true_labels = []\n",
    "for item in val_split:\n",
    "    val = trainer.predict(test_dataset=item)\n",
    "    true_labels.extend([id for id in val.label_ids])\n",
    "    pred_labels.extend([id for id in val.predictions[0].argmax(-1)])\n",
    "mif1, maf1 = test_compute_metrics(pred_labels, true_labels)\n",
    "print(mif1, maf1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Saved Model and prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): RobertaForSequenceClassification(\n",
       "    (roberta): RobertaModel(\n",
       "      (embeddings): RobertaEmbeddings(\n",
       "        (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "        (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "        (token_type_embeddings): Embedding(1, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): RobertaEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (2): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (3): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (4): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (5): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (6): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (7): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (8): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (9): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (10): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (11): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (classifier): RobertaClassificationHead(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (out_proj): Linear(in_features=768, out_features=7, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "model = AutoModelForSequenceClassification.from_pretrained('../../baseline_public/'+'teacher_dact_roberta_base.pth/', local_files_only=True)\n",
    "model= nn.DataParallel(model)\n",
    "model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/p37/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/ec2-user/anaconda3/envs/p37/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/ec2-user/anaconda3/envs/p37/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/ec2-user/anaconda3/envs/p37/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/ec2-user/anaconda3/envs/p37/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/ec2-user/anaconda3/envs/p37/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=model_store,\n",
    "    learning_rate=3e-5,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_steps=100,\n",
    "    overwrite_output_dir=True,\n",
    "    fp16=False,\n",
    "    seed=seed,\n",
    "    save_steps=30000,\n",
    "    evaluation_strategy='epoch',\n",
    "    logging_dir='./logs',\n",
    "    eval_accumulation_steps=1\n",
    ")\n",
    "\n",
    "\n",
    "#Select the tokenizer to use\n",
    "tokenizer = AutoTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "#Fetch the data\n",
    "train_df, valid_df, test_df = fetch_train_valid_testsets(tokenizer)\n",
    "val_split = np.array_split(test_df, 3)\n",
    "\n",
    "#Set up the DNN model trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_df,\n",
    "    eval_dataset=valid_df,\n",
    "    compute_metrics=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sd': array([0., 0., 0., 0., 0., 1., 0.]), 'qy': array([0., 0., 0., 0., 1., 0., 0.]), 'ba': array([0., 0., 1., 0., 0., 0., 0.]), 'b': array([0., 1., 0., 0., 0., 0., 0.]), 'sv': array([0., 0., 0., 0., 0., 0., 1.]), 'fc': array([0., 0., 0., 1., 0., 0., 0.]), 'aa': array([1., 0., 0., 0., 0., 0., 0.])}\n"
     ]
    }
   ],
   "source": [
    "train_data = pd.read_csv(\n",
    "    '../data/train_teacher_dact.tsv', sep='\\t'\n",
    ")\n",
    "target = list(set(train_data['damsl_labels']))\n",
    "\n",
    "def one_hot(array):\n",
    "    unique, inverse = np.unique(array, return_inverse=True)\n",
    "    onehot = np.eye(unique.shape[0])[inverse]\n",
    "    return onehot\n",
    "\n",
    "one_hot_vectors = one_hot(target)\n",
    "one_hot_target = {}\n",
    "for i in range(len(target)):\n",
    "    one_hot_target[target[i]] = one_hot_vectors[i]\n",
    "    \n",
    "print(one_hot_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(143829, 775) (143829,)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Getting the embeddings of size - 768\n",
    "Get the one-hot encoding vector - 7\n",
    "\n",
    "'''\n",
    "train_x = np.array([])\n",
    "train_y = np.array([])\n",
    "\n",
    "df_split = np.array_split(train_df, 30)\n",
    "i = 1\n",
    "for item in df_split:\n",
    "    temp = trainer.predict(test_dataset=item)\n",
    "    embeddings = np.array([i[0] for i in temp.predictions[1][12]])\n",
    "    one_hot_damsl = [one_hot_target[i['damsl_label']] for i in item]\n",
    "    combined = np.array([np.concatenate((embeddings[i], one_hot_damsl[i]), axis=0) for i in range(len(embeddings))])\n",
    "    target_labels = np.array([i['label'].cpu().detach().numpy() for i in item])\n",
    "    if  i == 1: \n",
    "        train_x = combined\n",
    "        train_y = target_labels\n",
    "    else:\n",
    "        train_x = np.append(train_x, combined, axis=0)\n",
    "        train_y = np.append(train_y, target_labels, axis=0)\n",
    "    i+=1\n",
    "\n",
    "print(train_x.shape, train_y.shape)\n",
    "    \n",
    "with open('../../train_dact_embeddings.pkl', 'wb') as f:\n",
    "    pickle.dump([train_x, train_y], f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19560, 775) (19560,)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Getting the embeddings of size - 768\n",
    "Get the one-hot encoding vector - 7\n",
    "\n",
    "'''\n",
    "test_x = np.array([])\n",
    "test_y = np.array([])\n",
    "\n",
    "df_split = np.array_split(test_df, 10)\n",
    "i = 1\n",
    "for item in df_split:\n",
    "    temp = trainer.predict(test_dataset=item)\n",
    "    embeddings = np.array([i[0] for i in temp.predictions[1][12]])\n",
    "    one_hot_damsl = [one_hot_target[i['damsl_label']] for i in item]\n",
    "    target_labels = np.array([i['label'].cpu().detach().numpy() for i in item])\n",
    "    combined = np.array([np.concatenate((embeddings[i], one_hot_damsl[i]), axis=0) for i in range(len(embeddings))])\n",
    "    if  i == 1: \n",
    "        test_x = combined\n",
    "        test_y = target_labels\n",
    "    else:\n",
    "        test_x = np.append(test_x, combined, axis=0)\n",
    "        test_y = np.append(test_y, target_labels, axis=0)\n",
    "    i += 1\n",
    "\n",
    "print(test_x.shape, test_y.shape)\n",
    "    \n",
    "with open('../../test_dact_embeddings.pkl', 'wb') as f:\n",
    "    pickle.dump([test_x, test_y], f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Test model to predict Talk moves based on concatenated embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Tensor datasets\n",
    "train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
    "valid_data = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n",
    "\n",
    "# dataloaders\n",
    "batch_size = 19560\n",
    "\n",
    "# make sure to SHUFFLE your data\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample input size:  torch.Size([19560, 775])\n",
      "Sample input: \n",
      " tensor([[-0.3037, -1.1001,  0.6740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2577, -1.5493,  1.0307,  ...,  0.0000,  1.0000,  0.0000],\n",
      "        [-0.4984, -1.3651,  0.4327,  ...,  0.0000,  1.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1132, -1.3215,  0.5682,  ...,  0.0000,  1.0000,  0.0000],\n",
      "        [-0.9925, -1.8903,  0.5632,  ...,  0.0000,  1.0000,  0.0000],\n",
      "        [ 0.7415,  0.1074, -0.0606,  ...,  0.0000,  1.0000,  0.0000]],\n",
      "       dtype=torch.float64)\n",
      "Sample input: \n",
      " tensor([0, 0, 0,  ..., 0, 0, 5])\n"
     ]
    }
   ],
   "source": [
    "# obtain one batch of training data\n",
    "dataiter = iter(train_loader)\n",
    "sample_x, sample_y = dataiter.next()\n",
    "\n",
    "print('Sample input size: ', sample_x.size()) # batch_size, seq_length\n",
    "print('Sample input: \\n', sample_x)\n",
    "print('Sample input: \\n', sample_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the new model\n",
    "class Feedforward(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(Feedforward, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size  = hidden_size\n",
    "        self.fc1 = torch.nn.Linear(self.input_size, self.hidden_size)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.fc2 = torch.nn.Linear(self.hidden_size, 7)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        hidden = self.fc1(x)\n",
    "        relu = self.relu(hidden)\n",
    "        output = self.fc2(relu)\n",
    "        output = F.log_softmax(output, dim=1)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available\n"
     ]
    }
   ],
   "source": [
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU not available, CPU used\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feedforward(\n",
      "  (fc1): Linear(in_features=775, out_features=1620, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (fc2): Linear(in_features=1620, out_features=7, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = Feedforward(775, 1620)\n",
    "\n",
    "#moving to gpu\n",
    "model.to(device)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss and optimization functions\n",
    "lr=0.0001\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to predict accuracy\n",
    "def mcc(pred, target):\n",
    "    pred_y = torch.round(pred.argmax(-1).float().squeeze()).cpu().detach().numpy()\n",
    "    target_y = np.array([float(i) for i in target.cpu().detach().numpy()])\n",
    "    \n",
    "    _, _, mif1 = custom_micro_f1_score(target_y, pred_y)\n",
    "    maf1, _ = custom_macro_f1_score(target_y, pred_y)\n",
    "\n",
    "    #print(confusion_matrix(target_y, pred_y))\n",
    "    #print(classification_report(target_y, pred_y))\n",
    "    #print(matthews_corrcoef(target_y, pred_y))\n",
    "    #print(maf1, mif1)\n",
    "    return torch.tensor(maf1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "train_loss : 1.1059598699212074 val_loss : 0.5925055742263794\n",
      "train_score : 2.394859075138811e-05 val_score : 2.1926585759501904e-05\n",
      "Validation loss decreased (inf --> 0.592506).  Saving model ...\n",
      "==================================================\n",
      "Epoch 2\n",
      "train_loss : 0.36460597813129425 val_loss : 0.4961670935153961\n",
      "train_score : 3.089259917032905e-05 val_score : 2.355009928578511e-05\n",
      "Validation loss decreased (0.592506 --> 0.496167).  Saving model ...\n",
      "==================================================\n",
      "Epoch 3\n",
      "train_loss : 0.2758903093636036 val_loss : 0.5093463659286499\n",
      "train_score : 3.48027897416614e-05 val_score : 2.6570543923298828e-05\n",
      "==================================================\n",
      "Epoch 4\n",
      "train_loss : 0.25008177757263184 val_loss : 0.5061612725257874\n",
      "train_score : 3.923468466382474e-05 val_score : 2.944473089883104e-05\n",
      "==================================================\n",
      "Epoch 5\n",
      "train_loss : 0.23024428077042103 val_loss : 0.4956793189048767\n",
      "train_score : 4.231636194163002e-05 val_score : 3.070936872973107e-05\n",
      "Validation loss decreased (0.496167 --> 0.495679).  Saving model ...\n",
      "==================================================\n",
      "Epoch 6\n",
      "train_loss : 0.2199031561613083 val_loss : 0.48795413970947266\n",
      "train_score : 4.384437852422707e-05 val_score : 3.1170293368631974e-05\n",
      "Validation loss decreased (0.495679 --> 0.487954).  Saving model ...\n",
      "==================================================\n",
      "Epoch 7\n",
      "train_loss : 0.21585099399089813 val_loss : 0.479838490486145\n",
      "train_score : 4.390536923892796e-05 val_score : 3.1145536922849715e-05\n",
      "Validation loss decreased (0.487954 --> 0.479838).  Saving model ...\n",
      "==================================================\n",
      "Epoch 8\n",
      "train_loss : 0.21345638297498226 val_loss : 0.47264233231544495\n",
      "train_score : 4.3940490286331624e-05 val_score : 3.128878233837895e-05\n",
      "Validation loss decreased (0.479838 --> 0.472642).  Saving model ...\n",
      "==================================================\n",
      "Epoch 9\n",
      "train_loss : 0.21120978705585003 val_loss : 0.467349648475647\n",
      "train_score : 4.393007475300692e-05 val_score : 3.121478948742151e-05\n",
      "Validation loss decreased (0.472642 --> 0.467350).  Saving model ...\n",
      "==================================================\n",
      "Epoch 10\n",
      "train_loss : 0.21176790818572044 val_loss : 0.4637680947780609\n",
      "train_score : 4.3916046706726775e-05 val_score : 3.123686110484414e-05\n",
      "Validation loss decreased (0.467350 --> 0.463768).  Saving model ...\n",
      "==================================================\n",
      "Epoch 11\n",
      "train_loss : 0.21027428470551968 val_loss : 0.4615043103694916\n",
      "train_score : 4.390485628391616e-05 val_score : 3.1236071663443e-05\n",
      "Validation loss decreased (0.463768 --> 0.461504).  Saving model ...\n",
      "==================================================\n",
      "Epoch 12\n",
      "train_loss : 0.20872995629906654 val_loss : 0.4604979455471039\n",
      "train_score : 4.39566902059596e-05 val_score : 3.118656240985729e-05\n",
      "Validation loss decreased (0.461504 --> 0.460498).  Saving model ...\n",
      "==================================================\n",
      "Epoch 13\n",
      "train_loss : 0.20950296707451344 val_loss : 0.4605153203010559\n",
      "train_score : 4.4035405153408647e-05 val_score : 3.117294909316115e-05\n",
      "==================================================\n",
      "Epoch 14\n",
      "train_loss : 0.2079166043549776 val_loss : 0.46034297347068787\n",
      "train_score : 4.405208528623916e-05 val_score : 3.1198760552797467e-05\n",
      "Validation loss decreased (0.460498 --> 0.460343).  Saving model ...\n",
      "==================================================\n",
      "Epoch 15\n",
      "train_loss : 0.2081245705485344 val_loss : 0.46050140261650085\n",
      "train_score : 4.411835107021034e-05 val_score : 3.11909789161291e-05\n",
      "==================================================\n",
      "Epoch 16\n",
      "train_loss : 0.20696059241890907 val_loss : 0.4606197774410248\n",
      "train_score : 4.404063292895444e-05 val_score : 3.1244395358953625e-05\n",
      "==================================================\n",
      "Epoch 17\n",
      "train_loss : 0.20669257268309593 val_loss : 0.4608670175075531\n",
      "train_score : 4.403428829391487e-05 val_score : 3.122269845334813e-05\n",
      "==================================================\n",
      "Epoch 18\n",
      "train_loss : 0.2050558477640152 val_loss : 0.4609124958515167\n",
      "train_score : 4.4058448111172765e-05 val_score : 3.123197893728502e-05\n",
      "==================================================\n",
      "Epoch 19\n",
      "train_loss : 0.20557439513504505 val_loss : 0.4612981081008911\n",
      "train_score : 4.404642459121533e-05 val_score : 3.1237526854965836e-05\n",
      "==================================================\n",
      "Epoch 20\n",
      "train_loss : 0.20464604906737804 val_loss : 0.46169522404670715\n",
      "train_score : 4.413855276652612e-05 val_score : 3.1156494515016675e-05\n",
      "==================================================\n",
      "Epoch 21\n",
      "train_loss : 0.20481868647038937 val_loss : 0.4616607427597046\n",
      "train_score : 4.403810453368351e-05 val_score : 3.118267341051251e-05\n",
      "==================================================\n",
      "Epoch 22\n",
      "train_loss : 0.20616443641483784 val_loss : 0.46173256635665894\n",
      "train_score : 4.399579847813584e-05 val_score : 3.119519533356652e-05\n",
      "==================================================\n",
      "Epoch 23\n",
      "train_loss : 0.2040058895945549 val_loss : 0.46103435754776\n",
      "train_score : 4.4217587856110185e-05 val_score : 3.109401586698368e-05\n",
      "==================================================\n",
      "Epoch 24\n",
      "train_loss : 0.204325458034873 val_loss : 0.46143242716789246\n",
      "train_score : 4.4096424971939996e-05 val_score : 3.114180435659364e-05\n",
      "==================================================\n",
      "Epoch 25\n",
      "train_loss : 0.20434032380580902 val_loss : 0.461437851190567\n",
      "train_score : 4.4133579649496824e-05 val_score : 3.115741128567606e-05\n",
      "==================================================\n",
      "Epoch 26\n",
      "train_loss : 0.20405212603509426 val_loss : 0.4615917503833771\n",
      "train_score : 4.411260670167394e-05 val_score : 3.111497426289134e-05\n",
      "==================================================\n",
      "Epoch 27\n",
      "train_loss : 0.20386836864054203 val_loss : 0.4618850648403168\n",
      "train_score : 4.4146363507024944e-05 val_score : 3.111634578090161e-05\n",
      "==================================================\n",
      "Epoch 28\n",
      "train_loss : 0.20383982174098492 val_loss : 0.4619702696800232\n",
      "train_score : 4.4098076614318416e-05 val_score : 3.1084819056559354e-05\n",
      "==================================================\n",
      "Epoch 29\n",
      "train_loss : 0.20408634468913078 val_loss : 0.46159711480140686\n",
      "train_score : 4.410757537698373e-05 val_score : 3.110709076281637e-05\n",
      "==================================================\n",
      "Epoch 30\n",
      "train_loss : 0.2035272680222988 val_loss : 0.4619324207305908\n",
      "train_score : 4.4100990635342896e-05 val_score : 3.1112525903154165e-05\n",
      "==================================================\n",
      "Epoch 31\n",
      "train_loss : 0.20249905996024609 val_loss : 0.4628508687019348\n",
      "train_score : 4.406402513268404e-05 val_score : 3.113256025244482e-05\n",
      "==================================================\n",
      "Epoch 32\n",
      "train_loss : 0.20376089215278625 val_loss : 0.4624333083629608\n",
      "train_score : 4.416624142322689e-05 val_score : 3.108466989942826e-05\n",
      "==================================================\n",
      "Epoch 33\n",
      "train_loss : 0.20211918838322163 val_loss : 0.46204516291618347\n",
      "train_score : 4.422510028234683e-05 val_score : 3.1127499823924154e-05\n",
      "==================================================\n",
      "Epoch 34\n",
      "train_loss : 0.20191369578242302 val_loss : 0.46255776286125183\n",
      "train_score : 4.4177548261359334e-05 val_score : 3.1113275326788425e-05\n",
      "==================================================\n",
      "Epoch 35\n",
      "train_loss : 0.20313076674938202 val_loss : 0.4623352885246277\n",
      "train_score : 4.412985799717717e-05 val_score : 3.1105362722883e-05\n",
      "==================================================\n",
      "Epoch 36\n",
      "train_loss : 0.2017490193247795 val_loss : 0.46181175112724304\n",
      "train_score : 4.424326471053064e-05 val_score : 3.1169041903922334e-05\n",
      "==================================================\n",
      "Epoch 37\n",
      "train_loss : 0.20221244916319847 val_loss : 0.4627712070941925\n",
      "train_score : 4.4200474803801626e-05 val_score : 3.109206591034308e-05\n",
      "==================================================\n",
      "Epoch 38\n",
      "train_loss : 0.201607346534729 val_loss : 0.46301543712615967\n",
      "train_score : 4.417581294546835e-05 val_score : 3.111675687250681e-05\n",
      "==================================================\n",
      "Epoch 39\n",
      "train_loss : 0.2021857611835003 val_loss : 0.4627354145050049\n",
      "train_score : 4.4184991565998644e-05 val_score : 3.1084455258678645e-05\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40\n",
      "train_loss : 0.20270264893770218 val_loss : 0.4627295136451721\n",
      "train_score : 4.417216405272484e-05 val_score : 3.108157397946343e-05\n",
      "==================================================\n",
      "Epoch 41\n",
      "train_loss : 0.20267910324037075 val_loss : 0.4623699486255646\n",
      "train_score : 4.420725235831924e-05 val_score : 3.114562787231989e-05\n",
      "==================================================\n",
      "Epoch 42\n",
      "train_loss : 0.20186679065227509 val_loss : 0.4622545540332794\n",
      "train_score : 4.417895252117887e-05 val_score : 3.1135266908677295e-05\n",
      "==================================================\n",
      "Epoch 43\n",
      "train_loss : 0.20216416008770466 val_loss : 0.462779700756073\n",
      "train_score : 4.4175219954922795e-05 val_score : 3.1076699087861925e-05\n",
      "==================================================\n",
      "Epoch 44\n",
      "train_loss : 0.20117821916937828 val_loss : 0.4630518853664398\n",
      "train_score : 4.4192645873408765e-05 val_score : 3.1077899620868266e-05\n",
      "==================================================\n",
      "Epoch 45\n",
      "train_loss : 0.20196669176220894 val_loss : 0.46310654282569885\n",
      "train_score : 4.4241056457394734e-05 val_score : 3.111006662948057e-05\n",
      "==================================================\n",
      "Epoch 46\n",
      "train_loss : 0.2009449154138565 val_loss : 0.46260765194892883\n",
      "train_score : 4.422717393026687e-05 val_score : 3.112496779067442e-05\n",
      "==================================================\n",
      "Epoch 47\n",
      "train_loss : 0.19987123273313046 val_loss : 0.4635148048400879\n",
      "train_score : 4.425567385624163e-05 val_score : 3.113313505309634e-05\n",
      "==================================================\n",
      "Epoch 48\n",
      "train_loss : 0.20180907659232616 val_loss : 0.4643062949180603\n",
      "train_score : 4.411681948113255e-05 val_score : 3.115768049610779e-05\n",
      "==================================================\n",
      "Epoch 49\n",
      "train_loss : 0.20092790760099888 val_loss : 0.46341001987457275\n",
      "train_score : 4.4241493014851585e-05 val_score : 3.1140807550400496e-05\n",
      "==================================================\n",
      "Epoch 50\n",
      "train_loss : 0.20119060948491096 val_loss : 0.4629497826099396\n",
      "train_score : 4.418847311171703e-05 val_score : 3.107886004727334e-05\n",
      "==================================================\n",
      "Epoch 51\n",
      "train_loss : 0.20045821741223335 val_loss : 0.4633127450942993\n",
      "train_score : 4.4301923480816185e-05 val_score : 3.1162184313870966e-05\n",
      "==================================================\n",
      "Epoch 52\n",
      "train_loss : 0.20069252885878086 val_loss : 0.46401408314704895\n",
      "train_score : 4.41492120444309e-05 val_score : 3.1142710213316604e-05\n",
      "==================================================\n",
      "Epoch 53\n",
      "train_loss : 0.19951188936829567 val_loss : 0.4641205072402954\n",
      "train_score : 4.43129101768136e-05 val_score : 3.106887743342668e-05\n",
      "==================================================\n",
      "Epoch 54\n",
      "train_loss : 0.2007078006863594 val_loss : 0.46421319246292114\n",
      "train_score : 4.42604286945425e-05 val_score : 3.117914457106963e-05\n",
      "==================================================\n",
      "Epoch 55\n",
      "train_loss : 0.19956901669502258 val_loss : 0.46418482065200806\n",
      "train_score : 4.431360139278695e-05 val_score : 3.118015229119919e-05\n",
      "==================================================\n",
      "Epoch 56\n",
      "train_loss : 0.19911127351224422 val_loss : 0.4638296961784363\n",
      "train_score : 4.427960084285587e-05 val_score : 3.117684900644235e-05\n",
      "==================================================\n",
      "Epoch 57\n",
      "train_loss : 0.19989357702434063 val_loss : 0.4650475084781647\n",
      "train_score : 4.419146353029646e-05 val_score : 3.1121107895160094e-05\n",
      "==================================================\n",
      "Epoch 58\n",
      "train_loss : 0.19956610165536404 val_loss : 0.4651757478713989\n",
      "train_score : 4.433070353115909e-05 val_score : 3.1100553314900026e-05\n",
      "==================================================\n",
      "Epoch 59\n",
      "train_loss : 0.2004210166633129 val_loss : 0.4639042913913727\n",
      "train_score : 4.424012149684131e-05 val_score : 3.112947888439521e-05\n",
      "==================================================\n",
      "Epoch 60\n",
      "train_loss : 0.2000537272542715 val_loss : 0.4642866551876068\n",
      "train_score : 4.4332558900350705e-05 val_score : 3.1108371331356466e-05\n",
      "==================================================\n",
      "Epoch 61\n",
      "train_loss : 0.19918654300272465 val_loss : 0.4638625383377075\n",
      "train_score : 4.4277920096646994e-05 val_score : 3.1126561225391924e-05\n",
      "==================================================\n",
      "Epoch 62\n",
      "train_loss : 0.19929066859185696 val_loss : 0.46484291553497314\n",
      "train_score : 4.427261956152506e-05 val_score : 3.1165902328211814e-05\n",
      "==================================================\n",
      "Epoch 63\n",
      "train_loss : 0.1988938096910715 val_loss : 0.4650541841983795\n",
      "train_score : 4.437611278262921e-05 val_score : 3.1129424314713106e-05\n",
      "==================================================\n",
      "Epoch 64\n",
      "train_loss : 0.20023036003112793 val_loss : 0.4645290672779083\n",
      "train_score : 4.433480353327468e-05 val_score : 3.1121719075599685e-05\n",
      "==================================================\n",
      "Epoch 65\n",
      "train_loss : 0.19957681000232697 val_loss : 0.4646655023097992\n",
      "train_score : 4.4353160774335265e-05 val_score : 3.116323205176741e-05\n",
      "==================================================\n",
      "Epoch 66\n",
      "train_loss : 0.19953993894159794 val_loss : 0.46461108326911926\n",
      "train_score : 4.434476068126969e-05 val_score : 3.116287916782312e-05\n",
      "==================================================\n",
      "Epoch 67\n",
      "train_loss : 0.1983963493257761 val_loss : 0.46449223160743713\n",
      "train_score : 4.443129728315398e-05 val_score : 3.1144136301008984e-05\n",
      "==================================================\n",
      "Epoch 68\n",
      "train_loss : 0.19906272180378437 val_loss : 0.46533986926078796\n",
      "train_score : 4.434363290783949e-05 val_score : 3.112350532319397e-05\n",
      "==================================================\n",
      "Epoch 69\n",
      "train_loss : 0.1982522439211607 val_loss : 0.46557316184043884\n",
      "train_score : 4.4338135921861976e-05 val_score : 3.112044942099601e-05\n",
      "==================================================\n",
      "Epoch 70\n",
      "train_loss : 0.19987383112311363 val_loss : 0.465373158454895\n",
      "train_score : 4.420828918227926e-05 val_score : 3.115258732577786e-05\n",
      "==================================================\n",
      "Epoch 71\n",
      "train_loss : 0.19889535754919052 val_loss : 0.46336886286735535\n",
      "train_score : 4.437136522028595e-05 val_score : 3.114657738478854e-05\n",
      "==================================================\n",
      "Epoch 72\n",
      "train_loss : 0.19744334556162357 val_loss : 0.4641881585121155\n",
      "train_score : 4.43511271441821e-05 val_score : 3.113753700745292e-05\n",
      "==================================================\n",
      "Epoch 73\n",
      "train_loss : 0.19895522482693195 val_loss : 0.4673999845981598\n",
      "train_score : 4.4240718125365674e-05 val_score : 3.1143747037276626e-05\n",
      "==================================================\n",
      "Epoch 74\n",
      "train_loss : 0.1975714974105358 val_loss : 0.4660360813140869\n",
      "train_score : 4.431391425896436e-05 val_score : 3.109430690528825e-05\n",
      "==================================================\n",
      "Epoch 75\n",
      "train_loss : 0.1982252076268196 val_loss : 0.46572861075401306\n",
      "train_score : 4.434177026269026e-05 val_score : 3.108604869339615e-05\n",
      "==================================================\n",
      "Epoch 76\n",
      "train_loss : 0.19776930287480354 val_loss : 0.4660363495349884\n",
      "train_score : 4.445346348802559e-05 val_score : 3.099239256698638e-05\n",
      "==================================================\n",
      "Epoch 77\n",
      "train_loss : 0.19794232957065105 val_loss : 0.4662144184112549\n",
      "train_score : 4.4331281969789416e-05 val_score : 3.1148112611845136e-05\n",
      "==================================================\n",
      "Epoch 78\n",
      "train_loss : 0.1982837561517954 val_loss : 0.466725617647171\n",
      "train_score : 4.433512731338851e-05 val_score : 3.110040779574774e-05\n",
      "==================================================\n",
      "Epoch 79\n",
      "train_loss : 0.19830956868827343 val_loss : 0.46650299429893494\n",
      "train_score : 4.4286149204708636e-05 val_score : 3.098775414400734e-05\n",
      "==================================================\n",
      "Epoch 80\n",
      "train_loss : 0.19723077304661274 val_loss : 0.4658856689929962\n",
      "train_score : 4.43492281192448e-05 val_score : 3.1134892196860164e-05\n",
      "==================================================\n",
      "Epoch 81\n",
      "train_loss : 0.19696789048612118 val_loss : 0.46723711490631104\n",
      "train_score : 4.441135752131231e-05 val_score : 3.11406038235873e-05\n",
      "==================================================\n",
      "Epoch 82\n",
      "train_loss : 0.19736281409859657 val_loss : 0.46716487407684326\n",
      "train_score : 4.443591387826018e-05 val_score : 3.104622373939492e-05\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83\n",
      "train_loss : 0.19711185805499554 val_loss : 0.46785271167755127\n",
      "train_score : 4.45273908553645e-05 val_score : 3.098283559666015e-05\n",
      "==================================================\n",
      "Epoch 84\n",
      "train_loss : 0.1966867782175541 val_loss : 0.4668912887573242\n",
      "train_score : 4.430282206158154e-05 val_score : 3.106538497377187e-05\n",
      "==================================================\n",
      "Epoch 85\n",
      "train_loss : 0.19751008041203022 val_loss : 0.4669138491153717\n",
      "train_score : 4.445459126145579e-05 val_score : 3.101911715930328e-05\n",
      "==================================================\n",
      "Epoch 86\n",
      "train_loss : 0.19705996476113796 val_loss : 0.46718424558639526\n",
      "train_score : 4.4359541789162904e-05 val_score : 3.109173121629283e-05\n",
      "==================================================\n",
      "Epoch 87\n",
      "train_loss : 0.19697227142751217 val_loss : 0.4675455093383789\n",
      "train_score : 4.427112435223535e-05 val_score : 3.11184958263766e-05\n",
      "==================================================\n",
      "Epoch 88\n",
      "train_loss : 0.19623480923473835 val_loss : 0.4675220549106598\n",
      "train_score : 4.4397242163540795e-05 val_score : 3.110440957243554e-05\n",
      "==================================================\n",
      "Epoch 89\n",
      "train_loss : 0.19556289166212082 val_loss : 0.468397319316864\n",
      "train_score : 4.432447531144135e-05 val_score : 3.107082739006728e-05\n",
      "==================================================\n",
      "Epoch 90\n",
      "train_loss : 0.19535658694803715 val_loss : 0.46795278787612915\n",
      "train_score : 4.444166552275419e-05 val_score : 3.1077375751920044e-05\n",
      "==================================================\n",
      "Epoch 91\n",
      "train_loss : 0.1960969865322113 val_loss : 0.4680813252925873\n",
      "train_score : 4.444899968802929e-05 val_score : 3.110410398221575e-05\n",
      "==================================================\n",
      "Epoch 92\n",
      "train_loss : 0.195296423509717 val_loss : 0.46824920177459717\n",
      "train_score : 4.4379354221746325e-05 val_score : 3.0963841709308326e-05\n",
      "==================================================\n",
      "Epoch 93\n",
      "train_loss : 0.19568958319723606 val_loss : 0.4687635898590088\n",
      "train_score : 4.431645720615052e-05 val_score : 3.105047653662041e-05\n",
      "==================================================\n",
      "Epoch 94\n",
      "train_loss : 0.1947543304413557 val_loss : 0.4691510498523712\n",
      "train_score : 4.43595927208662e-05 val_score : 3.095078136539087e-05\n",
      "==================================================\n",
      "Epoch 95\n",
      "train_loss : 0.1952146179974079 val_loss : 0.4681680500507355\n",
      "train_score : 4.448427353054285e-05 val_score : 3.103569179074839e-05\n",
      "==================================================\n",
      "Epoch 96\n",
      "train_loss : 0.1951116845011711 val_loss : 0.4692789316177368\n",
      "train_score : 4.4533226173371077e-05 val_score : 3.099254536209628e-05\n",
      "==================================================\n",
      "Epoch 97\n",
      "train_loss : 0.19620751217007637 val_loss : 0.468988835811615\n",
      "train_score : 4.436970266397111e-05 val_score : 3.110411125817336e-05\n",
      "==================================================\n",
      "Epoch 98\n",
      "train_loss : 0.19509977102279663 val_loss : 0.4679880738258362\n",
      "train_score : 4.45255427621305e-05 val_score : 3.114263017778285e-05\n",
      "==================================================\n",
      "Epoch 99\n",
      "train_loss : 0.1960641611367464 val_loss : 0.4698571264743805\n",
      "train_score : 4.442010686034337e-05 val_score : 3.102725895587355e-05\n",
      "==================================================\n",
      "Epoch 100\n",
      "train_loss : 0.1945275068283081 val_loss : 0.4686274230480194\n",
      "train_score : 4.440877819433808e-05 val_score : 3.0990839150035754e-05\n",
      "==================================================\n",
      "Epoch 101\n",
      "train_loss : 0.19637997448444366 val_loss : 0.46874335408210754\n",
      "train_score : 4.443127545528114e-05 val_score : 3.10881769109983e-05\n",
      "==================================================\n",
      "Epoch 102\n",
      "train_loss : 0.19512537494301796 val_loss : 0.4688018262386322\n",
      "train_score : 4.44127872469835e-05 val_score : 3.101185939158313e-05\n",
      "==================================================\n",
      "Epoch 103\n",
      "train_loss : 0.19477476552128792 val_loss : 0.468822181224823\n",
      "train_score : 4.446729872142896e-05 val_score : 3.119064786005765e-05\n",
      "==================================================\n",
      "Epoch 104\n",
      "train_loss : 0.194727823138237 val_loss : 0.47072848677635193\n",
      "train_score : 4.444221485755406e-05 val_score : 3.110658144578338e-05\n",
      "==================================================\n",
      "Epoch 105\n",
      "train_loss : 0.19487738609313965 val_loss : 0.4695320129394531\n",
      "train_score : 4.451557106222026e-05 val_score : 3.0997667636256665e-05\n",
      "==================================================\n",
      "Epoch 106\n",
      "train_loss : 0.19570531323552132 val_loss : 0.46964654326438904\n",
      "train_score : 4.4456985051510856e-05 val_score : 3.108644159510732e-05\n",
      "==================================================\n",
      "Epoch 107\n",
      "train_loss : 0.1948958784341812 val_loss : 0.469635933637619\n",
      "train_score : 4.4486070692073554e-05 val_score : 3.0954048270359635e-05\n",
      "==================================================\n",
      "Epoch 108\n",
      "train_loss : 0.19463963992893696 val_loss : 0.46986591815948486\n",
      "train_score : 4.44049401266966e-05 val_score : 3.106019357801415e-05\n",
      "==================================================\n",
      "Epoch 109\n",
      "train_loss : 0.19377708807587624 val_loss : 0.47032272815704346\n",
      "train_score : 4.450376218301244e-05 val_score : 3.106590884272009e-05\n",
      "==================================================\n",
      "Epoch 110\n",
      "train_loss : 0.19451630860567093 val_loss : 0.4700576663017273\n",
      "train_score : 4.43657991127111e-05 val_score : 3.1104682420846075e-05\n",
      "==================================================\n",
      "Epoch 111\n",
      "train_loss : 0.1938975639641285 val_loss : 0.4713938534259796\n",
      "train_score : 4.447913670446724e-05 val_score : 3.1081766792340204e-05\n",
      "==================================================\n",
      "Epoch 112\n",
      "train_loss : 0.19448747672140598 val_loss : 0.4696202278137207\n",
      "train_score : 4.4488297135103494e-05 val_score : 3.1048642995301634e-05\n",
      "==================================================\n",
      "Epoch 113\n",
      "train_loss : 0.19411709532141685 val_loss : 0.46976011991500854\n",
      "train_score : 4.440726479515433e-05 val_score : 3.1086517992662266e-05\n",
      "==================================================\n",
      "Epoch 114\n",
      "train_loss : 0.1942090354859829 val_loss : 0.47319087386131287\n",
      "train_score : 4.446137972990982e-05 val_score : 3.101485708612017e-05\n",
      "==================================================\n",
      "Epoch 115\n",
      "train_loss : 0.1942984890192747 val_loss : 0.4698209762573242\n",
      "train_score : 4.4534830522025004e-05 val_score : 3.095999272773042e-05\n",
      "==================================================\n",
      "Epoch 116\n",
      "train_loss : 0.19309775717556477 val_loss : 0.47103309631347656\n",
      "train_score : 4.4556720240507275e-05 val_score : 3.105562427663244e-05\n",
      "==================================================\n",
      "Epoch 117\n",
      "train_loss : 0.19437509402632713 val_loss : 0.47133690118789673\n",
      "train_score : 4.4574670027941465e-05 val_score : 3.1112111173570156e-05\n",
      "==================================================\n",
      "Epoch 118\n",
      "train_loss : 0.1922904159873724 val_loss : 0.4718899428844452\n",
      "train_score : 4.457069735508412e-05 val_score : 3.100114918197505e-05\n",
      "==================================================\n",
      "Epoch 119\n",
      "train_loss : 0.1930637639015913 val_loss : 0.473154217004776\n",
      "train_score : 4.4504446123028174e-05 val_score : 3.1071322155185044e-05\n",
      "==================================================\n",
      "Epoch 120\n",
      "train_loss : 0.19404160603880882 val_loss : 0.4707344174385071\n",
      "train_score : 4.4420623453333974e-05 val_score : 3.095080683124252e-05\n",
      "==================================================\n",
      "Epoch 121\n",
      "train_loss : 0.19248276576399803 val_loss : 0.4708688259124756\n",
      "train_score : 4.44895304099191e-05 val_score : 3.104702045675367e-05\n",
      "==================================================\n",
      "Epoch 122\n",
      "train_loss : 0.19269588217139244 val_loss : 0.47367799282073975\n",
      "train_score : 4.4515476474771276e-05 val_score : 3.1055933504831046e-05\n",
      "==================================================\n",
      "Epoch 123\n",
      "train_loss : 0.19252781197428703 val_loss : 0.47214066982269287\n",
      "train_score : 4.4516527850646526e-05 val_score : 3.0976265406934544e-05\n",
      "==================================================\n",
      "Epoch 124\n",
      "train_loss : 0.1917253267019987 val_loss : 0.47173070907592773\n",
      "train_score : 4.460884883883409e-05 val_score : 3.106711301370524e-05\n",
      "==================================================\n",
      "Epoch 125\n",
      "train_loss : 0.19225249998271465 val_loss : 0.4732310473918915\n",
      "train_score : 4.451753193279728e-05 val_score : 3.0960778531152755e-05\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 126\n",
      "train_loss : 0.1920426432043314 val_loss : 0.4730525314807892\n",
      "train_score : 4.4555006752489135e-05 val_score : 3.111458136118017e-05\n",
      "==================================================\n",
      "Epoch 127\n",
      "train_loss : 0.19393381103873253 val_loss : 0.47274911403656006\n",
      "train_score : 4.439914846443571e-05 val_score : 3.1089221010915935e-05\n",
      "==================================================\n",
      "Epoch 128\n",
      "train_loss : 0.19303688034415245 val_loss : 0.4728095829486847\n",
      "train_score : 4.4595799408853054e-05 val_score : 3.097558146691881e-05\n",
      "==================================================\n",
      "Epoch 129\n",
      "train_loss : 0.19196996092796326 val_loss : 0.4721404016017914\n",
      "train_score : 4.464348603505641e-05 val_score : 3.106862277491018e-05\n",
      "==================================================\n",
      "Epoch 130\n",
      "train_loss : 0.19245866499841213 val_loss : 0.4742448031902313\n",
      "train_score : 4.456733222468756e-05 val_score : 3.100502726738341e-05\n",
      "==================================================\n",
      "Epoch 131\n",
      "train_loss : 0.1914099119603634 val_loss : 0.47274765372276306\n",
      "train_score : 4.4690430513583124e-05 val_score : 3.103832568740472e-05\n",
      "==================================================\n",
      "Epoch 132\n",
      "train_loss : 0.19231132976710796 val_loss : 0.47444602847099304\n",
      "train_score : 4.4527223508339375e-05 val_score : 3.099054083577357e-05\n",
      "==================================================\n",
      "Epoch 133\n",
      "train_loss : 0.19163517467677593 val_loss : 0.47255444526672363\n",
      "train_score : 4.460047057364136e-05 val_score : 3.098723755101673e-05\n",
      "==================================================\n",
      "Epoch 134\n",
      "train_loss : 0.19160956144332886 val_loss : 0.47392967343330383\n",
      "train_score : 4.459460615180433e-05 val_score : 3.10313371301163e-05\n",
      "==================================================\n",
      "Epoch 135\n",
      "train_loss : 0.19093051552772522 val_loss : 0.4739863872528076\n",
      "train_score : 4.471312786336057e-05 val_score : 3.103043491137214e-05\n",
      "==================================================\n",
      "Epoch 136\n",
      "train_loss : 0.19070491008460522 val_loss : 0.47440117597579956\n",
      "train_score : 4.467435064725578e-05 val_score : 3.1025687349028885e-05\n",
      "==================================================\n",
      "Epoch 137\n",
      "train_loss : 0.1928088366985321 val_loss : 0.47456035017967224\n",
      "train_score : 4.462850120035e-05 val_score : 3.098617889918387e-05\n",
      "==================================================\n",
      "Epoch 138\n",
      "train_loss : 0.19207589142024517 val_loss : 0.4717284142971039\n",
      "train_score : 4.4577522203326225e-05 val_score : 3.0991814128356054e-05\n",
      "==================================================\n",
      "Epoch 139\n",
      "train_loss : 0.19234830886125565 val_loss : 0.47546911239624023\n",
      "train_score : 4.459193587535992e-05 val_score : 3.0979143048170954e-05\n",
      "==================================================\n",
      "Epoch 140\n",
      "train_loss : 0.19042318314313889 val_loss : 0.4740396738052368\n",
      "train_score : 4.4731084926752374e-05 val_score : 3.1030642276164144e-05\n",
      "==================================================\n",
      "Epoch 141\n",
      "train_loss : 0.19143198430538177 val_loss : 0.4742555022239685\n",
      "train_score : 4.459535557543859e-05 val_score : 3.097716034972109e-05\n",
      "==================================================\n",
      "Epoch 142\n",
      "train_loss : 0.19172772020101547 val_loss : 0.47498372197151184\n",
      "train_score : 4.467421240406111e-05 val_score : 3.0950646760175005e-05\n",
      "==================================================\n",
      "Epoch 143\n",
      "train_loss : 0.19107542373239994 val_loss : 0.47382229566574097\n",
      "train_score : 4.449124025995843e-05 val_score : 3.096422733506188e-05\n",
      "==================================================\n",
      "Epoch 144\n",
      "train_loss : 0.19080249220132828 val_loss : 0.4750363528728485\n",
      "train_score : 4.4653563236352056e-05 val_score : 3.0944567697588354e-05\n",
      "==================================================\n",
      "Epoch 145\n",
      "train_loss : 0.18952508829534054 val_loss : 0.47504618763923645\n",
      "train_score : 4.4766245991922915e-05 val_score : 3.0975199479144067e-05\n",
      "==================================================\n",
      "Epoch 146\n",
      "train_loss : 0.1900823973119259 val_loss : 0.4750516712665558\n",
      "train_score : 4.4637588871410117e-05 val_score : 3.095702049904503e-05\n",
      "==================================================\n",
      "Epoch 147\n",
      "train_loss : 0.1897379271686077 val_loss : 0.4762565493583679\n",
      "train_score : 4.4733973481925204e-05 val_score : 3.09531569655519e-05\n",
      "==================================================\n",
      "Epoch 148\n",
      "train_loss : 0.19110405258834362 val_loss : 0.4748062789440155\n",
      "train_score : 4.47143756900914e-05 val_score : 3.103256312897429e-05\n",
      "==================================================\n",
      "Epoch 149\n",
      "train_loss : 0.19010203331708908 val_loss : 0.47693419456481934\n",
      "train_score : 4.462285141926259e-05 val_score : 3.1011713872430846e-05\n",
      "==================================================\n",
      "Epoch 150\n",
      "train_loss : 0.1900100614875555 val_loss : 0.4741933345794678\n",
      "train_score : 4.467785402084701e-05 val_score : 3.1134830351220444e-05\n",
      "==================================================\n",
      "Epoch 151\n",
      "train_loss : 0.19104046747088432 val_loss : 0.4778839945793152\n",
      "train_score : 4.469656050787307e-05 val_score : 3.0806379072600976e-05\n",
      "==================================================\n",
      "Epoch 152\n",
      "train_loss : 0.18937402963638306 val_loss : 0.47543275356292725\n",
      "train_score : 4.4759439333574846e-05 val_score : 3.104181814705953e-05\n",
      "==================================================\n",
      "Epoch 153\n",
      "train_loss : 0.18947656452655792 val_loss : 0.4749862253665924\n",
      "train_score : 4.4779353629564866e-05 val_score : 3.1087871320778504e-05\n",
      "==================================================\n",
      "Epoch 154\n",
      "train_loss : 0.18886489793658257 val_loss : 0.4767777919769287\n",
      "train_score : 4.4683049054583535e-05 val_score : 3.0980441806605086e-05\n",
      "==================================================\n",
      "Epoch 155\n",
      "train_loss : 0.18916067853569984 val_loss : 0.476255863904953\n",
      "train_score : 4.480611460166983e-05 val_score : 3.104545976384543e-05\n",
      "==================================================\n",
      "Epoch 156\n",
      "train_loss : 0.18987993523478508 val_loss : 0.4763633906841278\n",
      "train_score : 4.465534948394634e-05 val_score : 3.1021023460198194e-05\n",
      "==================================================\n",
      "Epoch 157\n",
      "train_loss : 0.1883409060537815 val_loss : 0.4792725741863251\n",
      "train_score : 4.479218114283867e-05 val_score : 3.0884799343766645e-05\n",
      "==================================================\n",
      "Epoch 158\n",
      "train_loss : 0.18875516392290592 val_loss : 0.4774819612503052\n",
      "train_score : 4.471701322472654e-05 val_score : 3.0861057894071564e-05\n",
      "==================================================\n",
      "Epoch 159\n",
      "train_loss : 0.1898894365876913 val_loss : 0.4762450158596039\n",
      "train_score : 4.4706575863529e-05 val_score : 3.0984858312876895e-05\n",
      "==================================================\n",
      "Epoch 160\n",
      "train_loss : 0.18803068436682224 val_loss : 0.4768114686012268\n",
      "train_score : 4.476112735574134e-05 val_score : 3.088927405769937e-05\n",
      "==================================================\n",
      "Epoch 161\n",
      "train_loss : 0.18872027099132538 val_loss : 0.47826239466667175\n",
      "train_score : 4.472646105568856e-05 val_score : 3.094378189416602e-05\n",
      "==================================================\n",
      "Epoch 162\n",
      "train_loss : 0.18860811553895473 val_loss : 0.47779250144958496\n",
      "train_score : 4.480380812310614e-05 val_score : 3.099417153862305e-05\n",
      "==================================================\n",
      "Epoch 163\n",
      "train_loss : 0.1879013478755951 val_loss : 0.4778938889503479\n",
      "train_score : 4.4844651711173356e-05 val_score : 3.093589111813344e-05\n",
      "==================================================\n",
      "Epoch 164\n",
      "train_loss : 0.1885144729167223 val_loss : 0.47682127356529236\n",
      "train_score : 4.475849709706381e-05 val_score : 3.094901330769062e-05\n",
      "==================================================\n",
      "Epoch 165\n",
      "train_loss : 0.18756582215428352 val_loss : 0.4778982698917389\n",
      "train_score : 4.4814180000685155e-05 val_score : 3.097069202340208e-05\n",
      "==================================================\n",
      "Epoch 166\n",
      "train_loss : 0.18829281255602837 val_loss : 0.47992411255836487\n",
      "train_score : 4.4788535888073966e-05 val_score : 3.08498929371126e-05\n",
      "==================================================\n",
      "Epoch 167\n",
      "train_loss : 0.1885584108531475 val_loss : 0.477356493473053\n",
      "train_score : 4.4754349801223725e-05 val_score : 3.095470674452372e-05\n",
      "==================================================\n",
      "Epoch 168\n",
      "train_loss : 0.18684653006494045 val_loss : 0.4798746705055237\n",
      "train_score : 4.477976108319126e-05 val_score : 3.0868213798385113e-05\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 169\n",
      "train_loss : 0.18648160621523857 val_loss : 0.4784817695617676\n",
      "train_score : 4.4817832531407475e-05 val_score : 3.1077048333827406e-05\n",
      "==================================================\n",
      "Epoch 170\n",
      "train_loss : 0.18793990835547447 val_loss : 0.4792412519454956\n",
      "train_score : 4.4721651647705585e-05 val_score : 3.103348353761248e-05\n",
      "==================================================\n",
      "Epoch 171\n",
      "train_loss : 0.18737865798175335 val_loss : 0.4788021147251129\n",
      "train_score : 4.4779168092645705e-05 val_score : 3.094262501690537e-05\n",
      "==================================================\n",
      "Epoch 172\n",
      "train_loss : 0.1869767438620329 val_loss : 0.4792070686817169\n",
      "train_score : 4.4839900510851294e-05 val_score : 3.090429163421504e-05\n",
      "==================================================\n",
      "Epoch 173\n",
      "train_loss : 0.18728677928447723 val_loss : 0.480108380317688\n",
      "train_score : 4.477657421375625e-05 val_score : 3.0850813345750794e-05\n",
      "==================================================\n",
      "Epoch 174\n",
      "train_loss : 0.18722187168896198 val_loss : 0.47892147302627563\n",
      "train_score : 4.477184120332822e-05 val_score : 3.1059487810125574e-05\n",
      "==================================================\n",
      "Epoch 175\n",
      "train_loss : 0.18725388683378696 val_loss : 0.47764351963996887\n",
      "train_score : 4.474464731174521e-05 val_score : 3.112575723207556e-05\n",
      "==================================================\n",
      "Epoch 176\n",
      "train_loss : 0.18610489927232265 val_loss : 0.48039087653160095\n",
      "train_score : 4.488502599997446e-05 val_score : 3.0974289984442294e-05\n",
      "==================================================\n",
      "Epoch 177\n",
      "train_loss : 0.18776937015354633 val_loss : 0.4804967939853668\n",
      "train_score : 4.470639396458864e-05 val_score : 3.0864084692439064e-05\n",
      "==================================================\n",
      "Epoch 178\n",
      "train_loss : 0.18536952137947083 val_loss : 0.48009079694747925\n",
      "train_score : 4.4903496018378064e-05 val_score : 3.0845469154883176e-05\n",
      "==================================================\n",
      "Epoch 179\n",
      "train_loss : 0.1864098161458969 val_loss : 0.4814422130584717\n",
      "train_score : 4.4751923269359395e-05 val_score : 3.10885370709002e-05\n",
      "==================================================\n",
      "Epoch 180\n",
      "train_loss : 0.18572070635855198 val_loss : 0.48019304871559143\n",
      "train_score : 4.4741740566678345e-05 val_score : 3.102606569882482e-05\n",
      "==================================================\n",
      "Epoch 181\n",
      "train_loss : 0.1855762768536806 val_loss : 0.48042240738868713\n",
      "train_score : 4.47988641099073e-05 val_score : 3.090329119004309e-05\n",
      "==================================================\n",
      "Epoch 182\n",
      "train_loss : 0.18391048535704613 val_loss : 0.48395150899887085\n",
      "train_score : 4.4912238081451505e-05 val_score : 3.090738391620107e-05\n",
      "==================================================\n",
      "Epoch 183\n",
      "train_loss : 0.18610318191349506 val_loss : 0.4802388846874237\n",
      "train_score : 4.488120976020582e-05 val_score : 3.090437894570641e-05\n",
      "==================================================\n",
      "Epoch 184\n",
      "train_loss : 0.18659569136798382 val_loss : 0.48282408714294434\n",
      "train_score : 4.485635508899577e-05 val_score : 3.099927198491059e-05\n",
      "==================================================\n",
      "Epoch 185\n",
      "train_loss : 0.1840804796665907 val_loss : 0.48080894351005554\n",
      "train_score : 4.498437192523852e-05 val_score : 3.09278148051817e-05\n",
      "==================================================\n",
      "Epoch 186\n",
      "train_loss : 0.18596445210278034 val_loss : 0.48160701990127563\n",
      "train_score : 4.488747799769044e-05 val_score : 3.0864182917866856e-05\n",
      "==================================================\n",
      "Epoch 187\n",
      "train_loss : 0.1854190807789564 val_loss : 0.4818657338619232\n",
      "train_score : 4.490587889449671e-05 val_score : 3.097310764132999e-05\n",
      "==================================================\n",
      "Epoch 188\n",
      "train_loss : 0.1842588074505329 val_loss : 0.4806597828865051\n",
      "train_score : 4.496784822549671e-05 val_score : 3.102063783444464e-05\n",
      "==================================================\n",
      "Epoch 189\n",
      "train_loss : 0.18581091612577438 val_loss : 0.4827848970890045\n",
      "train_score : 4.488538979785517e-05 val_score : 3.093131090281531e-05\n",
      "==================================================\n",
      "Epoch 190\n",
      "train_loss : 0.18527496047317982 val_loss : 0.4800674319267273\n",
      "train_score : 4.4885695388074964e-05 val_score : 3.093181294389069e-05\n",
      "==================================================\n",
      "Epoch 191\n",
      "train_loss : 0.18517677672207355 val_loss : 0.48269662261009216\n",
      "train_score : 4.49136205133982e-05 val_score : 3.105326322838664e-05\n",
      "==================================================\n",
      "Epoch 192\n",
      "train_loss : 0.18410981260240078 val_loss : 0.4835931360721588\n",
      "train_score : 4.49135186499916e-05 val_score : 3.084003765252419e-05\n",
      "==================================================\n",
      "Epoch 193\n",
      "train_loss : 0.18427086621522903 val_loss : 0.48219701647758484\n",
      "train_score : 4.4877928303321823e-05 val_score : 3.087070945184678e-05\n",
      "==================================================\n",
      "Epoch 194\n",
      "train_loss : 0.18437991105020046 val_loss : 0.4816834628582001\n",
      "train_score : 4.479939889279194e-05 val_score : 3.101406036876142e-05\n",
      "==================================================\n",
      "Epoch 195\n",
      "train_loss : 0.1842246986925602 val_loss : 0.48268744349479675\n",
      "train_score : 4.4991909817326814e-05 val_score : 3.095580905210227e-05\n",
      "==================================================\n",
      "Epoch 196\n",
      "train_loss : 0.18435241095721722 val_loss : 0.48279473185539246\n",
      "train_score : 4.501085641095415e-05 val_score : 3.094613566645421e-05\n",
      "==================================================\n",
      "Epoch 197\n",
      "train_loss : 0.1839799452573061 val_loss : 0.48333093523979187\n",
      "train_score : 4.493645974434912e-05 val_score : 3.098512388532981e-05\n",
      "==================================================\n",
      "Epoch 198\n",
      "train_loss : 0.1840404625982046 val_loss : 0.4853890538215637\n",
      "train_score : 4.488487684284337e-05 val_score : 3.090236714342609e-05\n",
      "==================================================\n",
      "Epoch 199\n",
      "train_loss : 0.18397311680018902 val_loss : 0.48137834668159485\n",
      "train_score : 4.4856737076770514e-05 val_score : 3.090763493673876e-05\n",
      "==================================================\n",
      "Epoch 200\n",
      "train_loss : 0.18323315866291523 val_loss : 0.48567962646484375\n",
      "train_score : 4.505004471866414e-05 val_score : 3.0892617360223085e-05\n",
      "==================================================\n",
      "Epoch 201\n",
      "train_loss : 0.1833822950720787 val_loss : 0.4819527566432953\n",
      "train_score : 4.4977026846027e-05 val_score : 3.095835927524604e-05\n",
      "==================================================\n",
      "Epoch 202\n",
      "train_loss : 0.18447507359087467 val_loss : 0.48349708318710327\n",
      "train_score : 4.4986267312197015e-05 val_score : 3.1009101803647354e-05\n",
      "==================================================\n",
      "Epoch 203\n",
      "train_loss : 0.18385875970125198 val_loss : 0.4859848916530609\n",
      "train_score : 4.49979561381042e-05 val_score : 3.0942865123506635e-05\n",
      "==================================================\n",
      "Epoch 204\n",
      "train_loss : 0.1831892542541027 val_loss : 0.4836726486682892\n",
      "train_score : 4.4997123040957376e-05 val_score : 3.0972460081102327e-05\n",
      "==================================================\n",
      "Epoch 205\n",
      "train_loss : 0.18303122743964195 val_loss : 0.48223400115966797\n",
      "train_score : 4.491385698202066e-05 val_score : 3.0971536034485325e-05\n",
      "==================================================\n",
      "Epoch 206\n",
      "train_loss : 0.18200550973415375 val_loss : 0.48617520928382874\n",
      "train_score : 4.5058968680677935e-05 val_score : 3.091325197601691e-05\n",
      "==================================================\n",
      "Epoch 207\n",
      "train_loss : 0.1813848465681076 val_loss : 0.4832102656364441\n",
      "train_score : 4.510378130362369e-05 val_score : 3.1016443244880065e-05\n",
      "==================================================\n",
      "Epoch 208\n",
      "train_loss : 0.1826541442424059 val_loss : 0.484439492225647\n",
      "train_score : 4.509813879849389e-05 val_score : 3.096246291534044e-05\n",
      "==================================================\n",
      "Epoch 209\n",
      "train_loss : 0.18240267224609852 val_loss : 0.48507827520370483\n",
      "train_score : 4.5019303797744215e-05 val_score : 3.093403211096302e-05\n",
      "==================================================\n",
      "Epoch 210\n",
      "train_loss : 0.18299774639308453 val_loss : 0.48364776372909546\n",
      "train_score : 4.506950790528208e-05 val_score : 3.093554551014677e-05\n",
      "==================================================\n",
      "Epoch 211\n",
      "train_loss : 0.18286139518022537 val_loss : 0.48512399196624756\n",
      "train_score : 4.486940451897681e-05 val_score : 3.093632767559029e-05\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 212\n",
      "train_loss : 0.18214849941432476 val_loss : 0.485166996717453\n",
      "train_score : 4.502430238062516e-05 val_score : 3.089631354669109e-05\n",
      "==================================================\n",
      "Epoch 213\n",
      "train_loss : 0.18205120041966438 val_loss : 0.4870918393135071\n",
      "train_score : 4.502575029619038e-05 val_score : 3.092112820013426e-05\n",
      "==================================================\n",
      "Epoch 214\n",
      "train_loss : 0.18152406439185143 val_loss : 0.4840904176235199\n",
      "train_score : 4.507183621171862e-05 val_score : 3.08746712107677e-05\n",
      "==================================================\n",
      "Epoch 215\n",
      "train_loss : 0.18275990895926952 val_loss : 0.48704996705055237\n",
      "train_score : 4.491646177484654e-05 val_score : 3.098836896242574e-05\n",
      "==================================================\n",
      "Epoch 216\n",
      "train_loss : 0.18076981231570244 val_loss : 0.4862353205680847\n",
      "train_score : 4.510317376116291e-05 val_score : 3.0908689950592816e-05\n",
      "==================================================\n",
      "Epoch 217\n",
      "train_loss : 0.1820842456072569 val_loss : 0.4851098656654358\n",
      "train_score : 4.5103726733941585e-05 val_score : 3.092552287853323e-05\n",
      "==================================================\n",
      "Epoch 218\n",
      "train_loss : 0.18055866099894047 val_loss : 0.4876234829425812\n",
      "train_score : 4.502373849391006e-05 val_score : 3.090263999183662e-05\n",
      "==================================================\n",
      "Epoch 219\n",
      "train_loss : 0.18018272519111633 val_loss : 0.4869954288005829\n",
      "train_score : 4.522036397247575e-05 val_score : 3.0911756766727194e-05\n",
      "==================================================\n",
      "Epoch 220\n",
      "train_loss : 0.18076073564589024 val_loss : 0.4857870638370514\n",
      "train_score : 4.5118849811842665e-05 val_score : 3.094442217843607e-05\n",
      "==================================================\n",
      "Epoch 221\n",
      "train_loss : 0.18054517917335033 val_loss : 0.48711782693862915\n",
      "train_score : 4.5140728616388515e-05 val_score : 3.088249286520295e-05\n",
      "==================================================\n",
      "Epoch 222\n",
      "train_loss : 0.18033957853913307 val_loss : 0.48650312423706055\n",
      "train_score : 4.5112934458302334e-05 val_score : 3.0877556127961725e-05\n",
      "==================================================\n",
      "Epoch 223\n",
      "train_loss : 0.18117217160761356 val_loss : 0.48511430621147156\n",
      "train_score : 4.511136648943648e-05 val_score : 3.080104579566978e-05\n",
      "==================================================\n",
      "Epoch 224\n",
      "train_loss : 0.18054801784455776 val_loss : 0.4875071942806244\n",
      "train_score : 4.5081222197040915e-05 val_score : 3.094636485911906e-05\n",
      "==================================================\n",
      "Epoch 225\n",
      "train_loss : 0.17899292521178722 val_loss : 0.4879854917526245\n",
      "train_score : 4.5169072109274566e-05 val_score : 3.085679418290965e-05\n",
      "==================================================\n",
      "Epoch 226\n",
      "train_loss : 0.17971301078796387 val_loss : 0.48622792959213257\n",
      "train_score : 4.5170549128670245e-05 val_score : 3.082560215261765e-05\n",
      "==================================================\n",
      "Epoch 227\n",
      "train_loss : 0.1803810726851225 val_loss : 0.4911443293094635\n",
      "train_score : 4.504945536609739e-05 val_score : 3.0802450055489317e-05\n",
      "==================================================\n",
      "Epoch 228\n",
      "train_loss : 0.18017769418656826 val_loss : 0.48610150814056396\n",
      "train_score : 4.512232408160344e-05 val_score : 3.091891630901955e-05\n",
      "==================================================\n",
      "Epoch 229\n",
      "train_loss : 0.17943931929767132 val_loss : 0.4887731373310089\n",
      "train_score : 4.517785782809369e-05 val_score : 3.086342985625379e-05\n",
      "==================================================\n",
      "Epoch 230\n",
      "train_loss : 0.17998497560620308 val_loss : 0.48733779788017273\n",
      "train_score : 4.518962305155583e-05 val_score : 3.0949144274927676e-05\n",
      "==================================================\n",
      "Epoch 231\n",
      "train_loss : 0.17942810989916325 val_loss : 0.4870362877845764\n",
      "train_score : 4.515007458394393e-05 val_score : 3.080460737692192e-05\n",
      "==================================================\n",
      "Epoch 232\n",
      "train_loss : 0.1798467580229044 val_loss : 0.4902699291706085\n",
      "train_score : 4.5050775952404365e-05 val_score : 3.092903716606088e-05\n",
      "==================================================\n",
      "Epoch 233\n",
      "train_loss : 0.17988867312669754 val_loss : 0.4903712272644043\n",
      "train_score : 4.51495434390381e-05 val_score : 3.085503340116702e-05\n",
      "==================================================\n",
      "Epoch 234\n",
      "train_loss : 0.17992013692855835 val_loss : 0.4854390323162079\n",
      "train_score : 4.524692485574633e-05 val_score : 3.078775989706628e-05\n",
      "==================================================\n",
      "Epoch 235\n",
      "train_loss : 0.17935686744749546 val_loss : 0.48899099230766296\n",
      "train_score : 4.5184911869000643e-05 val_score : 3.085930075030774e-05\n",
      "==================================================\n",
      "Epoch 236\n",
      "train_loss : 0.17904049344360828 val_loss : 0.48829182982444763\n",
      "train_score : 4.521170194493607e-05 val_score : 3.093669511144981e-05\n",
      "==================================================\n",
      "Epoch 237\n",
      "train_loss : 0.17892300710082054 val_loss : 0.4891929030418396\n",
      "train_score : 4.52580425189808e-05 val_score : 3.089967503910884e-05\n",
      "==================================================\n",
      "Epoch 238\n",
      "train_loss : 0.17796574346721172 val_loss : 0.49143919348716736\n",
      "train_score : 4.517521301750094e-05 val_score : 3.085785647272132e-05\n",
      "==================================================\n",
      "Epoch 239\n",
      "train_loss : 0.17895300313830376 val_loss : 0.48895955085754395\n",
      "train_score : 4.52060230600182e-05 val_score : 3.081496834056452e-05\n",
      "==================================================\n",
      "Epoch 240\n",
      "train_loss : 0.17827271111309528 val_loss : 0.4896881878376007\n",
      "train_score : 4.5252192649059e-05 val_score : 3.09777824440971e-05\n",
      "==================================================\n",
      "Epoch 241\n",
      "train_loss : 0.17880160175263882 val_loss : 0.4915374517440796\n",
      "train_score : 4.5337306801229715e-05 val_score : 3.0852937925374135e-05\n",
      "==================================================\n",
      "Epoch 242\n",
      "train_loss : 0.1787442434579134 val_loss : 0.4907539486885071\n",
      "train_score : 4.518168861977756e-05 val_score : 3.07604786939919e-05\n",
      "==================================================\n",
      "Epoch 243\n",
      "train_loss : 0.17786220461130142 val_loss : 0.48847800493240356\n",
      "train_score : 4.5317254262045026e-05 val_score : 3.0912993679521605e-05\n",
      "==================================================\n",
      "Epoch 244\n",
      "train_loss : 0.1778196133673191 val_loss : 0.49078771471977234\n",
      "train_score : 4.53760803793557e-05 val_score : 3.084768104599789e-05\n",
      "==================================================\n",
      "Epoch 245\n",
      "train_loss : 0.17783023230731487 val_loss : 0.4894156754016876\n",
      "train_score : 4.524345422396436e-05 val_score : 3.08296293951571e-05\n",
      "==================================================\n",
      "Epoch 246\n",
      "train_loss : 0.17709139734506607 val_loss : 0.4916689991950989\n",
      "train_score : 4.526938937488012e-05 val_score : 3.091965481871739e-05\n",
      "==================================================\n",
      "Epoch 247\n",
      "train_loss : 0.17787358351051807 val_loss : 0.4887043833732605\n",
      "train_score : 4.5237597078084946e-05 val_score : 3.079860471189022e-05\n",
      "==================================================\n",
      "Epoch 248\n",
      "train_loss : 0.17703534290194511 val_loss : 0.49250224232673645\n",
      "train_score : 4.526882912614383e-05 val_score : 3.080733586102724e-05\n",
      "==================================================\n",
      "Epoch 249\n",
      "train_loss : 0.1773631889373064 val_loss : 0.4895097315311432\n",
      "train_score : 4.537383574643172e-05 val_score : 3.0909577617421746e-05\n",
      "==================================================\n",
      "Epoch 250\n",
      "train_loss : 0.17643084190785885 val_loss : 0.4949174225330353\n",
      "train_score : 4.535297557595186e-05 val_score : 3.0749550205655396e-05\n",
      "==================================================\n",
      "Epoch 251\n",
      "train_loss : 0.1772992629557848 val_loss : 0.49053671956062317\n",
      "train_score : 4.5312164729693905e-05 val_score : 3.081205068156123e-05\n",
      "==================================================\n",
      "Epoch 252\n",
      "train_loss : 0.17631560750305653 val_loss : 0.49505603313446045\n",
      "train_score : 4.5334090827964246e-05 val_score : 3.091770486207679e-05\n",
      "==================================================\n",
      "Epoch 253\n",
      "train_loss : 0.1764744110405445 val_loss : 0.4894518256187439\n",
      "train_score : 4.5313554437598214e-05 val_score : 3.0810293537797406e-05\n",
      "==================================================\n",
      "Epoch 254\n",
      "train_loss : 0.17650178261101246 val_loss : 0.49241605401039124\n",
      "train_score : 4.534285471891053e-05 val_score : 3.0847222660668194e-05\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 255\n",
      "train_loss : 0.1768688950687647 val_loss : 0.49205198884010315\n",
      "train_score : 4.535511470749043e-05 val_score : 3.0878167308401316e-05\n",
      "==================================================\n",
      "Epoch 256\n",
      "train_loss : 0.17639952339231968 val_loss : 0.49525678157806396\n",
      "train_score : 4.5345899707172066e-05 val_score : 3.09482857119292e-05\n",
      "==================================================\n",
      "Epoch 257\n",
      "train_loss : 0.17649554088711739 val_loss : 0.49325960874557495\n",
      "train_score : 4.5368855353444815e-05 val_score : 3.0807605071458966e-05\n",
      "==================================================\n",
      "Epoch 258\n",
      "train_loss : 0.1766907051205635 val_loss : 0.49380508065223694\n",
      "train_score : 4.534814434009604e-05 val_score : 3.090887184953317e-05\n",
      "==================================================\n",
      "Epoch 259\n",
      "train_loss : 0.17550713941454887 val_loss : 0.49325302243232727\n",
      "train_score : 4.539580913842656e-05 val_score : 3.090819882345386e-05\n",
      "==================================================\n",
      "Epoch 260\n",
      "train_loss : 0.17600986547768116 val_loss : 0.4933716952800751\n",
      "train_score : 4.532675302471034e-05 val_score : 3.0660849006380886e-05\n",
      "==================================================\n",
      "Epoch 261\n",
      "train_loss : 0.1753797996789217 val_loss : 0.493501752614975\n",
      "train_score : 4.538522262009792e-05 val_score : 3.085142088821158e-05\n",
      "==================================================\n",
      "Epoch 262\n",
      "train_loss : 0.17508593015372753 val_loss : 0.49271073937416077\n",
      "train_score : 4.544063995126635e-05 val_score : 3.081638351432048e-05\n",
      "==================================================\n",
      "Epoch 263\n",
      "train_loss : 0.17631448805332184 val_loss : 0.49509233236312866\n",
      "train_score : 4.539658402791247e-05 val_score : 3.0843842978356406e-05\n",
      "==================================================\n",
      "Epoch 264\n",
      "train_loss : 0.17454915679991245 val_loss : 0.49713361263275146\n",
      "train_score : 4.541575981420465e-05 val_score : 3.066951467189938e-05\n",
      "==================================================\n",
      "Epoch 265\n",
      "train_loss : 0.17464555241167545 val_loss : 0.49199843406677246\n",
      "train_score : 4.538243229035288e-05 val_score : 3.0897495889803395e-05\n",
      "==================================================\n",
      "Epoch 266\n",
      "train_loss : 0.1748936027288437 val_loss : 0.4961185157299042\n",
      "train_score : 4.534229447017424e-05 val_score : 3.089851088589057e-05\n",
      "==================================================\n",
      "Epoch 267\n",
      "train_loss : 0.17482183314859867 val_loss : 0.49483269453048706\n",
      "train_score : 4.539515430224128e-05 val_score : 3.0667932151118293e-05\n",
      "==================================================\n",
      "Epoch 268\n",
      "train_loss : 0.17471932619810104 val_loss : 0.49466100335121155\n",
      "train_score : 4.548989818431437e-05 val_score : 3.095101783401333e-05\n",
      "==================================================\n",
      "Epoch 269\n",
      "train_loss : 0.17475731298327446 val_loss : 0.49467894434928894\n",
      "train_score : 4.546415584627539e-05 val_score : 3.084324998781085e-05\n",
      "==================================================\n",
      "Epoch 270\n",
      "train_loss : 0.17366859503090382 val_loss : 0.49302729964256287\n",
      "train_score : 4.548678771243431e-05 val_score : 3.086397919105366e-05\n",
      "==================================================\n",
      "Epoch 271\n",
      "train_loss : 0.17400329932570457 val_loss : 0.4934666156768799\n",
      "train_score : 4.5552238589152694e-05 val_score : 3.070213278988376e-05\n",
      "==================================================\n",
      "Epoch 272\n",
      "train_loss : 0.17435326240956783 val_loss : 0.49597373604774475\n",
      "train_score : 4.55486006103456e-05 val_score : 3.089675374212675e-05\n",
      "==================================================\n",
      "Epoch 273\n",
      "train_loss : 0.17356682941317558 val_loss : 0.49852830171585083\n",
      "train_score : 4.542990427580662e-05 val_score : 3.07950031128712e-05\n",
      "==================================================\n",
      "Epoch 274\n",
      "train_loss : 0.1730168703943491 val_loss : 0.49572470784187317\n",
      "train_score : 4.551917299977504e-05 val_score : 3.0868101021042094e-05\n",
      "==================================================\n",
      "Epoch 275\n",
      "train_loss : 0.17349703423678875 val_loss : 0.4985169768333435\n",
      "train_score : 4.556596832117066e-05 val_score : 3.08892922475934e-05\n",
      "==================================================\n",
      "Epoch 276\n",
      "train_loss : 0.17202698066830635 val_loss : 0.4958413243293762\n",
      "train_score : 4.554793849820271e-05 val_score : 3.0919974960852414e-05\n",
      "==================================================\n",
      "Epoch 277\n",
      "train_loss : 0.17372815310955048 val_loss : 0.4968048632144928\n",
      "train_score : 4.551000893115997e-05 val_score : 3.0781047826167196e-05\n",
      "==================================================\n",
      "Epoch 278\n",
      "train_loss : 0.17362339980900288 val_loss : 0.4960605502128601\n",
      "train_score : 4.5471864723367617e-05 val_score : 3.090503014391288e-05\n",
      "==================================================\n",
      "Epoch 279\n",
      "train_loss : 0.17215774953365326 val_loss : 0.4965842366218567\n",
      "train_score : 4.5625565689988434e-05 val_score : 3.0814422643743455e-05\n",
      "==================================================\n",
      "Epoch 280\n",
      "train_loss : 0.17251666821539402 val_loss : 0.49779778718948364\n",
      "train_score : 4.5537690311903134e-05 val_score : 3.09282768284902e-05\n",
      "==================================================\n",
      "Epoch 281\n",
      "train_loss : 0.1731396410614252 val_loss : 0.49835535883903503\n",
      "train_score : 4.548842116491869e-05 val_score : 3.082141120103188e-05\n",
      "==================================================\n",
      "Epoch 282\n",
      "train_loss : 0.17140957154333591 val_loss : 0.4982319474220276\n",
      "train_score : 4.5505341404350474e-05 val_score : 3.0808580049779266e-05\n",
      "==================================================\n",
      "Epoch 283\n",
      "train_loss : 0.17222641967236996 val_loss : 0.4975424110889435\n",
      "train_score : 4.550660014501773e-05 val_score : 3.08965754811652e-05\n",
      "==================================================\n",
      "Epoch 284\n",
      "train_loss : 0.17186955735087395 val_loss : 0.4964972138404846\n",
      "train_score : 4.544095281744376e-05 val_score : 3.0851912015350536e-05\n",
      "==================================================\n",
      "Epoch 285\n",
      "train_loss : 0.17102962546050549 val_loss : 0.4991462528705597\n",
      "train_score : 4.555256964522414e-05 val_score : 3.086141441599466e-05\n",
      "==================================================\n",
      "Epoch 286\n",
      "train_loss : 0.17085283063352108 val_loss : 0.4991251826286316\n",
      "train_score : 4.560585512081161e-05 val_score : 3.081444083363749e-05\n",
      "==================================================\n",
      "Epoch 287\n",
      "train_loss : 0.1716054491698742 val_loss : 0.4953712522983551\n",
      "train_score : 4.553709732135758e-05 val_score : 3.072821709793061e-05\n",
      "==================================================\n",
      "Epoch 288\n",
      "train_loss : 0.17241787910461426 val_loss : 0.49741584062576294\n",
      "train_score : 4.552011887426488e-05 val_score : 3.078107692999765e-05\n",
      "==================================================\n",
      "Epoch 289\n",
      "train_loss : 0.17078637331724167 val_loss : 0.5018670558929443\n",
      "train_score : 4.565834387904033e-05 val_score : 3.086090873694047e-05\n",
      "==================================================\n",
      "Epoch 290\n",
      "train_loss : 0.1717140357941389 val_loss : 0.49907636642456055\n",
      "train_score : 4.559100489132106e-05 val_score : 3.078803638345562e-05\n",
      "==================================================\n",
      "Epoch 291\n",
      "train_loss : 0.17102442868053913 val_loss : 0.5009382963180542\n",
      "train_score : 4.561047535389662e-05 val_score : 3.082366310991347e-05\n",
      "==================================================\n",
      "Epoch 292\n",
      "train_loss : 0.17063837312161922 val_loss : 0.5006318688392639\n",
      "train_score : 4.560752131510526e-05 val_score : 3.078683221247047e-05\n",
      "==================================================\n",
      "Epoch 293\n",
      "train_loss : 0.17160362750291824 val_loss : 0.4977552592754364\n",
      "train_score : 4.5570275688078254e-05 val_score : 3.076484426856041e-05\n",
      "==================================================\n",
      "Epoch 294\n",
      "train_loss : 0.17078686878085136 val_loss : 0.49996545910835266\n",
      "train_score : 4.563839320326224e-05 val_score : 3.0825955036561936e-05\n",
      "==================================================\n",
      "Epoch 295\n",
      "train_loss : 0.17090237699449062 val_loss : 0.49512946605682373\n",
      "train_score : 4.564082337310538e-05 val_score : 3.0790943128522485e-05\n",
      "==================================================\n",
      "Epoch 296\n",
      "train_loss : 0.1702576782554388 val_loss : 0.5020022392272949\n",
      "train_score : 4.565639028442092e-05 val_score : 3.076017674175091e-05\n",
      "==================================================\n",
      "Epoch 297\n",
      "train_loss : 0.17027845792472363 val_loss : 0.49893057346343994\n",
      "train_score : 4.564103073789738e-05 val_score : 3.0744373361812904e-05\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 298\n",
      "train_loss : 0.1704870406538248 val_loss : 0.4997986853122711\n",
      "train_score : 4.572686884785071e-05 val_score : 3.07905902445782e-05\n",
      "==================================================\n",
      "Epoch 299\n",
      "train_loss : 0.1704283133149147 val_loss : 0.5005829334259033\n",
      "train_score : 4.5643853809451684e-05 val_score : 3.075127096963115e-05\n",
      "==================================================\n",
      "Epoch 300\n",
      "train_loss : 0.17045506089925766 val_loss : 0.5045170783996582\n",
      "train_score : 4.5630244130734354e-05 val_score : 3.084785203100182e-05\n",
      "==================================================\n",
      "Epoch 301\n",
      "train_loss : 0.16992969810962677 val_loss : 0.4998342990875244\n",
      "train_score : 4.5646229409612715e-05 val_score : 3.0724397220183164e-05\n",
      "==================================================\n",
      "Epoch 302\n",
      "train_loss : 0.16915517300367355 val_loss : 0.500457227230072\n",
      "train_score : 4.5732966100331396e-05 val_score : 3.079335147049278e-05\n",
      "==================================================\n",
      "Epoch 303\n",
      "train_loss : 0.16929837688803673 val_loss : 0.49696481227874756\n",
      "train_score : 4.57938585896045e-05 val_score : 3.0652157875010744e-05\n",
      "==================================================\n",
      "Epoch 304\n",
      "train_loss : 0.1699720174074173 val_loss : 0.50449538230896\n",
      "train_score : 4.576688661472872e-05 val_score : 3.076574284932576e-05\n",
      "==================================================\n",
      "Epoch 305\n",
      "train_loss : 0.1697722040116787 val_loss : 0.4976496696472168\n",
      "train_score : 4.585685019264929e-05 val_score : 3.0817613151157275e-05\n",
      "==================================================\n",
      "Epoch 306\n",
      "train_loss : 0.1685479898005724 val_loss : 0.5082560777664185\n",
      "train_score : 4.569051816361025e-05 val_score : 3.0841642001178116e-05\n",
      "==================================================\n",
      "Epoch 307\n",
      "train_loss : 0.1694862637668848 val_loss : 0.5022614002227783\n",
      "train_score : 4.573361366055906e-05 val_score : 3.078101508435793e-05\n",
      "==================================================\n",
      "Epoch 308\n",
      "train_loss : 0.16883211769163609 val_loss : 0.5031052231788635\n",
      "train_score : 4.5767905248794705e-05 val_score : 3.0754730687476695e-05\n",
      "==================================================\n",
      "Epoch 309\n",
      "train_loss : 0.16846556030213833 val_loss : 0.5049764513969421\n",
      "train_score : 4.5764489186694846e-05 val_score : 3.0798084480920807e-05\n",
      "==================================================\n",
      "Epoch 310\n",
      "train_loss : 0.16774443536996841 val_loss : 0.5021962523460388\n",
      "train_score : 4.5773133024340495e-05 val_score : 3.072209074161947e-05\n",
      "==================================================\n",
      "Epoch 311\n",
      "train_loss : 0.16914836689829826 val_loss : 0.5017476677894592\n",
      "train_score : 4.5796168706147e-05 val_score : 3.081324030063115e-05\n",
      "==================================================\n",
      "Epoch 312\n",
      "train_loss : 0.16867376118898392 val_loss : 0.5048424005508423\n",
      "train_score : 4.5779052015859634e-05 val_score : 3.072414256166667e-05\n",
      "==================================================\n",
      "Epoch 313\n",
      "train_loss : 0.16822094283998013 val_loss : 0.500182032585144\n",
      "train_score : 4.578371226671152e-05 val_score : 3.076435677940026e-05\n",
      "==================================================\n",
      "Epoch 314\n",
      "train_loss : 0.16818473488092422 val_loss : 0.5026401281356812\n",
      "train_score : 4.580341192195192e-05 val_score : 3.080025999224745e-05\n",
      "==================================================\n",
      "Epoch 315\n",
      "train_loss : 0.1682638581842184 val_loss : 0.5018932223320007\n",
      "train_score : 4.586984869092703e-05 val_score : 3.082718467339873e-05\n",
      "==================================================\n",
      "Epoch 316\n",
      "train_loss : 0.16701383888721466 val_loss : 0.5050921440124512\n",
      "train_score : 4.581272150971927e-05 val_score : 3.076931898249313e-05\n",
      "==================================================\n",
      "Epoch 317\n",
      "train_loss : 0.1676340401172638 val_loss : 0.5026782155036926\n",
      "train_score : 4.5890417823102325e-05 val_score : 3.0730137950740755e-05\n",
      "==================================================\n",
      "Epoch 318\n",
      "train_loss : 0.16767536662518978 val_loss : 0.5013061761856079\n",
      "train_score : 4.586077920976095e-05 val_score : 3.077106157434173e-05\n",
      "==================================================\n",
      "Epoch 319\n",
      "train_loss : 0.16620847955346107 val_loss : 0.504345715045929\n",
      "train_score : 4.588050796883181e-05 val_score : 3.0863244319334626e-05\n",
      "==================================================\n",
      "Epoch 320\n",
      "train_loss : 0.166613282635808 val_loss : 0.5057330131530762\n",
      "train_score : 4.595160862663761e-05 val_score : 3.084552372456528e-05\n",
      "==================================================\n",
      "Epoch 321\n",
      "train_loss : 0.16763651184737682 val_loss : 0.5034217238426208\n",
      "train_score : 4.581839675665833e-05 val_score : 3.083325282204896e-05\n",
      "==================================================\n",
      "Epoch 322\n",
      "train_loss : 0.16697699576616287 val_loss : 0.5068263411521912\n",
      "train_score : 4.593435369315557e-05 val_score : 3.073423431487754e-05\n",
      "==================================================\n",
      "Epoch 323\n",
      "train_loss : 0.16637961938977242 val_loss : 0.5039872527122498\n",
      "train_score : 4.585606075124815e-05 val_score : 3.0782499379711226e-05\n",
      "==================================================\n",
      "Epoch 324\n",
      "train_loss : 0.1666267104446888 val_loss : 0.5035279989242554\n",
      "train_score : 4.593802441377193e-05 val_score : 3.084750278503634e-05\n",
      "==================================================\n",
      "Epoch 325\n",
      "train_loss : 0.1658160276710987 val_loss : 0.5057095289230347\n",
      "train_score : 4.591958349919878e-05 val_score : 3.075718996115029e-05\n",
      "==================================================\n",
      "Epoch 326\n",
      "train_loss : 0.16646232828497887 val_loss : 0.5064408183097839\n",
      "train_score : 4.5787070121150464e-05 val_score : 3.0706894904142246e-05\n",
      "==================================================\n",
      "Epoch 327\n",
      "train_loss : 0.16586589440703392 val_loss : 0.5075191259384155\n",
      "train_score : 4.582721521728672e-05 val_score : 3.0784976843278855e-05\n",
      "==================================================\n",
      "Epoch 328\n",
      "train_loss : 0.16608885489404202 val_loss : 0.5041027665138245\n",
      "train_score : 4.581738539854996e-05 val_score : 3.062184987356886e-05\n",
      "==================================================\n",
      "Epoch 329\n",
      "train_loss : 0.16608330979943275 val_loss : 0.5070300102233887\n",
      "train_score : 4.589723903336562e-05 val_score : 3.072845356655307e-05\n",
      "==================================================\n",
      "Epoch 330\n",
      "train_loss : 0.16633391752839088 val_loss : 0.5096167922019958\n",
      "train_score : 4.5943805162096396e-05 val_score : 3.075084896408953e-05\n",
      "==================================================\n",
      "Epoch 331\n",
      "train_loss : 0.16485834680497646 val_loss : 0.505897045135498\n",
      "train_score : 4.593569246935658e-05 val_score : 3.0714225431438535e-05\n",
      "==================================================\n",
      "Epoch 332\n",
      "train_loss : 0.16512669995427132 val_loss : 0.5116039514541626\n",
      "train_score : 4.587029980029911e-05 val_score : 3.085084608756006e-05\n",
      "==================================================\n",
      "Epoch 333\n",
      "train_loss : 0.16542632319033146 val_loss : 0.507053554058075\n",
      "train_score : 4.595475911628455e-05 val_score : 3.071226456086151e-05\n",
      "==================================================\n",
      "Epoch 334\n",
      "train_loss : 0.16537269577383995 val_loss : 0.5085293650627136\n",
      "train_score : 4.59502698504366e-05 val_score : 3.0870862246956676e-05\n",
      "==================================================\n",
      "Epoch 335\n",
      "train_loss : 0.1632875818759203 val_loss : 0.5087323188781738\n",
      "train_score : 4.604977584676817e-05 val_score : 3.080592432525009e-05\n",
      "==================================================\n",
      "Epoch 336\n",
      "train_loss : 0.16558660008013248 val_loss : 0.5042799115180969\n",
      "train_score : 4.5936951210023835e-05 val_score : 3.074963387916796e-05\n",
      "==================================================\n",
      "Epoch 337\n",
      "train_loss : 0.16449189744889736 val_loss : 0.5117371082305908\n",
      "train_score : 4.596667713485658e-05 val_score : 3.083716001128778e-05\n",
      "==================================================\n",
      "Epoch 338\n",
      "train_loss : 0.16433114744722843 val_loss : 0.5074427723884583\n",
      "train_score : 4.597287988872267e-05 val_score : 3.078333611483686e-05\n",
      "==================================================\n",
      "Epoch 339\n",
      "train_loss : 0.1635801810771227 val_loss : 0.5090422034263611\n",
      "train_score : 4.596823418978602e-05 val_score : 3.073688640142791e-05\n",
      "==================================================\n",
      "Epoch 340\n",
      "train_loss : 0.16370598413050175 val_loss : 0.5108473896980286\n",
      "train_score : 4.604801142704673e-05 val_score : 3.067473153350875e-05\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 341\n",
      "train_loss : 0.16420220397412777 val_loss : 0.5063361525535583\n",
      "train_score : 4.6018973080208525e-05 val_score : 3.086708966293372e-05\n",
      "==================================================\n",
      "Epoch 342\n",
      "train_loss : 0.16384436562657356 val_loss : 0.5118513107299805\n",
      "train_score : 4.611213807947934e-05 val_score : 3.072797699132934e-05\n",
      "==================================================\n",
      "Epoch 343\n",
      "train_loss : 0.16360093280673027 val_loss : 0.5069221258163452\n",
      "train_score : 4.6092300181044266e-05 val_score : 3.067177385673858e-05\n",
      "==================================================\n",
      "Epoch 344\n",
      "train_loss : 0.16286278516054153 val_loss : 0.5133616328239441\n",
      "train_score : 4.602785702445544e-05 val_score : 3.079478119616397e-05\n",
      "==================================================\n",
      "Epoch 345\n",
      "train_loss : 0.1636542435735464 val_loss : 0.5051266551017761\n",
      "train_score : 4.6031505917198956e-05 val_score : 3.070078309974633e-05\n",
      "==================================================\n",
      "Epoch 346\n",
      "train_loss : 0.1632143296301365 val_loss : 0.5092378258705139\n",
      "train_score : 4.60994488094002e-05 val_score : 3.075880158576183e-05\n",
      "==================================================\n",
      "Epoch 347\n",
      "train_loss : 0.16374505870044231 val_loss : 0.5121081471443176\n",
      "train_score : 4.597636507241987e-05 val_score : 3.068212026846595e-05\n",
      "==================================================\n",
      "Epoch 348\n",
      "train_loss : 0.16239836812019348 val_loss : 0.5046268105506897\n",
      "train_score : 4.6053024561842903e-05 val_score : 3.064384145545773e-05\n",
      "==================================================\n",
      "Epoch 349\n",
      "train_loss : 0.16203196719288826 val_loss : 0.5128483176231384\n",
      "train_score : 4.6187808038666844e-05 val_score : 3.075971835642122e-05\n",
      "==================================================\n",
      "Epoch 350\n",
      "train_loss : 0.16156847029924393 val_loss : 0.5110771656036377\n",
      "train_score : 4.619459286914207e-05 val_score : 3.0691786378156394e-05\n",
      "==================================================\n",
      "Epoch 351\n",
      "train_loss : 0.16249574162065983 val_loss : 0.5062127709388733\n",
      "train_score : 4.596509097609669e-05 val_score : 3.071085302508436e-05\n",
      "==================================================\n",
      "Epoch 352\n",
      "train_loss : 0.16165748797357082 val_loss : 0.5126373767852783\n",
      "train_score : 4.61668023490347e-05 val_score : 3.0675812013214454e-05\n",
      "==================================================\n",
      "Epoch 353\n",
      "train_loss : 0.1615121141076088 val_loss : 0.507845938205719\n",
      "train_score : 4.6113265852909535e-05 val_score : 3.079475573031232e-05\n",
      "==================================================\n",
      "Epoch 354\n",
      "train_loss : 0.16257065907120705 val_loss : 0.5132612586021423\n",
      "train_score : 4.609900133800693e-05 val_score : 3.0813909688731655e-05\n",
      "==================================================\n",
      "Epoch 355\n",
      "train_loss : 0.16228982247412205 val_loss : 0.5090936422348022\n",
      "train_score : 4.60694900539238e-05 val_score : 3.062624455196783e-05\n",
      "==================================================\n",
      "Epoch 356\n",
      "train_loss : 0.1610743347555399 val_loss : 0.5143622159957886\n",
      "train_score : 4.6071312681306154e-05 val_score : 3.0692084692418575e-05\n",
      "==================================================\n",
      "Epoch 357\n",
      "train_loss : 0.162348922342062 val_loss : 0.5115863084793091\n",
      "train_score : 4.6088571252767e-05 val_score : 3.07984919345472e-05\n",
      "==================================================\n",
      "Epoch 358\n",
      "train_loss : 0.16197794675827026 val_loss : 0.5153526663780212\n",
      "train_score : 4.620008257916197e-05 val_score : 3.075868880841881e-05\n",
      "==================================================\n",
      "Epoch 359\n",
      "train_loss : 0.16150011494755745 val_loss : 0.5102691054344177\n",
      "train_score : 4.6254179324023426e-05 val_score : 3.073025800404139e-05\n",
      "==================================================\n",
      "Epoch 360\n",
      "train_loss : 0.1599834095686674 val_loss : 0.5070768594741821\n",
      "train_score : 4.6347722673090175e-05 val_score : 3.076526991208084e-05\n",
      "==================================================\n",
      "Epoch 361\n",
      "train_loss : 0.161064887419343 val_loss : 0.5163642764091492\n",
      "train_score : 4.626791996997781e-05 val_score : 3.061270035686903e-05\n",
      "==================================================\n",
      "Epoch 362\n",
      "train_loss : 0.16174191236495972 val_loss : 0.5184820890426636\n",
      "train_score : 4.6122560888761654e-05 val_score : 3.0759321816731244e-05\n",
      "==================================================\n",
      "Epoch 363\n",
      "train_loss : 0.16131973452866077 val_loss : 0.5106087327003479\n",
      "train_score : 4.629847899195738e-05 val_score : 3.061565803363919e-05\n",
      "==================================================\n",
      "Epoch 364\n",
      "train_loss : 0.16203219816088676 val_loss : 0.5157888531684875\n",
      "train_score : 4.623063068720512e-05 val_score : 3.072007530136034e-05\n",
      "==================================================\n",
      "Epoch 365\n",
      "train_loss : 0.15988021343946457 val_loss : 0.5076109766960144\n",
      "train_score : 4.62021489511244e-05 val_score : 3.061973620788194e-05\n",
      "==================================================\n",
      "Epoch 366\n",
      "train_loss : 0.1599638443440199 val_loss : 0.5170475840568542\n",
      "train_score : 4.622507549356669e-05 val_score : 3.083159390371293e-05\n",
      "==================================================\n",
      "Epoch 367\n",
      "train_loss : 0.15911039151251316 val_loss : 0.5112835168838501\n",
      "train_score : 4.621413245331496e-05 val_score : 3.0748113204026595e-05\n",
      "==================================================\n",
      "Epoch 368\n",
      "train_loss : 0.15950826555490494 val_loss : 0.5166841149330139\n",
      "train_score : 4.633736170944758e-05 val_score : 3.068951264140196e-05\n",
      "==================================================\n",
      "Epoch 369\n",
      "train_loss : 0.16007937118411064 val_loss : 0.5120828151702881\n",
      "train_score : 4.6039614971959963e-05 val_score : 3.060578092117794e-05\n",
      "==================================================\n",
      "Epoch 370\n",
      "train_loss : 0.15876618400216103 val_loss : 0.5162766575813293\n",
      "train_score : 4.627224916475825e-05 val_score : 3.05989888147451e-05\n",
      "==================================================\n",
      "Epoch 371\n",
      "train_loss : 0.1596530955284834 val_loss : 0.5135950446128845\n",
      "train_score : 4.6319113607751206e-05 val_score : 3.071859828196466e-05\n",
      "==================================================\n",
      "Epoch 372\n",
      "train_loss : 0.15975510887801647 val_loss : 0.5185656547546387\n",
      "train_score : 4.624409484677017e-05 val_score : 3.077587462030351e-05\n",
      "==================================================\n",
      "Epoch 373\n",
      "train_loss : 0.1593649834394455 val_loss : 0.5132228136062622\n",
      "train_score : 4.626688314601779e-05 val_score : 3.056834975723177e-05\n",
      "==================================================\n",
      "Epoch 374\n",
      "train_loss : 0.15867654606699944 val_loss : 0.5112072825431824\n",
      "train_score : 4.640699989977293e-05 val_score : 3.06314104818739e-05\n",
      "==================================================\n",
      "Epoch 375\n",
      "train_loss : 0.1584241334348917 val_loss : 0.5171525478363037\n",
      "train_score : 4.635787263396196e-05 val_score : 3.0786126444581896e-05\n",
      "==================================================\n",
      "Epoch 376\n",
      "train_loss : 0.15887572430074215 val_loss : 0.5085484385490417\n",
      "train_score : 4.637733945855871e-05 val_score : 3.06166875816416e-05\n",
      "==================================================\n",
      "Epoch 377\n",
      "train_loss : 0.1580693144351244 val_loss : 0.5152615904808044\n",
      "train_score : 4.633479693438858e-05 val_score : 3.0717692425241694e-05\n",
      "==================================================\n",
      "Epoch 378\n",
      "train_loss : 0.15867825411260128 val_loss : 0.5181921720504761\n",
      "train_score : 4.626129157259129e-05 val_score : 3.070860111620277e-05\n",
      "==================================================\n",
      "Epoch 379\n",
      "train_loss : 0.15788305178284645 val_loss : 0.5153212547302246\n",
      "train_score : 4.64143231511116e-05 val_score : 3.070828097406775e-05\n",
      "==================================================\n",
      "Epoch 380\n",
      "train_loss : 0.15767454728484154 val_loss : 0.5116621255874634\n",
      "train_score : 4.6379256673390046e-05 val_score : 3.0782073736190796e-05\n",
      "==================================================\n",
      "Epoch 381\n",
      "train_loss : 0.15816725976765156 val_loss : 0.5147513151168823\n",
      "train_score : 4.638199243345298e-05 val_score : 3.052689498872496e-05\n",
      "==================================================\n",
      "Epoch 382\n",
      "train_loss : 0.15754015557467937 val_loss : 0.5175979733467102\n",
      "train_score : 4.6402688894886523e-05 val_score : 3.068636215175502e-05\n",
      "==================================================\n",
      "Epoch 383\n",
      "train_loss : 0.15732993930578232 val_loss : 0.5202487111091614\n",
      "train_score : 4.639389590010978e-05 val_score : 3.069395825150423e-05\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 384\n",
      "train_loss : 0.15732809901237488 val_loss : 0.5167813897132874\n",
      "train_score : 4.63125434180256e-05 val_score : 3.066494537051767e-05\n",
      "==================================================\n",
      "Epoch 385\n",
      "train_loss : 0.15672233700752258 val_loss : 0.5206883549690247\n",
      "train_score : 4.644305590773001e-05 val_score : 3.075762651860714e-05\n",
      "==================================================\n",
      "Epoch 386\n",
      "train_loss : 0.15699257142841816 val_loss : 0.5168892741203308\n",
      "train_score : 4.643607826437801e-05 val_score : 3.070096863666549e-05\n",
      "==================================================\n",
      "Epoch 387\n",
      "train_loss : 0.15661074593663216 val_loss : 0.5220664739608765\n",
      "train_score : 4.639293547370471e-05 val_score : 3.069641024922021e-05\n",
      "==================================================\n",
      "Epoch 388\n",
      "train_loss : 0.15611256286501884 val_loss : 0.519896924495697\n",
      "train_score : 4.657823228626512e-05 val_score : 3.067014768021181e-05\n",
      "==================================================\n",
      "Epoch 389\n",
      "train_loss : 0.15694663301110268 val_loss : 0.5199648141860962\n",
      "train_score : 4.629749309970066e-05 val_score : 3.0703329684911296e-05\n",
      "==================================================\n",
      "Epoch 390\n",
      "train_loss : 0.15637456811964512 val_loss : 0.52085942029953\n",
      "train_score : 4.6362121793208644e-05 val_score : 3.0786952265771106e-05\n",
      "==================================================\n",
      "Epoch 391\n",
      "train_loss : 0.15717821195721626 val_loss : 0.5203267335891724\n",
      "train_score : 4.6369401388801634e-05 val_score : 3.067979560000822e-05\n",
      "==================================================\n",
      "Epoch 392\n",
      "train_loss : 0.15599150583148003 val_loss : 0.5148258209228516\n",
      "train_score : 4.6453009417746216e-05 val_score : 3.065732744289562e-05\n",
      "==================================================\n",
      "Epoch 393\n",
      "train_loss : 0.15589254163205624 val_loss : 0.5208876729011536\n",
      "train_score : 4.649881520890631e-05 val_score : 3.065527562284842e-05\n",
      "==================================================\n",
      "Epoch 394\n",
      "train_loss : 0.15604990534484386 val_loss : 0.5231865644454956\n",
      "train_score : 4.654101576306857e-05 val_score : 3.079952875850722e-05\n",
      "==================================================\n",
      "Epoch 395\n",
      "train_loss : 0.15596213564276695 val_loss : 0.5193718075752258\n",
      "train_score : 4.646324669010937e-05 val_score : 3.07047666865401e-05\n",
      "==================================================\n",
      "Epoch 396\n",
      "train_loss : 0.15546416491270065 val_loss : 0.5171810388565063\n",
      "train_score : 4.652412826544605e-05 val_score : 3.078143345192075e-05\n",
      "==================================================\n",
      "Epoch 397\n",
      "train_loss : 0.15585996769368649 val_loss : 0.5155772566795349\n",
      "train_score : 4.654192161979154e-05 val_score : 3.06904039462097e-05\n",
      "==================================================\n",
      "Epoch 398\n",
      "train_loss : 0.15503146313130856 val_loss : 0.5214203000068665\n",
      "train_score : 4.6529679821105674e-05 val_score : 3.0610262911068276e-05\n",
      "==================================================\n",
      "Epoch 399\n",
      "train_loss : 0.15484530478715897 val_loss : 0.5222594738006592\n",
      "train_score : 4.6523109631380066e-05 val_score : 3.078075678786263e-05\n",
      "==================================================\n",
      "Epoch 400\n",
      "train_loss : 0.1545430775731802 val_loss : 0.5206518769264221\n",
      "train_score : 4.6516295697074383e-05 val_score : 3.081709292018786e-05\n",
      "==================================================\n",
      "Epoch 401\n",
      "train_loss : 0.15541998483240604 val_loss : 0.5204156637191772\n",
      "train_score : 4.648907997761853e-05 val_score : 3.074765845667571e-05\n",
      "==================================================\n",
      "Epoch 402\n",
      "train_loss : 0.15568704344332218 val_loss : 0.5188698172569275\n",
      "train_score : 4.6643199311802164e-05 val_score : 3.0731025617569685e-05\n",
      "==================================================\n",
      "Epoch 403\n",
      "train_loss : 0.15377946570515633 val_loss : 0.5208169221878052\n",
      "train_score : 4.655935481423512e-05 val_score : 3.070439925068058e-05\n",
      "==================================================\n",
      "Epoch 404\n",
      "train_loss : 0.1536555141210556 val_loss : 0.5197401642799377\n",
      "train_score : 4.6479028242174536e-05 val_score : 3.071371611440554e-05\n",
      "==================================================\n",
      "Epoch 405\n",
      "train_loss : 0.15476767346262932 val_loss : 0.5182253122329712\n",
      "train_score : 4.661348430090584e-05 val_score : 3.0590763344662264e-05\n",
      "==================================================\n",
      "Epoch 406\n",
      "train_loss : 0.1538597047328949 val_loss : 0.5186845660209656\n",
      "train_score : 4.653304131352343e-05 val_score : 3.059193477383815e-05\n",
      "==================================================\n",
      "Epoch 407\n",
      "train_loss : 0.15454097092151642 val_loss : 0.522930920124054\n",
      "train_score : 4.6592773287557065e-05 val_score : 3.059002119698562e-05\n",
      "==================================================\n",
      "Epoch 408\n",
      "train_loss : 0.15369862131774426 val_loss : 0.5169556736946106\n",
      "train_score : 4.655845623346977e-05 val_score : 3.06740403175354e-05\n",
      "==================================================\n",
      "Epoch 409\n",
      "train_loss : 0.15251825004816055 val_loss : 0.5221195220947266\n",
      "train_score : 4.672030991059728e-05 val_score : 3.0737519409740344e-05\n",
      "==================================================\n",
      "Epoch 410\n",
      "train_loss : 0.15367132425308228 val_loss : 0.5276687145233154\n",
      "train_score : 4.660129707190208e-05 val_score : 3.0719791539013386e-05\n",
      "==================================================\n",
      "Epoch 411\n",
      "train_loss : 0.15296565368771553 val_loss : 0.5300044417381287\n",
      "train_score : 4.671896022045985e-05 val_score : 3.063028634642251e-05\n",
      "==================================================\n",
      "Epoch 412\n",
      "train_loss : 0.15395217016339302 val_loss : 0.5190154314041138\n",
      "train_score : 4.656312376027927e-05 val_score : 3.066348654101603e-05\n",
      "==================================================\n",
      "Epoch 413\n",
      "train_loss : 0.15271381102502346 val_loss : 0.5223293304443359\n",
      "train_score : 4.6667664719279855e-05 val_score : 3.057819412788376e-05\n",
      "==================================================\n",
      "Epoch 414\n",
      "train_loss : 0.15301316231489182 val_loss : 0.5221501588821411\n",
      "train_score : 4.6544402721337974e-05 val_score : 3.063632902922109e-05\n",
      "==================================================\n",
      "Epoch 415\n",
      "train_loss : 0.15110674500465393 val_loss : 0.5221083760261536\n",
      "train_score : 4.674984666053206e-05 val_score : 3.082219700445421e-05\n",
      "==================================================\n",
      "Epoch 416\n",
      "train_loss : 0.15258552879095078 val_loss : 0.5249887704849243\n",
      "train_score : 4.669128975365311e-05 val_score : 3.064081465709023e-05\n",
      "==================================================\n",
      "Epoch 417\n",
      "train_loss : 0.1521815974265337 val_loss : 0.5239505171775818\n",
      "train_score : 4.670419366448186e-05 val_score : 3.059892696910538e-05\n",
      "==================================================\n",
      "Epoch 418\n",
      "train_loss : 0.15263240039348602 val_loss : 0.5237281322479248\n",
      "train_score : 4.669751797337085e-05 val_score : 3.0670944397570565e-05\n",
      "==================================================\n",
      "Epoch 419\n",
      "train_loss : 0.15236390009522438 val_loss : 0.5166170001029968\n",
      "train_score : 4.6772980567766353e-05 val_score : 3.060739618376829e-05\n",
      "==================================================\n",
      "Epoch 420\n",
      "train_loss : 0.1529591679573059 val_loss : 0.5197060704231262\n",
      "train_score : 4.6658569772262126e-05 val_score : 3.06479305436369e-05\n",
      "==================================================\n",
      "Epoch 421\n",
      "train_loss : 0.1524881925433874 val_loss : 0.5300370454788208\n",
      "train_score : 4.6710694732610136e-05 val_score : 3.071741230087355e-05\n",
      "==================================================\n",
      "Epoch 422\n",
      "train_loss : 0.15107153169810772 val_loss : 0.524858295917511\n",
      "train_score : 4.677047400036827e-05 val_score : 3.062969699385576e-05\n",
      "==================================================\n",
      "Epoch 423\n",
      "train_loss : 0.15096412599086761 val_loss : 0.5192596316337585\n",
      "train_score : 4.679773701354861e-05 val_score : 3.059461232624017e-05\n",
      "==================================================\n",
      "Epoch 424\n",
      "train_loss : 0.15182690881192684 val_loss : 0.5239647030830383\n",
      "train_score : 4.6748187742196023e-05 val_score : 3.081374234170653e-05\n",
      "==================================================\n",
      "Epoch 425\n",
      "train_loss : 0.15123997256159782 val_loss : 0.5275523066520691\n",
      "train_score : 4.678170444094576e-05 val_score : 3.0553022952517495e-05\n",
      "==================================================\n",
      "Epoch 426\n",
      "train_loss : 0.1501010600477457 val_loss : 0.523306131362915\n",
      "train_score : 4.687907494371757e-05 val_score : 3.071301034651697e-05\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 427\n",
      "train_loss : 0.15123739279806614 val_loss : 0.5244140625\n",
      "train_score : 4.682667349698022e-05 val_score : 3.053452383028343e-05\n",
      "==================================================\n",
      "Epoch 428\n",
      "train_loss : 0.15008282102644444 val_loss : 0.5320975184440613\n",
      "train_score : 4.67919853690546e-05 val_score : 3.0734165193280205e-05\n",
      "==================================================\n",
      "Epoch 429\n",
      "train_loss : 0.15026183612644672 val_loss : 0.5240243077278137\n",
      "train_score : 4.687572436523624e-05 val_score : 3.062932955799624e-05\n",
      "==================================================\n",
      "Epoch 430\n",
      "train_loss : 0.15034768730401993 val_loss : 0.5218722820281982\n",
      "train_score : 4.686991815106012e-05 val_score : 3.056970308534801e-05\n",
      "==================================================\n",
      "Epoch 431\n",
      "train_loss : 0.1504769939929247 val_loss : 0.5223926901817322\n",
      "train_score : 4.677316246670671e-05 val_score : 3.067110083065927e-05\n",
      "==================================================\n",
      "Epoch 432\n",
      "train_loss : 0.15016229264438152 val_loss : 0.5289232730865479\n",
      "train_score : 4.6835659304633737e-05 val_score : 3.073483821935952e-05\n",
      "==================================================\n",
      "Epoch 433\n",
      "train_loss : 0.15028774365782738 val_loss : 0.5219335556030273\n",
      "train_score : 4.6744109567953274e-05 val_score : 3.0614268325734884e-05\n",
      "==================================================\n",
      "Epoch 434\n",
      "train_loss : 0.14987091720104218 val_loss : 0.528069019317627\n",
      "train_score : 4.684437226387672e-05 val_score : 3.069238300668076e-05\n",
      "==================================================\n",
      "Epoch 435\n",
      "train_loss : 0.1491971518844366 val_loss : 0.5250604152679443\n",
      "train_score : 4.6857727284077555e-05 val_score : 3.057996946154162e-05\n",
      "==================================================\n",
      "Epoch 436\n",
      "train_loss : 0.14913136512041092 val_loss : 0.528537392616272\n",
      "train_score : 4.699101555161178e-05 val_score : 3.060992094106041e-05\n",
      "==================================================\n",
      "Epoch 437\n",
      "train_loss : 0.1486311163753271 val_loss : 0.5332962870597839\n",
      "train_score : 4.69662481918931e-05 val_score : 3.067292345804162e-05\n",
      "==================================================\n",
      "Epoch 438\n",
      "train_loss : 0.14938704296946526 val_loss : 0.5237439274787903\n",
      "train_score : 4.684772648033686e-05 val_score : 3.062772157136351e-05\n",
      "==================================================\n",
      "Epoch 439\n",
      "train_loss : 0.14861816726624966 val_loss : 0.5283091068267822\n",
      "train_score : 4.683987936004996e-05 val_score : 3.073311017942615e-05\n",
      "==================================================\n",
      "Epoch 440\n",
      "train_loss : 0.14842311292886734 val_loss : 0.5290154814720154\n",
      "train_score : 4.7006084059830755e-05 val_score : 3.0494284146698192e-05\n",
      "==================================================\n",
      "Epoch 441\n",
      "train_loss : 0.14802051149308681 val_loss : 0.5323721766471863\n",
      "train_score : 4.691544017987326e-05 val_score : 3.0630453693447635e-05\n",
      "==================================================\n",
      "Epoch 442\n",
      "train_loss : 0.14923906698822975 val_loss : 0.5297002196311951\n",
      "train_score : 4.697643089457415e-05 val_score : 3.0685081583214924e-05\n",
      "==================================================\n",
      "Epoch 443\n",
      "train_loss : 0.14829488284885883 val_loss : 0.5251506567001343\n",
      "train_score : 4.6959150495240465e-05 val_score : 3.062321411562152e-05\n",
      "==================================================\n",
      "Epoch 444\n",
      "train_loss : 0.14943226613104343 val_loss : 0.5246469974517822\n",
      "train_score : 4.693845767178573e-05 val_score : 3.059753726120107e-05\n",
      "==================================================\n",
      "Epoch 445\n",
      "train_loss : 0.1481298878788948 val_loss : 0.5357468128204346\n",
      "train_score : 4.694522431236692e-05 val_score : 3.0602132028434426e-05\n",
      "==================================================\n",
      "Epoch 446\n",
      "train_loss : 0.14817696996033192 val_loss : 0.5307409167289734\n",
      "train_score : 4.691516369348392e-05 val_score : 3.0643612262792885e-05\n",
      "==================================================\n",
      "Epoch 447\n",
      "train_loss : 0.1473602131009102 val_loss : 0.5291165709495544\n",
      "train_score : 4.7068904677871615e-05 val_score : 3.0606348445871845e-05\n",
      "==================================================\n",
      "Epoch 448\n",
      "train_loss : 0.14778579212725163 val_loss : 0.5312485694885254\n",
      "train_score : 4.698237535194494e-05 val_score : 3.063645999645814e-05\n",
      "==================================================\n",
      "Epoch 449\n",
      "train_loss : 0.1473213341087103 val_loss : 0.5323140621185303\n",
      "train_score : 4.703247032011859e-05 val_score : 3.068668593186885e-05\n",
      "==================================================\n",
      "Epoch 450\n",
      "train_loss : 0.14745036326348782 val_loss : 0.530324399471283\n",
      "train_score : 4.698340126196854e-05 val_score : 3.0619157769251615e-05\n",
      "==================================================\n",
      "Epoch 451\n",
      "train_loss : 0.14649209566414356 val_loss : 0.5288945436477661\n",
      "train_score : 4.711372093879618e-05 val_score : 3.0651455745100975e-05\n",
      "==================================================\n",
      "Epoch 452\n",
      "train_loss : 0.1474143173545599 val_loss : 0.5306670665740967\n",
      "train_score : 4.6958175516920164e-05 val_score : 3.0658786272397265e-05\n",
      "==================================================\n",
      "Epoch 453\n",
      "train_loss : 0.14602009020745754 val_loss : 0.5303713083267212\n",
      "train_score : 4.699925921158865e-05 val_score : 3.064726843149401e-05\n",
      "==================================================\n",
      "Epoch 454\n",
      "train_loss : 0.14734181575477123 val_loss : 0.52952641248703\n",
      "train_score : 4.699219061876647e-05 val_score : 3.063372423639521e-05\n",
      "==================================================\n",
      "Epoch 455\n",
      "train_loss : 0.14711317420005798 val_loss : 0.531072199344635\n",
      "train_score : 4.693387381848879e-05 val_score : 3.066338103963062e-05\n",
      "==================================================\n",
      "Epoch 456\n",
      "train_loss : 0.14689936861395836 val_loss : 0.5272231698036194\n",
      "train_score : 4.6961002226453274e-05 val_score : 3.0463696020888165e-05\n",
      "==================================================\n",
      "Epoch 457\n",
      "train_loss : 0.14583858288824558 val_loss : 0.5375258326530457\n",
      "train_score : 4.70695122203324e-05 val_score : 3.064260454266332e-05\n",
      "==================================================\n",
      "Epoch 458\n",
      "train_loss : 0.1459757387638092 val_loss : 0.535792350769043\n",
      "train_score : 4.7082739911274984e-05 val_score : 3.0663966754218563e-05\n",
      "==================================================\n",
      "Epoch 459\n",
      "train_loss : 0.14639858528971672 val_loss : 0.527077317237854\n",
      "train_score : 4.7085817641345784e-05 val_score : 3.060231392737478e-05\n",
      "==================================================\n",
      "Epoch 460\n",
      "train_loss : 0.1443385500460863 val_loss : 0.5400195121765137\n",
      "train_score : 4.718704803963192e-05 val_score : 3.058954462176189e-05\n",
      "==================================================\n",
      "Epoch 461\n",
      "train_loss : 0.14571738243103027 val_loss : 0.530338704586029\n",
      "train_score : 4.7065903345355764e-05 val_score : 3.067202851525508e-05\n",
      "==================================================\n",
      "Epoch 462\n",
      "train_loss : 0.14515458792448044 val_loss : 0.5398719310760498\n",
      "train_score : 4.709787754109129e-05 val_score : 3.0630890250904486e-05\n",
      "==================================================\n",
      "Epoch 463\n",
      "train_loss : 0.1451672539114952 val_loss : 0.5332654714584351\n",
      "train_score : 4.707191328634508e-05 val_score : 3.0640811019111425e-05\n",
      "==================================================\n",
      "Epoch 464\n",
      "train_loss : 0.14485821314156055 val_loss : 0.5258393883705139\n",
      "train_score : 4.717415140476078e-05 val_score : 3.0082235753070563e-05\n",
      "==================================================\n",
      "Epoch 465\n",
      "train_loss : 0.14618714153766632 val_loss : 0.5359592437744141\n",
      "train_score : 4.696216274169274e-05 val_score : 3.052395550184883e-05\n",
      "==================================================\n",
      "Epoch 466\n",
      "train_loss : 0.1453375145792961 val_loss : 0.5437416434288025\n",
      "train_score : 4.709347922471352e-05 val_score : 3.061689858441241e-05\n",
      "==================================================\n",
      "Epoch 467\n",
      "train_loss : 0.14543789438903332 val_loss : 0.5357798337936401\n",
      "train_score : 4.710859138867818e-05 val_score : 3.0611801776103675e-05\n",
      "==================================================\n",
      "Epoch 468\n",
      "train_loss : 0.14533638581633568 val_loss : 0.5367411375045776\n",
      "train_score : 4.700890349340625e-05 val_score : 3.051789462915622e-05\n",
      "==================================================\n",
      "Epoch 469\n",
      "train_loss : 0.14569679647684097 val_loss : 0.5339379906654358\n",
      "train_score : 4.715082832262851e-05 val_score : 3.061654933844693e-05\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 470\n",
      "train_loss : 0.14453392289578915 val_loss : 0.5320388674736023\n",
      "train_score : 4.715822069556452e-05 val_score : 3.0674185836687684e-05\n",
      "==================================================\n",
      "Epoch 471\n",
      "train_loss : 0.1435686144977808 val_loss : 0.5362977981567383\n",
      "train_score : 4.7240915591828525e-05 val_score : 3.063444091822021e-05\n",
      "==================================================\n",
      "Epoch 472\n",
      "train_loss : 0.14325899071991444 val_loss : 0.5327261686325073\n",
      "train_score : 4.713693851954304e-05 val_score : 3.054531043744646e-05\n",
      "==================================================\n",
      "Epoch 473\n",
      "train_loss : 0.1434956006705761 val_loss : 0.5333012342453003\n",
      "train_score : 4.7237361286533996e-05 val_score : 3.0677299946546555e-05\n",
      "==================================================\n",
      "Epoch 474\n",
      "train_loss : 0.14337296783924103 val_loss : 0.534065306186676\n",
      "train_score : 4.719323624158278e-05 val_score : 3.0654144211439416e-05\n",
      "==================================================\n",
      "Epoch 475\n",
      "train_loss : 0.14328479953110218 val_loss : 0.5321880578994751\n",
      "train_score : 4.736768823931925e-05 val_score : 3.067531724809669e-05\n",
      "==================================================\n",
      "Epoch 476\n",
      "train_loss : 0.14398550800979137 val_loss : 0.539006233215332\n",
      "train_score : 4.7146659198915586e-05 val_score : 3.04246750602033e-05\n",
      "==================================================\n",
      "Epoch 477\n",
      "train_loss : 0.1440322082489729 val_loss : 0.5340923070907593\n",
      "train_score : 4.722138328361325e-05 val_score : 3.0536884878529236e-05\n",
      "==================================================\n",
      "Epoch 478\n",
      "train_loss : 0.14394515752792358 val_loss : 0.5350158214569092\n",
      "train_score : 4.7328154323622584e-05 val_score : 3.060789458686486e-05\n",
      "==================================================\n",
      "Epoch 479\n",
      "train_loss : 0.14285684749484062 val_loss : 0.5359065532684326\n",
      "train_score : 4.718953277915716e-05 val_score : 3.059170921915211e-05\n",
      "==================================================\n",
      "Epoch 480\n",
      "train_loss : 0.14214479736983776 val_loss : 0.5328495502471924\n",
      "train_score : 4.7344077756861225e-05 val_score : 3.057742287637666e-05\n",
      "==================================================\n",
      "Epoch 481\n",
      "train_loss : 0.14311147294938564 val_loss : 0.545332133769989\n",
      "train_score : 4.730532964458689e-05 val_score : 3.053489126614295e-05\n",
      "==================================================\n",
      "Epoch 482\n",
      "train_loss : 0.1427240688353777 val_loss : 0.5411046743392944\n",
      "train_score : 4.724024256574921e-05 val_score : 3.051724524993915e-05\n",
      "==================================================\n",
      "Epoch 483\n",
      "train_loss : 0.14113767817616463 val_loss : 0.548665463924408\n",
      "train_score : 4.730415821541101e-05 val_score : 3.0608276574639603e-05\n",
      "==================================================\n",
      "Epoch 484\n",
      "train_loss : 0.14259669370949268 val_loss : 0.5396699905395508\n",
      "train_score : 4.731779335997999e-05 val_score : 3.0470815545413643e-05\n",
      "==================================================\n",
      "Epoch 485\n",
      "train_loss : 0.14245943911373615 val_loss : 0.5311483144760132\n",
      "train_score : 4.7392062697326764e-05 val_score : 3.0478271582978778e-05\n",
      "==================================================\n",
      "Epoch 486\n",
      "train_loss : 0.14197878539562225 val_loss : 0.5437200665473938\n",
      "train_score : 4.729465217678808e-05 val_score : 3.05829344142694e-05\n",
      "==================================================\n",
      "Epoch 487\n",
      "train_loss : 0.14147179014980793 val_loss : 0.5439754724502563\n",
      "train_score : 4.737039853353053e-05 val_score : 3.0708295525982976e-05\n",
      "==================================================\n",
      "Epoch 488\n",
      "train_loss : 0.14186307787895203 val_loss : 0.5389767289161682\n",
      "train_score : 4.7275552788050845e-05 val_score : 3.05270696117077e-05\n",
      "==================================================\n",
      "Epoch 489\n",
      "train_loss : 0.14182909205555916 val_loss : 0.5374121069908142\n",
      "train_score : 4.731239459943026e-05 val_score : 3.0580114980693907e-05\n",
      "==================================================\n",
      "Epoch 490\n",
      "train_loss : 0.14167865924537182 val_loss : 0.5349443554878235\n",
      "train_score : 4.734594403998926e-05 val_score : 3.0521023290930316e-05\n",
      "==================================================\n",
      "Epoch 491\n",
      "train_loss : 0.14038674160838127 val_loss : 0.5408636331558228\n",
      "train_score : 4.747652928926982e-05 val_score : 3.058235233766027e-05\n",
      "==================================================\n",
      "Epoch 492\n",
      "train_loss : 0.14029096066951752 val_loss : 0.5441339612007141\n",
      "train_score : 4.7392761189257726e-05 val_score : 3.0517339837388135e-05\n",
      "==================================================\n",
      "Epoch 493\n",
      "train_loss : 0.1401877012103796 val_loss : 0.5376739501953125\n",
      "train_score : 4.74170119559858e-05 val_score : 3.058114816667512e-05\n",
      "==================================================\n",
      "Epoch 494\n",
      "train_loss : 0.1412860695272684 val_loss : 0.5406018495559692\n",
      "train_score : 4.7359211748698726e-05 val_score : 3.053067484870553e-05\n",
      "==================================================\n",
      "Epoch 495\n",
      "train_loss : 0.1401801686733961 val_loss : 0.5366818904876709\n",
      "train_score : 4.748163701151498e-05 val_score : 3.0499739295919426e-05\n",
      "==================================================\n",
      "Epoch 496\n",
      "train_loss : 0.13938643224537373 val_loss : 0.5394834876060486\n",
      "train_score : 4.7451052523683757e-05 val_score : 3.0476576284854673e-05\n",
      "==================================================\n",
      "Epoch 497\n",
      "train_loss : 0.13966985046863556 val_loss : 0.539722740650177\n",
      "train_score : 4.733114110422321e-05 val_score : 3.0472381695290096e-05\n",
      "==================================================\n",
      "Epoch 498\n",
      "train_loss : 0.14003103412687778 val_loss : 0.5398454070091248\n",
      "train_score : 4.7466604883084074e-05 val_score : 3.0591600079787895e-05\n",
      "==================================================\n",
      "Epoch 499\n",
      "train_loss : 0.1393796056509018 val_loss : 0.5461013317108154\n",
      "train_score : 4.749082290800288e-05 val_score : 3.0499855711241253e-05\n",
      "==================================================\n",
      "Epoch 500\n",
      "train_loss : 0.1400308981537819 val_loss : 0.5427307486534119\n",
      "train_score : 4.749302752315998e-05 val_score : 3.062036557821557e-05\n",
      "==================================================\n",
      "Epoch 501\n",
      "train_loss : 0.14013352058827877 val_loss : 0.5438646078109741\n",
      "train_score : 4.749633080791682e-05 val_score : 3.057546564377844e-05\n",
      "==================================================\n",
      "Epoch 502\n",
      "train_loss : 0.1387269590049982 val_loss : 0.5450625419616699\n",
      "train_score : 4.7535704652545974e-05 val_score : 3.0401397452806123e-05\n",
      "==================================================\n",
      "Epoch 503\n",
      "train_loss : 0.13935684598982334 val_loss : 0.5400083065032959\n",
      "train_score : 4.7500852815574035e-05 val_score : 3.0437011446338147e-05\n",
      "==================================================\n",
      "Epoch 504\n",
      "train_loss : 0.13934289291501045 val_loss : 0.5424128770828247\n",
      "train_score : 4.744275065604597e-05 val_score : 3.055350680369884e-05\n",
      "==================================================\n",
      "Epoch 505\n",
      "train_loss : 0.1382947340607643 val_loss : 0.5483776926994324\n",
      "train_score : 4.753016401082277e-05 val_score : 3.0417953894357197e-05\n",
      "==================================================\n",
      "Epoch 506\n",
      "train_loss : 0.14016937837004662 val_loss : 0.540145754814148\n",
      "train_score : 4.748245555674657e-05 val_score : 3.055480192415416e-05\n",
      "==================================================\n",
      "Epoch 507\n",
      "train_loss : 0.13874560594558716 val_loss : 0.545111894607544\n",
      "train_score : 4.741887096315622e-05 val_score : 3.0661423807032406e-05\n",
      "==================================================\n",
      "Epoch 508\n",
      "train_loss : 0.13878403045237064 val_loss : 0.5409770011901855\n",
      "train_score : 4.7529971197946e-05 val_score : 3.044892036996316e-05\n",
      "==================================================\n",
      "Epoch 509\n",
      "train_loss : 0.13886319659650326 val_loss : 0.5381693840026855\n",
      "train_score : 4.753711255034432e-05 val_score : 3.0595401767641306e-05\n",
      "==================================================\n",
      "Epoch 510\n",
      "train_loss : 0.13855682127177715 val_loss : 0.5427066683769226\n",
      "train_score : 4.7509583964711055e-05 val_score : 3.0491411962429993e-05\n",
      "==================================================\n",
      "Epoch 511\n",
      "train_loss : 0.1384637989103794 val_loss : 0.5427113175392151\n",
      "train_score : 4.761603850056417e-05 val_score : 3.0382325348909944e-05\n",
      "==================================================\n",
      "Epoch 512\n",
      "train_loss : 0.13769466988742352 val_loss : 0.5490803718566895\n",
      "train_score : 4.756527050631121e-05 val_score : 3.0475781386485323e-05\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 513\n",
      "train_loss : 0.13827536441385746 val_loss : 0.5438356995582581\n",
      "train_score : 4.739580617751926e-05 val_score : 3.041276431758888e-05\n",
      "==================================================\n",
      "Epoch 514\n",
      "train_loss : 0.13869750685989857 val_loss : 0.5392067432403564\n",
      "train_score : 4.745855403598398e-05 val_score : 3.0475834137178026e-05\n",
      "==================================================\n",
      "Epoch 515\n",
      "train_loss : 0.13692211732268333 val_loss : 0.5549135208129883\n",
      "train_score : 4.7545523557346314e-05 val_score : 3.0506684197462164e-05\n",
      "==================================================\n",
      "Epoch 516\n",
      "train_loss : 0.13750879280269146 val_loss : 0.5425642728805542\n",
      "train_score : 4.750223160954192e-05 val_score : 3.045286939595826e-05\n",
      "==================================================\n",
      "Epoch 517\n",
      "train_loss : 0.13719123043119907 val_loss : 0.547302782535553\n",
      "train_score : 4.754717883770354e-05 val_score : 3.0431547202169895e-05\n",
      "==================================================\n",
      "Epoch 518\n",
      "train_loss : 0.13736602291464806 val_loss : 0.5426485538482666\n",
      "train_score : 4.754741894430481e-05 val_score : 3.0581541068386286e-05\n",
      "==================================================\n",
      "Epoch 519\n",
      "train_loss : 0.13635912910103798 val_loss : 0.5433840751647949\n",
      "train_score : 4.761760283145122e-05 val_score : 3.051303247048054e-05\n",
      "==================================================\n",
      "Epoch 520\n",
      "train_loss : 0.1357346586883068 val_loss : 0.5561426877975464\n",
      "train_score : 4.7592868213541806e-05 val_score : 3.0271226933109574e-05\n",
      "==================================================\n",
      "Epoch 521\n",
      "train_loss : 0.13614232279360294 val_loss : 0.5427258610725403\n",
      "train_score : 4.766881829709746e-05 val_score : 3.052498141187243e-05\n",
      "==================================================\n",
      "Epoch 522\n",
      "train_loss : 0.13597900047898293 val_loss : 0.5459573864936829\n",
      "train_score : 4.769255974679254e-05 val_score : 3.0477933250949718e-05\n",
      "==================================================\n",
      "Epoch 523\n",
      "train_loss : 0.13624091818928719 val_loss : 0.5472149848937988\n",
      "train_score : 4.771487874677405e-05 val_score : 3.0418194000958465e-05\n",
      "==================================================\n",
      "Epoch 524\n",
      "train_loss : 0.13551783561706543 val_loss : 0.5496698021888733\n",
      "train_score : 4.772012107423507e-05 val_score : 3.052073952858336e-05\n",
      "==================================================\n",
      "Epoch 525\n",
      "train_loss : 0.13633740320801735 val_loss : 0.5509368181228638\n",
      "train_score : 4.764142431668006e-05 val_score : 3.064651173190214e-05\n",
      "==================================================\n",
      "Epoch 526\n",
      "train_loss : 0.13561543449759483 val_loss : 0.5473121404647827\n",
      "train_score : 4.7688739869045094e-05 val_score : 3.052284228033386e-05\n",
      "==================================================\n",
      "Epoch 527\n",
      "train_loss : 0.13493209891021252 val_loss : 0.5474264025688171\n",
      "train_score : 4.769284350913949e-05 val_score : 3.0148998121148907e-05\n",
      "==================================================\n",
      "Epoch 528\n",
      "train_loss : 0.13461875915527344 val_loss : 0.5473229885101318\n",
      "train_score : 4.774347326019779e-05 val_score : 3.051972817047499e-05\n",
      "==================================================\n",
      "Epoch 529\n",
      "train_loss : 0.13572656363248825 val_loss : 0.5504991412162781\n",
      "train_score : 4.770260056830011e-05 val_score : 3.045175071747508e-05\n",
      "==================================================\n",
      "Epoch 530\n",
      "train_loss : 0.13500758819282055 val_loss : 0.5531293749809265\n",
      "train_score : 4.7751582314958796e-05 val_score : 3.0433415304287337e-05\n",
      "==================================================\n",
      "Epoch 531\n",
      "train_loss : 0.13496473245322704 val_loss : 0.5476904511451721\n",
      "train_score : 4.770309533341788e-05 val_score : 3.036996531591285e-05\n",
      "==================================================\n",
      "Epoch 532\n",
      "train_loss : 0.13466471433639526 val_loss : 0.5500181317329407\n",
      "train_score : 4.773000910063274e-05 val_score : 3.0484066883218475e-05\n",
      "==================================================\n",
      "Epoch 533\n",
      "train_loss : 0.13453965447843075 val_loss : 0.5497355461120605\n",
      "train_score : 4.782054020324722e-05 val_score : 3.038482100237161e-05\n",
      "==================================================\n",
      "Epoch 534\n",
      "train_loss : 0.13357044104486704 val_loss : 0.5474163293838501\n",
      "train_score : 4.781130701303482e-05 val_score : 3.055612978641875e-05\n",
      "==================================================\n",
      "Epoch 535\n",
      "train_loss : 0.13436923921108246 val_loss : 0.5470536351203918\n",
      "train_score : 4.789443119079806e-05 val_score : 3.0509798307321034e-05\n",
      "==================================================\n",
      "Epoch 536\n",
      "train_loss : 0.13452770560979843 val_loss : 0.5509009957313538\n",
      "train_score : 4.7803881898289546e-05 val_score : 3.036384077859111e-05\n",
      "==================================================\n",
      "Epoch 537\n",
      "train_loss : 0.13345272839069366 val_loss : 0.5538232922554016\n",
      "train_score : 4.7875524614937603e-05 val_score : 3.0446952223428525e-05\n",
      "==================================================\n",
      "Epoch 538\n",
      "train_loss : 0.13387193903326988 val_loss : 0.5557088851928711\n",
      "train_score : 4.7811852709855884e-05 val_score : 3.048085818591062e-05\n",
      "==================================================\n",
      "Epoch 539\n",
      "train_loss : 0.13398047536611557 val_loss : 0.5500392317771912\n",
      "train_score : 4.7694455133751035e-05 val_score : 3.0401346521102823e-05\n",
      "==================================================\n",
      "Epoch 540\n",
      "train_loss : 0.1342026349157095 val_loss : 0.5509571433067322\n",
      "train_score : 4.7772562538739294e-05 val_score : 3.04500681522768e-05\n",
      "==================================================\n",
      "Epoch 541\n",
      "train_loss : 0.13347491063177586 val_loss : 0.5545273423194885\n",
      "train_score : 4.789538070326671e-05 val_score : 3.0501845685648732e-05\n",
      "==================================================\n",
      "Epoch 542\n",
      "train_loss : 0.1331559494137764 val_loss : 0.5491928458213806\n",
      "train_score : 4.7862002247711644e-05 val_score : 3.0472889193333685e-05\n",
      "==================================================\n",
      "Epoch 543\n",
      "train_loss : 0.13265196792781353 val_loss : 0.5501310229301453\n",
      "train_score : 4.7899429773679e-05 val_score : 3.056109562749043e-05\n",
      "==================================================\n",
      "Epoch 544\n",
      "train_loss : 0.13332205265760422 val_loss : 0.5460172295570374\n",
      "train_score : 4.797692963620648e-05 val_score : 3.0494302336592227e-05\n",
      "==================================================\n",
      "Epoch 545\n",
      "train_loss : 0.13354330882430077 val_loss : 0.5527379512786865\n",
      "train_score : 4.7857472964096814e-05 val_score : 3.0393905035452917e-05\n",
      "==================================================\n",
      "Epoch 546\n",
      "train_loss : 0.13308358378708363 val_loss : 0.5497207045555115\n",
      "train_score : 4.79091249871999e-05 val_score : 3.042357275262475e-05\n",
      "==================================================\n",
      "Epoch 547\n",
      "train_loss : 0.13298645056784153 val_loss : 0.5573434233665466\n",
      "train_score : 4.791226456291042e-05 val_score : 3.031847154488787e-05\n",
      "==================================================\n",
      "Epoch 548\n",
      "train_loss : 0.13225951604545116 val_loss : 0.5595918893814087\n",
      "train_score : 4.7970741434255615e-05 val_score : 3.037521855731029e-05\n",
      "==================================================\n",
      "Epoch 549\n",
      "train_loss : 0.13238703273236752 val_loss : 0.5555267930030823\n",
      "train_score : 4.791210449184291e-05 val_score : 3.0497247280436568e-05\n",
      "==================================================\n",
      "Epoch 550\n",
      "train_loss : 0.13248250260949135 val_loss : 0.5511516332626343\n",
      "train_score : 4.790759339812212e-05 val_score : 3.0383747798623517e-05\n",
      "==================================================\n",
      "Epoch 551\n",
      "train_loss : 0.13246562331914902 val_loss : 0.5479716062545776\n",
      "train_score : 4.7964247642084956e-05 val_score : 3.0512410376104526e-05\n",
      "==================================================\n",
      "Epoch 552\n",
      "train_loss : 0.1320627462118864 val_loss : 0.556984543800354\n",
      "train_score : 4.793494736077264e-05 val_score : 3.0442814022535458e-05\n",
      "==================================================\n",
      "Epoch 553\n",
      "train_loss : 0.13224363140761852 val_loss : 0.562531054019928\n",
      "train_score : 4.76827917736955e-05 val_score : 3.0640483601018786e-05\n",
      "==================================================\n",
      "Epoch 554\n",
      "train_loss : 0.13221500907093287 val_loss : 0.5633630156517029\n",
      "train_score : 4.782039832207374e-05 val_score : 3.0329400033224374e-05\n",
      "==================================================\n",
      "Epoch 555\n",
      "train_loss : 0.13223785161972046 val_loss : 0.5505751371383667\n",
      "train_score : 4.79933041788172e-05 val_score : 3.0378494557226077e-05\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 556\n",
      "train_loss : 0.13144316338002682 val_loss : 0.5515187382698059\n",
      "train_score : 4.797441215487197e-05 val_score : 3.0481493013212457e-05\n",
      "==================================================\n",
      "Epoch 557\n",
      "train_loss : 0.13067593052983284 val_loss : 0.5677270889282227\n",
      "train_score : 4.801481918548234e-05 val_score : 3.0338158467202447e-05\n",
      "==================================================\n",
      "Epoch 558\n",
      "train_loss : 0.13077129609882832 val_loss : 0.5627026557922363\n",
      "train_score : 4.797732617589645e-05 val_score : 3.0459344998234883e-05\n",
      "==================================================\n",
      "Epoch 559\n",
      "train_loss : 0.13166656158864498 val_loss : 0.5519614815711975\n",
      "train_score : 4.791963510797359e-05 val_score : 3.049997758353129e-05\n",
      "==================================================\n",
      "Epoch 560\n",
      "train_loss : 0.13176627084612846 val_loss : 0.5633335709571838\n",
      "train_score : 4.786885983776301e-05 val_score : 3.0385137506527826e-05\n",
      "==================================================\n",
      "Epoch 561\n",
      "train_loss : 0.13073712401092052 val_loss : 0.5616192817687988\n",
      "train_score : 4.804881609743461e-05 val_score : 3.0384644560399465e-05\n",
      "==================================================\n",
      "Epoch 562\n",
      "train_loss : 0.13115363381803036 val_loss : 0.5605512261390686\n",
      "train_score : 4.804223499377258e-05 val_score : 3.0348253858392127e-05\n",
      "==================================================\n",
      "Epoch 563\n",
      "train_loss : 0.1312899524345994 val_loss : 0.5577370524406433\n",
      "train_score : 4.796568464371376e-05 val_score : 3.0370660169865005e-05\n",
      "==================================================\n",
      "Epoch 564\n",
      "train_loss : 0.1315384991466999 val_loss : 0.5517149567604065\n",
      "train_score : 4.785651253769174e-05 val_score : 3.0325018087751232e-05\n",
      "==================================================\n",
      "Epoch 565\n",
      "train_loss : 0.13029144424945116 val_loss : 0.5597404837608337\n",
      "train_score : 4.809784149983898e-05 val_score : 3.0490844437736087e-05\n",
      "==================================================\n",
      "Epoch 566\n",
      "train_loss : 0.13023369200527668 val_loss : 0.5543736815452576\n",
      "train_score : 4.7977795475162566e-05 val_score : 3.029480649274774e-05\n",
      "==================================================\n",
      "Epoch 567\n",
      "train_loss : 0.12904900219291449 val_loss : 0.5661441087722778\n",
      "train_score : 4.804464697372168e-05 val_score : 3.02931457554223e-05\n",
      "==================================================\n",
      "Epoch 568\n",
      "train_loss : 0.12940438278019428 val_loss : 0.5572339296340942\n",
      "train_score : 4.8144589527510107e-05 val_score : 3.0438668545684777e-05\n",
      "==================================================\n",
      "Epoch 569\n",
      "train_loss : 0.13044467754662037 val_loss : 0.5624851584434509\n",
      "train_score : 4.8058180254884064e-05 val_score : 3.0397966838791035e-05\n",
      "==================================================\n",
      "Epoch 570\n",
      "train_loss : 0.12930408865213394 val_loss : 0.5621817111968994\n",
      "train_score : 4.8060919652925804e-05 val_score : 3.0347284337040037e-05\n",
      "==================================================\n",
      "Epoch 571\n",
      "train_loss : 0.12996702548116446 val_loss : 0.5508796572685242\n",
      "train_score : 4.80161324958317e-05 val_score : 3.018654024344869e-05\n",
      "==================================================\n",
      "Epoch 572\n",
      "train_loss : 0.1294533060863614 val_loss : 0.5552152395248413\n",
      "train_score : 4.806708966498263e-05 val_score : 3.0484909075312316e-05\n",
      "==================================================\n",
      "Epoch 573\n",
      "train_loss : 0.12941496446728706 val_loss : 0.5627869367599487\n",
      "train_score : 4.8126941692316905e-05 val_score : 3.0241357308113948e-05\n",
      "==================================================\n",
      "Epoch 574\n",
      "train_loss : 0.12833051197230816 val_loss : 0.5678216218948364\n",
      "train_score : 4.815904685528949e-05 val_score : 3.041210220544599e-05\n",
      "==================================================\n",
      "Epoch 575\n",
      "train_loss : 0.12968046683818102 val_loss : 0.56197190284729\n",
      "train_score : 4.8086410970427096e-05 val_score : 3.0349185180966742e-05\n",
      "==================================================\n",
      "Epoch 576\n",
      "train_loss : 0.1278877155855298 val_loss : 0.5659080743789673\n",
      "train_score : 4.811849066754803e-05 val_score : 3.040578667423688e-05\n",
      "==================================================\n",
      "Epoch 577\n",
      "train_loss : 0.12802112940698862 val_loss : 0.55824875831604\n",
      "train_score : 4.8184207116719335e-05 val_score : 3.0451055863522924e-05\n",
      "==================================================\n",
      "Epoch 578\n",
      "train_loss : 0.12747357785701752 val_loss : 0.5603234767913818\n",
      "train_score : 4.8188550863415e-05 val_score : 3.030712468898855e-05\n",
      "==================================================\n",
      "Epoch 579\n",
      "train_loss : 0.12863605003803968 val_loss : 0.5571072101593018\n",
      "train_score : 4.8127916670637205e-05 val_score : 3.0451952625298873e-05\n",
      "==================================================\n",
      "Epoch 580\n",
      "train_loss : 0.12840054649859667 val_loss : 0.5570545792579651\n",
      "train_score : 4.811845064978115e-05 val_score : 3.0496215913444757e-05\n",
      "==================================================\n",
      "Epoch 581\n",
      "train_loss : 0.12874508742243052 val_loss : 0.5634917616844177\n",
      "train_score : 4.813707710127346e-05 val_score : 3.0307828637887724e-05\n",
      "==================================================\n",
      "Epoch 582\n",
      "train_loss : 0.1283380463719368 val_loss : 0.5568836331367493\n",
      "train_score : 4.822519622393884e-05 val_score : 3.0350069209816866e-05\n",
      "==================================================\n",
      "Epoch 583\n",
      "train_loss : 0.12752292957156897 val_loss : 0.5616230964660645\n",
      "train_score : 4.831647311220877e-05 val_score : 3.0439005058724433e-05\n",
      "==================================================\n",
      "Epoch 584\n",
      "train_loss : 0.1268410887569189 val_loss : 0.5632241368293762\n",
      "train_score : 4.822775372304022e-05 val_score : 3.0336152121890336e-05\n",
      "==================================================\n",
      "Epoch 585\n",
      "train_loss : 0.12790398206561804 val_loss : 0.5554724931716919\n",
      "train_score : 4.8133544623851776e-05 val_score : 3.0274622986325994e-05\n",
      "==================================================\n",
      "Epoch 586\n",
      "train_loss : 0.12789026647806168 val_loss : 0.5601691007614136\n",
      "train_score : 4.821094626095146e-05 val_score : 3.0339255317812786e-05\n",
      "==================================================\n",
      "Epoch 587\n",
      "train_loss : 0.12696456164121628 val_loss : 0.5634049773216248\n",
      "train_score : 4.8259513278026134e-05 val_score : 3.0462928407359868e-05\n",
      "==================================================\n",
      "Epoch 588\n",
      "train_loss : 0.12736383453011513 val_loss : 0.5627771615982056\n",
      "train_score : 4.826484291697852e-05 val_score : 3.0390201573027298e-05\n",
      "==================================================\n",
      "Epoch 589\n",
      "train_loss : 0.12726966384798288 val_loss : 0.5594766139984131\n",
      "train_score : 4.8253295972244814e-05 val_score : 3.0367560611921363e-05\n",
      "==================================================\n",
      "Epoch 590\n",
      "train_loss : 0.12691800575703382 val_loss : 0.5617089867591858\n",
      "train_score : 4.816488581127487e-05 val_score : 3.0350018278113566e-05\n",
      "==================================================\n",
      "Epoch 591\n",
      "train_loss : 0.12721958197653294 val_loss : 0.5580065846443176\n",
      "train_score : 4.818257730221376e-05 val_score : 3.0366094506462105e-05\n",
      "==================================================\n",
      "Epoch 592\n",
      "train_loss : 0.12700589187443256 val_loss : 0.5679733157157898\n",
      "train_score : 4.8269343096762896e-05 val_score : 3.044892946491018e-05\n",
      "==================================================\n",
      "Epoch 593\n",
      "train_loss : 0.1259833425283432 val_loss : 0.5614019632339478\n",
      "train_score : 4.832439299207181e-05 val_score : 3.0287508707260713e-05\n",
      "==================================================\n",
      "Epoch 594\n",
      "train_loss : 0.12566066440194845 val_loss : 0.5614359974861145\n",
      "train_score : 4.834406354348175e-05 val_score : 3.029044637514744e-05\n",
      "==================================================\n",
      "Epoch 595\n",
      "train_loss : 0.12558151129633188 val_loss : 0.5679932236671448\n",
      "train_score : 4.8347472329624e-05 val_score : 3.0339162549353205e-05\n",
      "==================================================\n",
      "Epoch 596\n",
      "train_loss : 0.12519478891044855 val_loss : 0.5647040605545044\n",
      "train_score : 4.8264453653246164e-05 val_score : 3.0340092052938417e-05\n",
      "==================================================\n",
      "Epoch 597\n",
      "train_loss : 0.12581858038902283 val_loss : 0.5711410641670227\n",
      "train_score : 4.83554758829996e-05 val_score : 3.0326251362566836e-05\n",
      "==================================================\n",
      "Epoch 598\n",
      "train_loss : 0.12588205561041832 val_loss : 0.5636192560195923\n",
      "train_score : 4.827573138754815e-05 val_score : 3.049319639103487e-05\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 599\n",
      "train_loss : 0.12517340946942568 val_loss : 0.5705755352973938\n",
      "train_score : 4.841083864448592e-05 val_score : 3.0458226319751702e-05\n",
      "==================================================\n",
      "Epoch 600\n",
      "train_loss : 0.1254381239414215 val_loss : 0.5646048784255981\n",
      "train_score : 4.835251093027182e-05 val_score : 3.0277782570919953e-05\n",
      "==================================================\n",
      "Epoch 601\n",
      "train_loss : 0.12437055353075266 val_loss : 0.5633788704872131\n",
      "train_score : 4.8322130169253796e-05 val_score : 3.0411023544729687e-05\n",
      "==================================================\n",
      "Epoch 602\n",
      "train_loss : 0.1250803330913186 val_loss : 0.5627683401107788\n",
      "train_score : 4.8394169425591826e-05 val_score : 3.0130040613585152e-05\n",
      "==================================================\n",
      "Epoch 603\n",
      "train_loss : 0.1253219796344638 val_loss : 0.5656310319900513\n",
      "train_score : 4.84108786622528e-05 val_score : 3.043546894332394e-05\n",
      "==================================================\n",
      "Epoch 604\n",
      "train_loss : 0.12431619595736265 val_loss : 0.5723906755447388\n",
      "train_score : 4.840088513446972e-05 val_score : 3.0192257327144034e-05\n",
      "==================================================\n",
      "Epoch 605\n",
      "train_loss : 0.12524063605815172 val_loss : 0.5677115321159363\n",
      "train_score : 4.825397627428174e-05 val_score : 3.0197244996088557e-05\n",
      "==================================================\n",
      "Epoch 606\n",
      "train_loss : 0.12494386918842793 val_loss : 0.5648497939109802\n",
      "train_score : 4.838186941924505e-05 val_score : 3.0286524633993395e-05\n",
      "==================================================\n",
      "Epoch 607\n",
      "train_loss : 0.12464135233312845 val_loss : 0.5693743228912354\n",
      "train_score : 4.8421265091747046e-05 val_score : 3.0465460440609604e-05\n",
      "==================================================\n",
      "Epoch 608\n",
      "train_loss : 0.12483568675816059 val_loss : 0.5728533864021301\n",
      "train_score : 4.829460885957815e-05 val_score : 3.01704385492485e-05\n",
      "==================================================\n",
      "Epoch 609\n",
      "train_loss : 0.12352123018354177 val_loss : 0.572324812412262\n",
      "train_score : 4.8550282372161746e-05 val_score : 3.0415372748393565e-05\n",
      "==================================================\n",
      "Epoch 610\n",
      "train_loss : 0.12404751498252153 val_loss : 0.5716953873634338\n",
      "train_score : 4.8425092245452106e-05 val_score : 3.0283812520792708e-05\n",
      "==================================================\n",
      "Epoch 611\n",
      "train_loss : 0.12467415258288383 val_loss : 0.5702433586120605\n",
      "train_score : 4.8397774662589654e-05 val_score : 3.0354685804923065e-05\n",
      "==================================================\n",
      "Epoch 612\n",
      "train_loss : 0.12416358292102814 val_loss : 0.5683228969573975\n",
      "train_score : 4.848283788305707e-05 val_score : 3.0354869522852823e-05\n",
      "==================================================\n",
      "Epoch 613\n",
      "train_loss : 0.1229431601241231 val_loss : 0.5700085759162903\n",
      "train_score : 4.8451824113726616e-05 val_score : 3.033785651496146e-05\n",
      "==================================================\n",
      "Epoch 614\n",
      "train_loss : 0.12289411295205355 val_loss : 0.5634233951568604\n",
      "train_score : 4.841912232222967e-05 val_score : 3.0268289265222847e-05\n",
      "==================================================\n",
      "Epoch 615\n",
      "train_loss : 0.12230172753334045 val_loss : 0.5682096481323242\n",
      "train_score : 4.85866803501267e-05 val_score : 3.035759618796874e-05\n",
      "==================================================\n",
      "Epoch 616\n",
      "train_loss : 0.1229000510647893 val_loss : 0.5638210773468018\n",
      "train_score : 4.848460594075732e-05 val_score : 3.0257797334343195e-05\n",
      "==================================================\n",
      "Epoch 617\n",
      "train_loss : 0.12233742326498032 val_loss : 0.5696118474006653\n",
      "train_score : 4.848018943448551e-05 val_score : 3.038072827621363e-05\n",
      "==================================================\n",
      "Epoch 618\n",
      "train_loss : 0.12304345704615116 val_loss : 0.5736262798309326\n",
      "train_score : 4.858570173382759e-05 val_score : 3.0292269002529792e-05\n",
      "==================================================\n",
      "Epoch 619\n",
      "train_loss : 0.12202095799148083 val_loss : 0.5686632990837097\n",
      "train_score : 4.848792013945058e-05 val_score : 3.039402872673236e-05\n",
      "==================================================\n",
      "Epoch 620\n",
      "train_loss : 0.123084950260818 val_loss : 0.5684835314750671\n",
      "train_score : 4.8482936108484864e-05 val_score : 3.031861524505075e-05\n",
      "==================================================\n",
      "Epoch 621\n",
      "train_loss : 0.12195215467363596 val_loss : 0.5796590447425842\n",
      "train_score : 4.856687519350089e-05 val_score : 3.032322092622053e-05\n",
      "==================================================\n",
      "Epoch 622\n",
      "train_loss : 0.12219339795410633 val_loss : 0.5752190947532654\n",
      "train_score : 4.849233664572239e-05 val_score : 3.020387885044329e-05\n",
      "==================================================\n",
      "Epoch 623\n",
      "train_loss : 0.1219164403155446 val_loss : 0.5648659467697144\n",
      "train_score : 4.852156052947976e-05 val_score : 3.032108725165017e-05\n",
      "==================================================\n",
      "Epoch 624\n",
      "train_loss : 0.12191957421600819 val_loss : 0.5712472200393677\n",
      "train_score : 4.8527326725889e-05 val_score : 3.0442526622209698e-05\n",
      "==================================================\n",
      "Epoch 625\n",
      "train_loss : 0.12149044964462519 val_loss : 0.5745392441749573\n",
      "train_score : 4.846259980695322e-05 val_score : 3.025991281901952e-05\n",
      "==================================================\n",
      "Epoch 626\n",
      "train_loss : 0.12135059293359518 val_loss : 0.5772716999053955\n",
      "train_score : 4.8541503929300234e-05 val_score : 3.0289354981505312e-05\n",
      "==================================================\n",
      "Epoch 627\n",
      "train_loss : 0.12174256984144449 val_loss : 0.5712986588478088\n",
      "train_score : 4.855625593336299e-05 val_score : 3.0270928618847392e-05\n",
      "==================================================\n",
      "Epoch 628\n",
      "train_loss : 0.12170583847910166 val_loss : 0.5739650726318359\n",
      "train_score : 4.8556688852841035e-05 val_score : 3.0260345738497563e-05\n",
      "==================================================\n",
      "Epoch 629\n",
      "train_loss : 0.12204907648265362 val_loss : 0.5628909468650818\n",
      "train_score : 4.864309448748827e-05 val_score : 3.0179951863829046e-05\n",
      "==================================================\n",
      "Epoch 630\n",
      "train_loss : 0.12108212150633335 val_loss : 0.579424262046814\n",
      "train_score : 4.859921318711713e-05 val_score : 3.0247476388467476e-05\n",
      "==================================================\n",
      "Epoch 631\n",
      "train_loss : 0.12163161113858223 val_loss : 0.5755499005317688\n",
      "train_score : 4.850443292525597e-05 val_score : 3.035319605260156e-05\n",
      "==================================================\n",
      "Epoch 632\n",
      "train_loss : 0.1215775040909648 val_loss : 0.5786784291267395\n",
      "train_score : 4.8594814870739356e-05 val_score : 3.0294922908069566e-05\n",
      "==================================================\n",
      "Epoch 633\n",
      "train_loss : 0.12053496483713388 val_loss : 0.5707895755767822\n",
      "train_score : 4.8558089474681765e-05 val_score : 3.029666368092876e-05\n",
      "==================================================\n",
      "Epoch 634\n",
      "train_loss : 0.12048549950122833 val_loss : 0.5702370405197144\n",
      "train_score : 4.869100666837767e-05 val_score : 3.012131310242694e-05\n",
      "==================================================\n",
      "Epoch 635\n",
      "train_loss : 0.120844766497612 val_loss : 0.5729478001594543\n",
      "train_score : 4.861242632614449e-05 val_score : 3.045793346245773e-05\n",
      "==================================================\n",
      "Epoch 636\n",
      "train_loss : 0.11934705264866352 val_loss : 0.5869203805923462\n",
      "train_score : 4.8689569666748866e-05 val_score : 3.0218516258173622e-05\n",
      "==================================================\n",
      "Epoch 637\n",
      "train_loss : 0.12024911865592003 val_loss : 0.581032395362854\n",
      "train_score : 4.872008867096156e-05 val_score : 3.032154927495867e-05\n",
      "==================================================\n",
      "Epoch 638\n",
      "train_loss : 0.12022417411208153 val_loss : 0.5793503522872925\n",
      "train_score : 4.8636746214469895e-05 val_score : 3.0347129722940736e-05\n",
      "==================================================\n",
      "Epoch 639\n",
      "train_loss : 0.1209248835220933 val_loss : 0.5740754008293152\n",
      "train_score : 4.861090928898193e-05 val_score : 3.0179346140357666e-05\n",
      "==================================================\n",
      "Epoch 640\n",
      "train_loss : 0.12000064086169004 val_loss : 0.5710487365722656\n",
      "train_score : 4.8631616664351895e-05 val_score : 3.0179638997651637e-05\n",
      "==================================================\n",
      "Epoch 641\n",
      "train_loss : 0.1197412945330143 val_loss : 0.5694547295570374\n",
      "train_score : 4.859900218434632e-05 val_score : 3.023132376256399e-05\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 642\n",
      "train_loss : 0.11992396228015423 val_loss : 0.5794950127601624\n",
      "train_score : 4.8655234422767535e-05 val_score : 3.0379884265130386e-05\n",
      "==================================================\n",
      "Epoch 643\n",
      "train_loss : 0.11924680136144161 val_loss : 0.5796191692352295\n",
      "train_score : 4.867069219471887e-05 val_score : 3.0367265935637988e-05\n",
      "==================================================\n",
      "Epoch 644\n",
      "train_loss : 0.11932752188295126 val_loss : 0.5803840160369873\n",
      "train_score : 4.86897915834561e-05 val_score : 3.0265697205322795e-05\n",
      "==================================================\n",
      "Epoch 645\n",
      "train_loss : 0.11907190270721912 val_loss : 0.5823606252670288\n",
      "train_score : 4.861086927121505e-05 val_score : 3.0218872780096717e-05\n",
      "==================================================\n",
      "Epoch 646\n",
      "train_loss : 0.11865623202174902 val_loss : 0.5750429630279541\n",
      "train_score : 4.871671262662858e-05 val_score : 3.024914622073993e-05\n",
      "==================================================\n",
      "Epoch 647\n",
      "train_loss : 0.11939684394747019 val_loss : 0.5746678113937378\n",
      "train_score : 4.876197999692522e-05 val_score : 3.0418446840485558e-05\n",
      "==================================================\n",
      "Epoch 648\n",
      "train_loss : 0.11831188667565584 val_loss : 0.5759466290473938\n",
      "train_score : 4.876924867858179e-05 val_score : 3.0247367249103263e-05\n",
      "==================================================\n",
      "Epoch 649\n",
      "train_loss : 0.11895422730594873 val_loss : 0.5789793133735657\n",
      "train_score : 4.8763286031316966e-05 val_score : 3.0194622013368644e-05\n",
      "==================================================\n",
      "Epoch 650\n",
      "train_loss : 0.11834424640983343 val_loss : 0.5886978507041931\n",
      "train_score : 4.870976772508584e-05 val_score : 3.0358904041349888e-05\n",
      "==================================================\n",
      "Epoch 651\n",
      "train_loss : 0.11795076075941324 val_loss : 0.5841717720031738\n",
      "train_score : 4.880217238678597e-05 val_score : 2.991542532981839e-05\n",
      "==================================================\n",
      "Epoch 652\n",
      "train_loss : 0.11739654466509819 val_loss : 0.5799160599708557\n",
      "train_score : 4.880696360487491e-05 val_score : 3.0258383048931137e-05\n",
      "==================================================\n",
      "Epoch 653\n",
      "train_loss : 0.11835501994937658 val_loss : 0.5826411247253418\n",
      "train_score : 4.8751004214864224e-05 val_score : 3.0143010008032434e-05\n",
      "==================================================\n",
      "Epoch 654\n",
      "train_loss : 0.11756606306880713 val_loss : 0.5839297771453857\n",
      "train_score : 4.862584319198504e-05 val_score : 3.0450224585365504e-05\n",
      "==================================================\n",
      "Epoch 655\n",
      "train_loss : 0.11763941776007414 val_loss : 0.5832191705703735\n",
      "train_score : 4.885547605226748e-05 val_score : 3.0118662834865972e-05\n",
      "==================================================\n",
      "Epoch 656\n",
      "train_loss : 0.1178106414154172 val_loss : 0.5889208316802979\n",
      "train_score : 4.880943015450612e-05 val_score : 3.0258834158303216e-05\n",
      "==================================================\n",
      "Epoch 657\n",
      "train_loss : 0.11830064840614796 val_loss : 0.5829402208328247\n",
      "train_score : 4.8661815526429564e-05 val_score : 3.0202247216948308e-05\n",
      "==================================================\n",
      "Epoch 658\n",
      "train_loss : 0.11816017236560583 val_loss : 0.5796952247619629\n",
      "train_score : 4.872341742157005e-05 val_score : 3.050932900805492e-05\n",
      "==================================================\n",
      "Epoch 659\n",
      "train_loss : 0.11849063076078892 val_loss : 0.5810325741767883\n",
      "train_score : 4.876698221778497e-05 val_score : 3.0140943636070006e-05\n",
      "==================================================\n",
      "Epoch 660\n",
      "train_loss : 0.11697194259613752 val_loss : 0.5924526453018188\n",
      "train_score : 4.8824531404534355e-05 val_score : 3.0331746529554948e-05\n",
      "==================================================\n",
      "Epoch 661\n",
      "train_loss : 0.11714626289904118 val_loss : 0.5868208408355713\n",
      "train_score : 4.882551729679108e-05 val_score : 3.0129724109428935e-05\n",
      "==================================================\n",
      "Epoch 662\n",
      "train_loss : 0.11724573746323586 val_loss : 0.5845245122909546\n",
      "train_score : 4.8866022552829236e-05 val_score : 3.0314913601614535e-05\n",
      "==================================================\n",
      "Epoch 663\n",
      "train_loss : 0.11711449082940817 val_loss : 0.5756971836090088\n",
      "train_score : 4.883822475676425e-05 val_score : 3.0177072403603233e-05\n",
      "==================================================\n",
      "Epoch 664\n",
      "train_loss : 0.11730925366282463 val_loss : 0.5792682766914368\n",
      "train_score : 4.883039218839258e-05 val_score : 3.0423194402828813e-05\n",
      "==================================================\n",
      "Epoch 665\n",
      "train_loss : 0.11615851242095232 val_loss : 0.5900010466575623\n",
      "train_score : 4.879437983618118e-05 val_score : 3.028260289283935e-05\n",
      "==================================================\n",
      "Epoch 666\n",
      "train_loss : 0.11595404427498579 val_loss : 0.5812076330184937\n",
      "train_score : 4.891941716778092e-05 val_score : 3.013341483892873e-05\n",
      "==================================================\n",
      "Epoch 667\n",
      "train_loss : 0.11629060283303261 val_loss : 0.579790472984314\n",
      "train_score : 4.8889116442296654e-05 val_score : 3.0346991479746066e-05\n",
      "==================================================\n",
      "Epoch 668\n",
      "train_loss : 0.11546911764889956 val_loss : 0.5822139382362366\n",
      "train_score : 4.8944268201012164e-05 val_score : 3.020713120349683e-05\n",
      "==================================================\n",
      "Epoch 669\n",
      "train_loss : 0.11624310072511435 val_loss : 0.5831806659698486\n",
      "train_score : 4.884279042016715e-05 val_score : 3.0377437724382617e-05\n",
      "==================================================\n",
      "Epoch 670\n",
      "train_loss : 0.11592821590602398 val_loss : 0.5851929187774658\n",
      "train_score : 4.881473432760686e-05 val_score : 3.041307172679808e-05\n",
      "==================================================\n",
      "Epoch 671\n",
      "train_loss : 0.11571065243333578 val_loss : 0.5825934410095215\n",
      "train_score : 4.8945519665721804e-05 val_score : 3.0070295906625688e-05\n",
      "==================================================\n",
      "Epoch 672\n",
      "train_loss : 0.11549222655594349 val_loss : 0.5825173258781433\n",
      "train_score : 4.898855331703089e-05 val_score : 3.0109849831205793e-05\n",
      "==================================================\n",
      "Epoch 673\n",
      "train_loss : 0.11477947048842907 val_loss : 0.5830720663070679\n",
      "train_score : 4.894441008218564e-05 val_score : 3.050821032957174e-05\n",
      "==================================================\n",
      "Epoch 674\n",
      "train_loss : 0.11657565832138062 val_loss : 0.5843585133552551\n",
      "train_score : 4.892374272458255e-05 val_score : 3.0291326766018756e-05\n",
      "==================================================\n",
      "Epoch 675\n",
      "train_loss : 0.11484327912330627 val_loss : 0.5867575407028198\n",
      "train_score : 4.888150579063222e-05 val_score : 3.0291268558357842e-05\n",
      "==================================================\n",
      "Epoch 676\n",
      "train_loss : 0.1155699510127306 val_loss : 0.5840779542922974\n",
      "train_score : 4.8977202823152766e-05 val_score : 3.0321534723043442e-05\n",
      "==================================================\n",
      "Epoch 677\n",
      "train_loss : 0.1153987618163228 val_loss : 0.5850521922111511\n",
      "train_score : 4.8975238314596936e-05 val_score : 3.0326860724017024e-05\n",
      "==================================================\n",
      "Epoch 678\n",
      "train_loss : 0.11552012991160154 val_loss : 0.5972901582717896\n",
      "train_score : 4.8945406888378784e-05 val_score : 3.0000604965607636e-05\n",
      "==================================================\n",
      "Epoch 679\n",
      "train_loss : 0.11455529648810625 val_loss : 0.5911092758178711\n",
      "train_score : 4.902613363810815e-05 val_score : 3.030320476682391e-05\n",
      "==================================================\n",
      "Epoch 680\n",
      "train_loss : 0.11477634310722351 val_loss : 0.5891826748847961\n",
      "train_score : 4.8932157369563356e-05 val_score : 2.992844019900076e-05\n",
      "==================================================\n",
      "Epoch 681\n",
      "train_loss : 0.11450831964612007 val_loss : 0.5819253325462341\n",
      "train_score : 4.898037877865136e-05 val_score : 3.0188872187864035e-05\n",
      "==================================================\n",
      "Epoch 682\n",
      "train_loss : 0.11339827533811331 val_loss : 0.5916067361831665\n",
      "train_score : 4.897636245004833e-05 val_score : 3.018402094312478e-05\n",
      "==================================================\n",
      "Epoch 683\n",
      "train_loss : 0.11482952069491148 val_loss : 0.5942745208740234\n",
      "train_score : 4.89485428261105e-05 val_score : 3.0359062293427996e-05\n",
      "==================================================\n",
      "Epoch 684\n",
      "train_loss : 0.11459662951529026 val_loss : 0.593604564666748\n",
      "train_score : 4.895626625511795e-05 val_score : 3.0076987968641333e-05\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 685\n",
      "train_loss : 0.11394209414720535 val_loss : 0.5850636959075928\n",
      "train_score : 4.90798702230677e-05 val_score : 3.0218372558010742e-05\n",
      "==================================================\n",
      "Epoch 686\n",
      "train_loss : 0.1139706652611494 val_loss : 0.5844943523406982\n",
      "train_score : 4.8952260840451345e-05 val_score : 3.0313714887597598e-05\n",
      "==================================================\n",
      "Epoch 687\n",
      "train_loss : 0.11317128222435713 val_loss : 0.5820527076721191\n",
      "train_score : 4.9094000132754445e-05 val_score : 3.0196633815648966e-05\n",
      "==================================================\n",
      "Epoch 688\n",
      "train_loss : 0.1131667923182249 val_loss : 0.585059404373169\n",
      "train_score : 4.9150014092447236e-05 val_score : 3.0197757951100357e-05\n",
      "==================================================\n",
      "Epoch 689\n",
      "train_loss : 0.11351157072931528 val_loss : 0.5944361686706543\n",
      "train_score : 4.90961319883354e-05 val_score : 3.023810677404981e-05\n",
      "==================================================\n",
      "Epoch 690\n",
      "train_loss : 0.11333447229117155 val_loss : 0.5864129662513733\n",
      "train_score : 4.908275150228292e-05 val_score : 3.0009570764377713e-05\n",
      "==================================================\n",
      "Epoch 691\n",
      "train_loss : 0.11332959029823542 val_loss : 0.5895205140113831\n",
      "train_score : 4.9066231440519914e-05 val_score : 3.0089935535215773e-05\n",
      "==================================================\n",
      "Epoch 692\n",
      "train_loss : 0.1135249175131321 val_loss : 0.5917753577232361\n",
      "train_score : 4.891270509688184e-05 val_score : 3.0037355827516876e-05\n",
      "==================================================\n",
      "Epoch 693\n",
      "train_loss : 0.1124805398285389 val_loss : 0.5908746123313904\n",
      "train_score : 4.91855971631594e-05 val_score : 3.001252844114788e-05\n",
      "==================================================\n",
      "Epoch 694\n",
      "train_loss : 0.11274554021656513 val_loss : 0.587205708026886\n",
      "train_score : 4.902426735498011e-05 val_score : 3.0375411370187066e-05\n",
      "==================================================\n",
      "Epoch 695\n",
      "train_loss : 0.11182598397135735 val_loss : 0.5992816686630249\n",
      "train_score : 4.908051778329536e-05 val_score : 3.0117724236333743e-05\n",
      "==================================================\n",
      "Epoch 696\n",
      "train_loss : 0.11264212615787983 val_loss : 0.5926750898361206\n",
      "train_score : 4.907023685518652e-05 val_score : 3.0114975743344985e-05\n",
      "==================================================\n",
      "Epoch 697\n",
      "train_loss : 0.11237095762044191 val_loss : 0.5963495373725891\n",
      "train_score : 4.9118018068838865e-05 val_score : 3.0158791560097598e-05\n",
      "==================================================\n",
      "Epoch 698\n",
      "train_loss : 0.11257873754948378 val_loss : 0.5946088433265686\n",
      "train_score : 4.911051655653864e-05 val_score : 3.0101211450528353e-05\n",
      "==================================================\n",
      "Epoch 699\n",
      "train_loss : 0.11162174679338932 val_loss : 0.595085620880127\n",
      "train_score : 4.924077802570537e-05 val_score : 3.0160161259118468e-05\n",
      "==================================================\n",
      "Epoch 700\n",
      "train_loss : 0.11184125300496817 val_loss : 0.5922667384147644\n",
      "train_score : 4.903939407086e-05 val_score : 3.034220208064653e-05\n",
      "==================================================\n",
      "Epoch 701\n",
      "train_loss : 0.1120915338397026 val_loss : 0.592022180557251\n",
      "train_score : 4.908295886707492e-05 val_score : 2.9999579055584036e-05\n",
      "==================================================\n",
      "Epoch 702\n",
      "train_loss : 0.11259188130497932 val_loss : 0.5888879299163818\n",
      "train_score : 4.901772990706377e-05 val_score : 3.0405206416617148e-05\n",
      "==================================================\n",
      "Epoch 703\n",
      "train_loss : 0.11230671033263206 val_loss : 0.6050196886062622\n",
      "train_score : 4.8987898480845615e-05 val_score : 3.0266975954873487e-05\n",
      "==================================================\n",
      "Epoch 704\n",
      "train_loss : 0.11232179589569569 val_loss : 0.5981147289276123\n",
      "train_score : 4.913013617624529e-05 val_score : 3.0083097954047844e-05\n",
      "==================================================\n",
      "Epoch 705\n",
      "train_loss : 0.11240200512111187 val_loss : 0.5895146131515503\n",
      "train_score : 4.91098289785441e-05 val_score : 3.0096667614998296e-05\n",
      "==================================================\n",
      "Epoch 706\n",
      "train_loss : 0.11170643847435713 val_loss : 0.5925199389457703\n",
      "train_score : 4.917891783406958e-05 val_score : 3.0097273338469677e-05\n",
      "==================================================\n",
      "Epoch 707\n",
      "train_loss : 0.11100772581994534 val_loss : 0.5963442921638489\n",
      "train_score : 4.914659439236857e-05 val_score : 3.0183686249074526e-05\n",
      "==================================================\n",
      "Epoch 708\n",
      "train_loss : 0.11070186644792557 val_loss : 0.5923620462417603\n",
      "train_score : 4.918689592159353e-05 val_score : 3.0052287911530584e-05\n",
      "==================================================\n",
      "Epoch 709\n",
      "train_loss : 0.11036351323127747 val_loss : 0.598006546497345\n",
      "train_score : 4.9259055231232196e-05 val_score : 3.012546949321404e-05\n",
      "==================================================\n",
      "Epoch 710\n",
      "train_loss : 0.11052128113806248 val_loss : 0.5983918905258179\n",
      "train_score : 4.92228391522076e-05 val_score : 3.0225561204133555e-05\n",
      "==================================================\n",
      "Epoch 711\n",
      "train_loss : 0.1104572806507349 val_loss : 0.6010286808013916\n",
      "train_score : 4.918510603602044e-05 val_score : 2.990337998198811e-05\n",
      "==================================================\n",
      "Epoch 712\n",
      "train_loss : 0.11104784067720175 val_loss : 0.5859670639038086\n",
      "train_score : 4.918767444905825e-05 val_score : 3.0196757506928407e-05\n",
      "==================================================\n",
      "Epoch 713\n",
      "train_loss : 0.11116151977330446 val_loss : 0.593803346157074\n",
      "train_score : 4.915115641779266e-05 val_score : 2.9926486604381353e-05\n",
      "==================================================\n",
      "Epoch 714\n",
      "train_loss : 0.11014581378549337 val_loss : 0.5973459482192993\n",
      "train_score : 4.92574145027902e-05 val_score : 3.0197919841157272e-05\n",
      "==================================================\n",
      "Epoch 715\n",
      "train_loss : 0.10956353042274714 val_loss : 0.6050796508789062\n",
      "train_score : 4.9276917707175016e-05 val_score : 3.0075127142481506e-05\n",
      "==================================================\n",
      "Epoch 716\n",
      "train_loss : 0.10973920859396458 val_loss : 0.5975290536880493\n",
      "train_score : 4.9282098188996315e-05 val_score : 3.0165207135723904e-05\n",
      "==================================================\n",
      "Epoch 717\n",
      "train_loss : 0.11062225420027971 val_loss : 0.5974313020706177\n",
      "train_score : 4.920309584122151e-05 val_score : 3.0128883736324497e-05\n",
      "==================================================\n",
      "Epoch 718\n",
      "train_loss : 0.11010042857378721 val_loss : 0.5985963940620422\n",
      "train_score : 4.9231373850489035e-05 val_score : 3.0100987714831717e-05\n",
      "==================================================\n",
      "Epoch 719\n",
      "train_loss : 0.10917409043759108 val_loss : 0.5956069231033325\n",
      "train_score : 4.9282734835287556e-05 val_score : 2.986363870149944e-05\n",
      "==================================================\n",
      "Epoch 720\n",
      "train_loss : 0.1094578206539154 val_loss : 0.6019546985626221\n",
      "train_score : 4.923314554616809e-05 val_score : 3.020014992216602e-05\n",
      "==================================================\n",
      "Epoch 721\n",
      "train_loss : 0.10940102022141218 val_loss : 0.5958247780799866\n",
      "train_score : 4.918629929306917e-05 val_score : 3.0345416234922595e-05\n",
      "==================================================\n",
      "Epoch 722\n",
      "train_loss : 0.11063067335635424 val_loss : 0.5996744632720947\n",
      "train_score : 4.9275029596174136e-05 val_score : 3.0301534934551455e-05\n",
      "==================================================\n",
      "Epoch 723\n",
      "train_loss : 0.10916636697947979 val_loss : 0.5995347499847412\n",
      "train_score : 4.912522490485571e-05 val_score : 3.0083159799687564e-05\n",
      "==================================================\n",
      "Epoch 724\n",
      "train_loss : 0.10930093377828598 val_loss : 0.5922850370407104\n",
      "train_score : 4.935753167956136e-05 val_score : 3.0084822356002405e-05\n",
      "==================================================\n",
      "Epoch 725\n",
      "train_loss : 0.10962844360619783 val_loss : 0.6014386415481567\n",
      "train_score : 4.929171336698346e-05 val_score : 3.006435690622311e-05\n",
      "==================================================\n",
      "Epoch 726\n",
      "train_loss : 0.10986115038394928 val_loss : 0.5922699570655823\n",
      "train_score : 4.938561687595211e-05 val_score : 3.0250477720983326e-05\n",
      "==================================================\n",
      "Epoch 727\n",
      "train_loss : 0.10939409025013447 val_loss : 0.60189288854599\n",
      "train_score : 4.9399332056054845e-05 val_score : 3.0102208256721497e-05\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 728\n",
      "train_loss : 0.10861929785460234 val_loss : 0.6076965928077698\n",
      "train_score : 4.928882481181063e-05 val_score : 3.018849020008929e-05\n",
      "==================================================\n",
      "Epoch 729\n",
      "train_loss : 0.10812539421021938 val_loss : 0.6099753379821777\n",
      "train_score : 4.934252501698211e-05 val_score : 3.013430796272587e-05\n",
      "==================================================\n",
      "Epoch 730\n",
      "train_loss : 0.10792516078799963 val_loss : 0.6024901866912842\n",
      "train_score : 4.938420170219615e-05 val_score : 3.0245326342992485e-05\n",
      "==================================================\n",
      "Epoch 731\n",
      "train_loss : 0.10800335183739662 val_loss : 0.6013035774230957\n",
      "train_score : 4.940043072565459e-05 val_score : 2.9990076654939912e-05\n",
      "==================================================\n",
      "Epoch 732\n",
      "train_loss : 0.10738530196249485 val_loss : 0.6053680777549744\n",
      "train_score : 4.950417496729642e-05 val_score : 3.0190258257789537e-05\n",
      "==================================================\n",
      "Epoch 733\n",
      "train_loss : 0.1081210495904088 val_loss : 0.6020607948303223\n",
      "train_score : 4.9392303480999544e-05 val_score : 2.9992721465532668e-05\n",
      "==================================================\n",
      "Epoch 734\n",
      "train_loss : 0.10802049655467272 val_loss : 0.6067825555801392\n",
      "train_score : 4.93284132971894e-05 val_score : 3.0422950658248737e-05\n",
      "==================================================\n",
      "Epoch 735\n",
      "train_loss : 0.10745622403919697 val_loss : 0.6134734749794006\n",
      "train_score : 4.945238106301986e-05 val_score : 3.0064815291552804e-05\n",
      "==================================================\n",
      "Epoch 736\n",
      "train_loss : 0.10722934734076262 val_loss : 0.6038588285446167\n",
      "train_score : 4.949164213030599e-05 val_score : 3.0342265745275654e-05\n",
      "==================================================\n",
      "Epoch 737\n",
      "train_loss : 0.10772661864757538 val_loss : 0.6027884483337402\n",
      "train_score : 4.9360605771653354e-05 val_score : 2.9984421416884288e-05\n",
      "==================================================\n",
      "Epoch 738\n",
      "train_loss : 0.10816828813403845 val_loss : 0.596963107585907\n",
      "train_score : 4.945674663758837e-05 val_score : 2.9986880690557882e-05\n",
      "==================================================\n",
      "Epoch 739\n",
      "train_loss : 0.10839213896542788 val_loss : 0.6016048789024353\n",
      "train_score : 4.933493983116932e-05 val_score : 3.0175646315910853e-05\n",
      "==================================================\n",
      "Epoch 740\n",
      "train_loss : 0.10829967353492975 val_loss : 0.6103284955024719\n",
      "train_score : 4.9416983529226854e-05 val_score : 3.0212004276108928e-05\n",
      "==================================================\n",
      "Epoch 741\n",
      "train_loss : 0.10749738197773695 val_loss : 0.6043128967285156\n",
      "train_score : 4.938877100357786e-05 val_score : 3.0329563742270693e-05\n",
      "==================================================\n",
      "Epoch 742\n",
      "train_loss : 0.10773506294935942 val_loss : 0.6070325970649719\n",
      "train_score : 4.9492849939269945e-05 val_score : 3.013885725522414e-05\n",
      "==================================================\n",
      "Epoch 743\n",
      "train_loss : 0.10661059338599443 val_loss : 0.6071203947067261\n",
      "train_score : 4.950205038767308e-05 val_score : 3.0004954169271514e-05\n",
      "==================================================\n",
      "Epoch 744\n",
      "train_loss : 0.10648811515420675 val_loss : 0.6076034307479858\n",
      "train_score : 4.951283699483611e-05 val_score : 3.0046829124330543e-05\n",
      "==================================================\n",
      "Epoch 745\n",
      "train_loss : 0.10656909178942442 val_loss : 0.6045900583267212\n",
      "train_score : 4.9464968469692394e-05 val_score : 2.999939169967547e-05\n",
      "==================================================\n",
      "Epoch 746\n",
      "train_loss : 0.10622645169496536 val_loss : 0.600029706954956\n",
      "train_score : 4.9605267122387886e-05 val_score : 3.0232682547648437e-05\n",
      "==================================================\n",
      "Epoch 747\n",
      "train_loss : 0.10690644197165966 val_loss : 0.6075834631919861\n",
      "train_score : 4.942589657730423e-05 val_score : 3.0071954824961722e-05\n",
      "==================================================\n",
      "Epoch 748\n",
      "train_loss : 0.10621044877916574 val_loss : 0.6000796556472778\n",
      "train_score : 4.951984010403976e-05 val_score : 3.0237195460358635e-05\n",
      "==================================================\n",
      "Epoch 749\n",
      "train_loss : 0.10638467874377966 val_loss : 0.6153252720832825\n",
      "train_score : 4.945799082634039e-05 val_score : 3.0055018214625306e-05\n",
      "==================================================\n",
      "Epoch 750\n",
      "train_loss : 0.10676902905106544 val_loss : 0.6024101376533508\n",
      "train_score : 4.94616360811051e-05 val_score : 3.034680958080571e-05\n",
      "==================================================\n",
      "Epoch 751\n",
      "train_loss : 0.1067856214940548 val_loss : 0.6096857190132141\n",
      "train_score : 4.949867070536129e-05 val_score : 3.0062614314374514e-05\n",
      "==================================================\n",
      "Epoch 752\n",
      "train_loss : 0.10585985146462917 val_loss : 0.6181445717811584\n",
      "train_score : 4.963609899277799e-05 val_score : 2.9984534194227308e-05\n",
      "==================================================\n",
      "Epoch 753\n",
      "train_loss : 0.10559557471424341 val_loss : 0.6113254427909851\n",
      "train_score : 4.9584654334466904e-05 val_score : 3.02628777717473e-05\n",
      "==================================================\n",
      "Epoch 754\n",
      "train_loss : 0.10558464471250772 val_loss : 0.6162517070770264\n",
      "train_score : 4.958782301400788e-05 val_score : 3.0050994610064663e-05\n",
      "==================================================\n",
      "Epoch 755\n",
      "train_loss : 0.10607080534100533 val_loss : 0.6023874878883362\n",
      "train_score : 4.93548832309898e-05 val_score : 3.0064975362620316e-05\n",
      "==================================================\n",
      "Epoch 756\n",
      "train_loss : 0.10586989019066095 val_loss : 0.6042160391807556\n",
      "train_score : 4.9525198846822605e-05 val_score : 3.0096709451754577e-05\n",
      "==================================================\n",
      "Epoch 757\n",
      "train_loss : 0.10574707388877869 val_loss : 0.6106773614883423\n",
      "train_score : 4.962398452335037e-05 val_score : 3.0060291464906186e-05\n",
      "==================================================\n",
      "Epoch 758\n",
      "train_loss : 0.1058115130290389 val_loss : 0.5988578200340271\n",
      "train_score : 4.961010199622251e-05 val_score : 3.007415398315061e-05\n",
      "==================================================\n",
      "Epoch 759\n",
      "train_loss : 0.10541151370853186 val_loss : 0.6046333312988281\n",
      "train_score : 4.956765042152256e-05 val_score : 3.009688953170553e-05\n",
      "==================================================\n",
      "Epoch 760\n",
      "train_loss : 0.10419612843543291 val_loss : 0.6158157587051392\n",
      "train_score : 4.9608031986281276e-05 val_score : 2.993909947690554e-05\n",
      "==================================================\n",
      "Epoch 761\n",
      "train_loss : 0.10528410505503416 val_loss : 0.6097897291183472\n",
      "train_score : 4.959032958140597e-05 val_score : 3.021721749973949e-05\n",
      "==================================================\n",
      "Epoch 762\n",
      "train_loss : 0.10395699366927147 val_loss : 0.609315037727356\n",
      "train_score : 4.959738362231292e-05 val_score : 3.0066661565797403e-05\n",
      "==================================================\n",
      "Epoch 763\n",
      "train_loss : 0.10493193473666906 val_loss : 0.6066908836364746\n",
      "train_score : 4.9503621994517744e-05 val_score : 3.0065488317632116e-05\n",
      "==================================================\n",
      "Epoch 764\n",
      "train_loss : 0.10488949902355671 val_loss : 0.6177346706390381\n",
      "train_score : 4.9622969527263194e-05 val_score : 2.9967521186335944e-05\n",
      "==================================================\n",
      "Epoch 765\n",
      "train_loss : 0.1052072448655963 val_loss : 0.6117858290672302\n",
      "train_score : 4.9614551244303584e-05 val_score : 3.0029386834939942e-05\n",
      "==================================================\n",
      "Epoch 766\n",
      "train_loss : 0.10369265824556351 val_loss : 0.6161441206932068\n",
      "train_score : 4.967044878867455e-05 val_score : 3.0098117349552922e-05\n",
      "==================================================\n",
      "Epoch 767\n",
      "train_loss : 0.10267104674130678 val_loss : 0.6199738383293152\n",
      "train_score : 4.971545786247589e-05 val_score : 2.996445618919097e-05\n",
      "==================================================\n",
      "Epoch 768\n",
      "train_loss : 0.1037269327789545 val_loss : 0.6160972118377686\n",
      "train_score : 4.963848550687544e-05 val_score : 3.018321694980841e-05\n",
      "==================================================\n",
      "Epoch 769\n",
      "train_loss : 0.10360445734113455 val_loss : 0.6171560883522034\n",
      "train_score : 4.969793008058332e-05 val_score : 2.993220005009789e-05\n",
      "==================================================\n",
      "Epoch 770\n",
      "train_loss : 0.103664705529809 val_loss : 0.6130273342132568\n",
      "train_score : 4.969910514773801e-05 val_score : 2.9557852030848153e-05\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 771\n",
      "train_loss : 0.10362460743635893 val_loss : 0.6147915720939636\n",
      "train_score : 4.952049130224623e-05 val_score : 3.010886939591728e-05\n",
      "==================================================\n",
      "Epoch 772\n",
      "train_loss : 0.10387304704636335 val_loss : 0.6112264394760132\n",
      "train_score : 4.9673304602038115e-05 val_score : 2.9906654162914492e-05\n",
      "==================================================\n",
      "Epoch 773\n",
      "train_loss : 0.1033213334158063 val_loss : 0.6199465990066528\n",
      "train_score : 4.969118526787497e-05 val_score : 3.002682024089154e-05\n",
      "==================================================\n",
      "Epoch 774\n",
      "train_loss : 0.10311684478074312 val_loss : 0.6133226156234741\n",
      "train_score : 4.978203651262447e-05 val_score : 2.992278132296633e-05\n",
      "==================================================\n",
      "Epoch 775\n",
      "train_loss : 0.10352589003741741 val_loss : 0.6242783665657043\n",
      "train_score : 4.9615002353675663e-05 val_score : 2.99838629871374e-05\n",
      "==================================================\n",
      "Epoch 776\n",
      "train_loss : 0.10322727356106043 val_loss : 0.6159789562225342\n",
      "train_score : 4.969300425727852e-05 val_score : 3.0183911803760566e-05\n",
      "==================================================\n",
      "Epoch 777\n",
      "train_loss : 0.10268877167254686 val_loss : 0.6125963926315308\n",
      "train_score : 4.974524927092716e-05 val_score : 3.0033092116354965e-05\n",
      "==================================================\n",
      "Epoch 778\n",
      "train_loss : 0.10238818638026714 val_loss : 0.61994868516922\n",
      "train_score : 4.975095362169668e-05 val_score : 3.005548751389142e-05\n",
      "==================================================\n",
      "Epoch 779\n",
      "train_loss : 0.10246425215154886 val_loss : 0.6190558075904846\n",
      "train_score : 4.9731221224647015e-05 val_score : 2.9835355235263705e-05\n",
      "==================================================\n",
      "Epoch 780\n",
      "train_loss : 0.10266068484634161 val_loss : 0.6167193651199341\n",
      "train_score : 4.9701262469170615e-05 val_score : 3.0251036150730215e-05\n",
      "==================================================\n",
      "Epoch 781\n",
      "train_loss : 0.10281766578555107 val_loss : 0.6251416802406311\n",
      "train_score : 4.977279604645446e-05 val_score : 2.97842070722254e-05\n",
      "==================================================\n",
      "Epoch 782\n",
      "train_loss : 0.10224845819175243 val_loss : 0.6245272755622864\n",
      "train_score : 4.984255792805925e-05 val_score : 3.0173108825692907e-05\n",
      "==================================================\n",
      "Epoch 783\n",
      "train_loss : 0.1022959491237998 val_loss : 0.6231563091278076\n",
      "train_score : 4.9660771765047684e-05 val_score : 3.0087054256000556e-05\n",
      "==================================================\n",
      "Epoch 784\n",
      "train_loss : 0.10148582700639963 val_loss : 0.6136351823806763\n",
      "train_score : 4.976598211214878e-05 val_score : 2.9828985134372488e-05\n",
      "==================================================\n",
      "Epoch 785\n",
      "train_loss : 0.10209278203547001 val_loss : 0.6099215745925903\n",
      "train_score : 4.9780181143432856e-05 val_score : 3.003542951773852e-05\n",
      "==================================================\n",
      "Epoch 786\n",
      "train_loss : 0.10203903261572123 val_loss : 0.6144759058952332\n",
      "train_score : 4.9775277148000896e-05 val_score : 3.0085078833508305e-05\n",
      "==================================================\n",
      "Epoch 787\n",
      "train_loss : 0.10173169244080782 val_loss : 0.6196781396865845\n",
      "train_score : 4.976775016984902e-05 val_score : 3.011203443747945e-05\n",
      "==================================================\n",
      "Epoch 788\n",
      "train_loss : 0.10178284998983145 val_loss : 0.6145347952842712\n",
      "train_score : 4.986930434824899e-05 val_score : 3.0114622859400697e-05\n",
      "==================================================\n",
      "Epoch 789\n",
      "train_loss : 0.10160850826650858 val_loss : 0.614912211894989\n",
      "train_score : 4.980264202458784e-05 val_score : 2.9981065381434746e-05\n",
      "==================================================\n",
      "Epoch 790\n",
      "train_loss : 0.10142057295888662 val_loss : 0.6137522459030151\n",
      "train_score : 4.9794518417911604e-05 val_score : 3.0098544812062755e-05\n",
      "==================================================\n",
      "Epoch 791\n",
      "train_loss : 0.10127026308327913 val_loss : 0.613511860370636\n",
      "train_score : 4.976871787221171e-05 val_score : 3.013828791154083e-05\n",
      "==================================================\n",
      "Epoch 792\n",
      "train_loss : 0.10102617740631104 val_loss : 0.6190358996391296\n",
      "train_score : 4.9866419431054965e-05 val_score : 2.9826058380422182e-05\n",
      "==================================================\n",
      "Epoch 793\n",
      "train_loss : 0.10135499760508537 val_loss : 0.6235219240188599\n",
      "train_score : 4.976853961125016e-05 val_score : 2.9908624128438532e-05\n",
      "==================================================\n",
      "Epoch 794\n",
      "train_loss : 0.10073388740420341 val_loss : 0.6219962239265442\n",
      "train_score : 4.9858124839374796e-05 val_score : 3.0027946195332333e-05\n",
      "==================================================\n",
      "Epoch 795\n",
      "train_loss : 0.10132619552314281 val_loss : 0.6143345832824707\n",
      "train_score : 4.9753951316233724e-05 val_score : 2.9997640012879856e-05\n",
      "==================================================\n",
      "Epoch 796\n",
      "train_loss : 0.10100159328430891 val_loss : 0.6227712035179138\n",
      "train_score : 4.979183358955197e-05 val_score : 2.94517376460135e-05\n",
      "==================================================\n",
      "Epoch 797\n",
      "train_loss : 0.10137638170272112 val_loss : 0.6228585839271545\n",
      "train_score : 4.9742466217139736e-05 val_score : 3.0042152502574027e-05\n",
      "==================================================\n",
      "Epoch 798\n",
      "train_loss : 0.10157089773565531 val_loss : 0.6246933341026306\n",
      "train_score : 4.984326005796902e-05 val_score : 2.9800738047924824e-05\n",
      "==================================================\n",
      "Epoch 799\n",
      "train_loss : 0.1003236910328269 val_loss : 0.6261168718338013\n",
      "train_score : 4.989754961570725e-05 val_score : 3.0019435143913142e-05\n",
      "==================================================\n",
      "Epoch 800\n",
      "train_loss : 0.10089786723256111 val_loss : 0.6312691569328308\n",
      "train_score : 4.983050166629255e-05 val_score : 2.985349783557467e-05\n",
      "==================================================\n",
      "Epoch 801\n",
      "train_loss : 0.10112973302602768 val_loss : 0.6182228326797485\n",
      "train_score : 4.988263390259817e-05 val_score : 3.0382840122911148e-05\n",
      "==================================================\n",
      "Epoch 802\n",
      "train_loss : 0.09974773041903973 val_loss : 0.6330795288085938\n",
      "train_score : 4.980736048310064e-05 val_score : 2.9994191208970733e-05\n",
      "==================================================\n",
      "Epoch 803\n",
      "train_loss : 0.0999661423265934 val_loss : 0.625584602355957\n",
      "train_score : 4.994887058273889e-05 val_score : 3.012597699125763e-05\n",
      "==================================================\n",
      "Epoch 804\n",
      "train_loss : 0.10002368781715631 val_loss : 0.6180490255355835\n",
      "train_score : 4.9909911467693746e-05 val_score : 3.0216620871215127e-05\n",
      "==================================================\n",
      "Epoch 805\n",
      "train_loss : 0.10032590851187706 val_loss : 0.6219280958175659\n",
      "train_score : 4.978362630936317e-05 val_score : 3.0016864911885932e-05\n",
      "==================================================\n",
      "Epoch 806\n",
      "train_loss : 0.09974806942045689 val_loss : 0.6326844096183777\n",
      "train_score : 5.000079545425251e-05 val_score : 2.9975333745824173e-05\n",
      "==================================================\n",
      "Epoch 807\n",
      "train_loss : 0.09967277199029922 val_loss : 0.62412428855896\n",
      "train_score : 4.988869113731198e-05 val_score : 3.0083750971243717e-05\n",
      "==================================================\n",
      "Epoch 808\n",
      "train_loss : 0.09972045477479696 val_loss : 0.6195536255836487\n",
      "train_score : 4.992435788153671e-05 val_score : 3.0024581064935774e-05\n",
      "==================================================\n",
      "Epoch 809\n",
      "train_loss : 0.09907623659819365 val_loss : 0.6175367832183838\n",
      "train_score : 4.988565342500806e-05 val_score : 2.991583096445538e-05\n",
      "==================================================\n",
      "Epoch 810\n",
      "train_loss : 0.09981539007276297 val_loss : 0.6253166794776917\n",
      "train_score : 4.98995250381995e-05 val_score : 2.9997065212228335e-05\n",
      "==================================================\n",
      "Epoch 811\n",
      "train_loss : 0.10003139730542898 val_loss : 0.622755765914917\n",
      "train_score : 4.9813130317488685e-05 val_score : 2.98057020700071e-05\n",
      "==================================================\n",
      "Epoch 812\n",
      "train_loss : 0.09950356464833021 val_loss : 0.6262896656990051\n",
      "train_score : 4.9920156016014516e-05 val_score : 3.0306273401947692e-05\n",
      "==================================================\n",
      "Epoch 813\n",
      "train_loss : 0.09941681008785963 val_loss : 0.6307373642921448\n",
      "train_score : 4.995433846488595e-05 val_score : 2.988125015690457e-05\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 814\n",
      "train_loss : 0.09969518799334764 val_loss : 0.6227108240127563\n",
      "train_score : 4.986270141671412e-05 val_score : 3.021377960976679e-05\n",
      "==================================================\n",
      "Epoch 815\n",
      "train_loss : 0.09917332697659731 val_loss : 0.6176145672798157\n",
      "train_score : 4.9886832130141556e-05 val_score : 2.9925999115221202e-05\n",
      "==================================================\n",
      "Epoch 816\n",
      "train_loss : 0.09772823099046946 val_loss : 0.6340278387069702\n",
      "train_score : 5.00053029099945e-05 val_score : 3.0092096494627185e-05\n",
      "==================================================\n",
      "Epoch 817\n",
      "train_loss : 0.0990633862093091 val_loss : 0.6202694773674011\n",
      "train_score : 4.988432556274347e-05 val_score : 2.972014590341132e-05\n",
      "==================================================\n",
      "Epoch 818\n",
      "train_loss : 0.09923294931650162 val_loss : 0.6180192232131958\n",
      "train_score : 5.0073656893800944e-05 val_score : 3.0105566111160442e-05\n",
      "==================================================\n",
      "Epoch 819\n",
      "train_loss : 0.09890714287757874 val_loss : 0.6290271282196045\n",
      "train_score : 4.9977599701378495e-05 val_score : 2.9852084480808116e-05\n",
      "==================================================\n",
      "Epoch 820\n",
      "train_loss : 0.09873512852936983 val_loss : 0.6371374130249023\n",
      "train_score : 4.987611100659706e-05 val_score : 3.0184030038071796e-05\n",
      "==================================================\n",
      "Epoch 821\n",
      "train_loss : 0.09863032680004835 val_loss : 0.6332015991210938\n",
      "train_score : 4.9896629207069054e-05 val_score : 3.007073792105075e-05\n",
      "==================================================\n",
      "Epoch 822\n",
      "train_loss : 0.09833818953484297 val_loss : 0.6352130174636841\n",
      "train_score : 5.0129634473705664e-05 val_score : 2.9864175303373486e-05\n",
      "==================================================\n",
      "Epoch 823\n",
      "train_loss : 0.09827212989330292 val_loss : 0.6281919479370117\n",
      "train_score : 4.993545371689834e-05 val_score : 2.982574369525537e-05\n",
      "==================================================\n",
      "Epoch 824\n",
      "train_loss : 0.09774096496403217 val_loss : 0.6265063285827637\n",
      "train_score : 5.005735147278756e-05 val_score : 2.9949376767035574e-05\n",
      "==================================================\n",
      "Epoch 825\n",
      "train_loss : 0.09769187122583389 val_loss : 0.6233616471290588\n",
      "train_score : 5.001811950933188e-05 val_score : 2.9822354917996563e-05\n",
      "==================================================\n",
      "Epoch 826\n",
      "train_loss : 0.09786668326705694 val_loss : 0.6305730938911438\n",
      "train_score : 5.0009835831588134e-05 val_score : 2.9961618565721437e-05\n",
      "==================================================\n",
      "Epoch 827\n",
      "train_loss : 0.09800819680094719 val_loss : 0.6259325742721558\n",
      "train_score : 5.0004942750092596e-05 val_score : 3.0129758670227602e-05\n",
      "==================================================\n",
      "Epoch 828\n",
      "train_loss : 0.0974260289222002 val_loss : 0.6253179907798767\n",
      "train_score : 5.011632674722932e-05 val_score : 3.023098543053493e-05\n",
      "==================================================\n",
      "Epoch 829\n",
      "train_loss : 0.09743435028940439 val_loss : 0.6341760158538818\n",
      "train_score : 5.014779890188947e-05 val_score : 2.9968166927574202e-05\n",
      "==================================================\n",
      "Epoch 830\n",
      "train_loss : 0.09684700425714254 val_loss : 0.6386191844940186\n",
      "train_score : 5.0036866014124826e-05 val_score : 3.0041079298825935e-05\n",
      "==================================================\n",
      "Epoch 831\n",
      "train_loss : 0.09751769993454218 val_loss : 0.642108142375946\n",
      "train_score : 5.004500417271629e-05 val_score : 3.010719046869781e-05\n",
      "==================================================\n",
      "Epoch 832\n",
      "train_loss : 0.0974089577794075 val_loss : 0.6341519951820374\n",
      "train_score : 5.003948899684474e-05 val_score : 3.0133613108773716e-05\n",
      "==================================================\n",
      "Epoch 833\n",
      "train_loss : 0.09673677757382393 val_loss : 0.6363829374313354\n",
      "train_score : 5.017441071686335e-05 val_score : 2.9802806238876656e-05\n",
      "==================================================\n",
      "Epoch 834\n",
      "train_loss : 0.09731261059641838 val_loss : 0.6382994651794434\n",
      "train_score : 5.00562455272302e-05 val_score : 2.997812225657981e-05\n",
      "==================================================\n",
      "Epoch 835\n",
      "train_loss : 0.09770928230136633 val_loss : 0.6358063220977783\n",
      "train_score : 5.006407081964426e-05 val_score : 2.9854278182028793e-05\n",
      "==================================================\n",
      "Epoch 836\n",
      "train_loss : 0.09664549212902784 val_loss : 0.6315724849700928\n",
      "train_score : 5.0160568207502365e-05 val_score : 2.9914903279859573e-05\n",
      "==================================================\n",
      "Epoch 837\n",
      "train_loss : 0.09737491142004728 val_loss : 0.6310078501701355\n",
      "train_score : 5.0048762204824015e-05 val_score : 2.997147930727806e-05\n",
      "==================================================\n",
      "Epoch 838\n",
      "train_loss : 0.09661989565938711 val_loss : 0.6402839422225952\n",
      "train_score : 5.014354974264279e-05 val_score : 2.9338729291339405e-05\n",
      "==================================================\n",
      "Epoch 839\n",
      "train_loss : 0.09687868691980839 val_loss : 0.6353104710578918\n",
      "train_score : 5.0093007303075865e-05 val_score : 2.9987539164721966e-05\n",
      "==================================================\n",
      "Epoch 840\n",
      "train_loss : 0.09619019832462072 val_loss : 0.6299020648002625\n",
      "train_score : 5.0108192226616666e-05 val_score : 3.000374090333935e-05\n",
      "==================================================\n",
      "Epoch 841\n",
      "train_loss : 0.09588578063994646 val_loss : 0.6392641663551331\n",
      "train_score : 5.0201448175357655e-05 val_score : 2.9846318284398876e-05\n",
      "==================================================\n",
      "Epoch 842\n",
      "train_loss : 0.09593145735561848 val_loss : 0.6298216581344604\n",
      "train_score : 5.015493661630899e-05 val_score : 2.977927215397358e-05\n",
      "==================================================\n",
      "Epoch 843\n",
      "train_loss : 0.09597557410597801 val_loss : 0.6259845495223999\n",
      "train_score : 5.0255897804163396e-05 val_score : 2.95891168207163e-05\n",
      "==================================================\n",
      "Epoch 844\n",
      "train_loss : 0.0963128237053752 val_loss : 0.6375732421875\n",
      "train_score : 5.0073544116457924e-05 val_score : 3.010929503943771e-05\n",
      "==================================================\n",
      "Epoch 845\n",
      "train_loss : 0.09651362895965576 val_loss : 0.6397479176521301\n",
      "train_score : 5.005838829674758e-05 val_score : 2.9818545954185538e-05\n",
      "==================================================\n",
      "Epoch 846\n",
      "train_loss : 0.09626216813921928 val_loss : 0.6370545625686646\n",
      "train_score : 5.019068339606747e-05 val_score : 3.0010767659405246e-05\n",
      "==================================================\n",
      "Epoch 847\n",
      "train_loss : 0.09578033909201622 val_loss : 0.6348147988319397\n",
      "train_score : 5.01235481351614e-05 val_score : 2.9955432182759978e-05\n",
      "==================================================\n",
      "Epoch 848\n",
      "train_loss : 0.0951568828895688 val_loss : 0.6413443684577942\n",
      "train_score : 5.021304605179466e-05 val_score : 2.9966189686092548e-05\n",
      "==================================================\n",
      "Epoch 849\n",
      "train_loss : 0.09488274995237589 val_loss : 0.6336703300476074\n",
      "train_score : 5.025168138672598e-05 val_score : 2.9933878977317363e-05\n",
      "==================================================\n",
      "Epoch 850\n",
      "train_loss : 0.0951773589476943 val_loss : 0.6342313289642334\n",
      "train_score : 5.0257225666427985e-05 val_score : 2.9928585718153045e-05\n",
      "==================================================\n",
      "Epoch 851\n",
      "train_loss : 0.09477136097848415 val_loss : 0.6403980255126953\n",
      "train_score : 5.024982965551317e-05 val_score : 2.972707625303883e-05\n",
      "==================================================\n",
      "Epoch 852\n",
      "train_loss : 0.0952187143266201 val_loss : 0.636573851108551\n",
      "train_score : 5.02850889461115e-05 val_score : 3.0043007427593693e-05\n",
      "==================================================\n",
      "Epoch 853\n",
      "train_loss : 0.09524387493729591 val_loss : 0.6360357403755188\n",
      "train_score : 5.017056537326425e-05 val_score : 2.9798517061863095e-05\n",
      "==================================================\n",
      "Epoch 854\n",
      "train_loss : 0.0942196398973465 val_loss : 0.6361719965934753\n",
      "train_score : 5.025573409511708e-05 val_score : 3.0047989639570005e-05\n",
      "==================================================\n",
      "Epoch 855\n",
      "train_loss : 0.09441074263304472 val_loss : 0.6413218379020691\n",
      "train_score : 5.0239650590810925e-05 val_score : 2.988027517858427e-05\n",
      "==================================================\n",
      "Epoch 856\n",
      "train_loss : 0.09465476498007774 val_loss : 0.6394606232643127\n",
      "train_score : 5.0192542403237894e-05 val_score : 2.9969880415592343e-05\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 857\n",
      "train_loss : 0.0946782911196351 val_loss : 0.6383956074714661\n",
      "train_score : 5.017539660912007e-05 val_score : 2.9995342629263178e-05\n",
      "==================================================\n",
      "Epoch 858\n",
      "train_loss : 0.09425504133105278 val_loss : 0.6400064826011658\n",
      "train_score : 5.028963278164156e-05 val_score : 2.9350752811296843e-05\n",
      "==================================================\n",
      "Epoch 859\n",
      "train_loss : 0.09508646186441183 val_loss : 0.6329591870307922\n",
      "train_score : 5.010129461879842e-05 val_score : 2.976729956571944e-05\n",
      "==================================================\n",
      "Epoch 860\n",
      "train_loss : 0.09573678765445948 val_loss : 0.6311632990837097\n",
      "train_score : 5.0144230044679716e-05 val_score : 2.9806536986143328e-05\n",
      "==================================================\n",
      "Epoch 861\n",
      "train_loss : 0.09481243509799242 val_loss : 0.6414690017700195\n",
      "train_score : 5.018326919525862e-05 val_score : 2.961976315418724e-05\n",
      "==================================================\n",
      "Epoch 862\n",
      "train_loss : 0.09467464033514261 val_loss : 0.6345992684364319\n",
      "train_score : 5.034092828282155e-05 val_score : 3.0171562684699893e-05\n",
      "==================================================\n",
      "Epoch 863\n",
      "train_loss : 0.09486541897058487 val_loss : 0.6385746598243713\n",
      "train_score : 5.030451211496256e-05 val_score : 2.9996750527061522e-05\n",
      "==================================================\n",
      "Epoch 864\n",
      "train_loss : 0.0943124433979392 val_loss : 0.6392263174057007\n",
      "train_score : 5.02823713759426e-05 val_score : 2.982311707455665e-05\n",
      "==================================================\n",
      "Epoch 865\n",
      "train_loss : 0.09386350214481354 val_loss : 0.6423362493515015\n",
      "train_score : 5.020689422963187e-05 val_score : 2.9931396056781523e-05\n",
      "==================================================\n",
      "Epoch 866\n",
      "train_loss : 0.092957379296422 val_loss : 0.6494711637496948\n",
      "train_score : 5.042245174990967e-05 val_score : 2.9971035473863594e-05\n",
      "==================================================\n",
      "Epoch 867\n",
      "train_loss : 0.09356295596808195 val_loss : 0.6440994143486023\n",
      "train_score : 5.035071080783382e-05 val_score : 2.9847593395970762e-05\n",
      "==================================================\n",
      "Epoch 868\n",
      "train_loss : 0.09343589842319489 val_loss : 0.6414405107498169\n",
      "train_score : 5.031343607697636e-05 val_score : 3.0062741643632762e-05\n",
      "==================================================\n",
      "Epoch 869\n",
      "train_loss : 0.0937165655195713 val_loss : 0.6409406661987305\n",
      "train_score : 5.029869498685002e-05 val_score : 2.990653047163505e-05\n",
      "==================================================\n",
      "Epoch 870\n",
      "train_loss : 0.09416658338159323 val_loss : 0.6518359184265137\n",
      "train_score : 5.02590810356196e-05 val_score : 2.9793111025355756e-05\n",
      "==================================================\n",
      "Epoch 871\n",
      "train_loss : 0.09338212572038174 val_loss : 0.6480722427368164\n",
      "train_score : 5.011837856727652e-05 val_score : 2.9922772228019312e-05\n",
      "==================================================\n",
      "Epoch 872\n",
      "train_loss : 0.0932439798489213 val_loss : 0.6436014771461487\n",
      "train_score : 5.032252011005767e-05 val_score : 2.9727636501775123e-05\n",
      "==================================================\n",
      "Epoch 873\n",
      "train_loss : 0.09341940470039845 val_loss : 0.6554860472679138\n",
      "train_score : 5.012232577428222e-05 val_score : 3.0090111977187917e-05\n",
      "==================================================\n",
      "Epoch 874\n",
      "train_loss : 0.09287928696721792 val_loss : 0.6489897966384888\n",
      "train_score : 5.0276437832508236e-05 val_score : 2.9838551199645735e-05\n",
      "==================================================\n",
      "Epoch 875\n",
      "train_loss : 0.09283701982349157 val_loss : 0.6477725505828857\n",
      "train_score : 5.0332553655607626e-05 val_score : 2.9981029001646675e-05\n",
      "==================================================\n",
      "Epoch 876\n",
      "train_loss : 0.09310975391417742 val_loss : 0.6423465013504028\n",
      "train_score : 5.033207344240509e-05 val_score : 2.9717022698605433e-05\n",
      "==================================================\n",
      "Epoch 877\n",
      "train_loss : 0.09193807002156973 val_loss : 0.6384747624397278\n",
      "train_score : 5.040801625000313e-05 val_score : 2.9995690056239255e-05\n",
      "==================================================\n",
      "Epoch 878\n",
      "train_loss : 0.09214210882782936 val_loss : 0.646167516708374\n",
      "train_score : 5.034220521338284e-05 val_score : 2.983709237014409e-05\n",
      "==================================================\n",
      "Epoch 879\n",
      "train_loss : 0.09276546817272902 val_loss : 0.6359673142433167\n",
      "train_score : 5.0345639465376735e-05 val_score : 2.9973301934660412e-05\n",
      "==================================================\n",
      "Epoch 880\n",
      "train_loss : 0.09283229149878025 val_loss : 0.6426079273223877\n",
      "train_score : 5.0395312428008765e-05 val_score : 2.953564398922026e-05\n",
      "==================================================\n",
      "Epoch 881\n",
      "train_loss : 0.09283619653433561 val_loss : 0.6489569544792175\n",
      "train_score : 5.036160655436106e-05 val_score : 2.9858894777134992e-05\n",
      "==================================================\n",
      "Epoch 882\n",
      "train_loss : 0.09351415000855923 val_loss : 0.6461158990859985\n",
      "train_score : 5.03571136505343e-05 val_score : 2.9970367904752493e-05\n",
      "==================================================\n",
      "Epoch 883\n",
      "train_loss : 0.09297470562160015 val_loss : 0.6407813429832458\n",
      "train_score : 5.0310238293604925e-05 val_score : 2.990365283039864e-05\n",
      "==================================================\n",
      "Epoch 884\n",
      "train_loss : 0.09193453751504421 val_loss : 0.6477981209754944\n",
      "train_score : 5.039917232352309e-05 val_score : 2.9853299565729685e-05\n",
      "==================================================\n",
      "Epoch 885\n",
      "train_loss : 0.09146347269415855 val_loss : 0.650892436504364\n",
      "train_score : 5.039730967837386e-05 val_score : 2.9759319659206085e-05\n",
      "==================================================\n",
      "Epoch 886\n",
      "train_loss : 0.09163442812860012 val_loss : 0.6427329182624817\n",
      "train_score : 5.043006603955291e-05 val_score : 3.0036053431103937e-05\n",
      "==================================================\n",
      "Epoch 887\n",
      "train_loss : 0.09134143218398094 val_loss : 0.64537513256073\n",
      "train_score : 5.041180702392012e-05 val_score : 2.9734746931353584e-05\n",
      "==================================================\n",
      "Epoch 888\n",
      "train_loss : 0.09161404334008694 val_loss : 0.6455280780792236\n",
      "train_score : 5.0444272346794605e-05 val_score : 2.986924664583057e-05\n",
      "==================================================\n",
      "Epoch 889\n",
      "train_loss : 0.09205147344619036 val_loss : 0.6443110108375549\n",
      "train_score : 5.0450253183953464e-05 val_score : 3.001547338499222e-05\n",
      "==================================================\n",
      "Epoch 890\n",
      "train_loss : 0.09175102319568396 val_loss : 0.6472291350364685\n",
      "train_score : 5.042612610850483e-05 val_score : 2.9839280614396557e-05\n",
      "==================================================\n",
      "Epoch 891\n",
      "train_loss : 0.09104334376752377 val_loss : 0.654748260974884\n",
      "train_score : 5.051533298683353e-05 val_score : 2.994363967445679e-05\n",
      "==================================================\n",
      "Epoch 892\n",
      "train_loss : 0.09130122885107994 val_loss : 0.6625461578369141\n",
      "train_score : 5.031536420574412e-05 val_score : 3.0018200050108135e-05\n",
      "==================================================\n",
      "Epoch 893\n",
      "train_loss : 0.09130292572081089 val_loss : 0.64572674036026\n",
      "train_score : 5.042366683483124e-05 val_score : 2.9902163078077137e-05\n",
      "==================================================\n",
      "Epoch 894\n",
      "train_loss : 0.09100548923015594 val_loss : 0.6531886458396912\n",
      "train_score : 5.053582935943268e-05 val_score : 3.0188393793650903e-05\n",
      "==================================================\n",
      "Epoch 895\n",
      "train_loss : 0.09137330949306488 val_loss : 0.6504951119422913\n",
      "train_score : 5.049717583460733e-05 val_score : 2.9741775506408885e-05\n",
      "==================================================\n",
      "Epoch 896\n",
      "train_loss : 0.09134655632078648 val_loss : 0.6478847861289978\n",
      "train_score : 5.041923941462301e-05 val_score : 3.0051640351302922e-05\n",
      "==================================================\n",
      "Epoch 897\n",
      "train_loss : 0.09037826675921679 val_loss : 0.6573008894920349\n",
      "train_score : 5.0492370064603165e-05 val_score : 3.003945130330976e-05\n",
      "==================================================\n",
      "Epoch 898\n",
      "train_loss : 0.09025050420314074 val_loss : 0.654150128364563\n",
      "train_score : 5.0475719035603106e-05 val_score : 3.011228545801714e-05\n",
      "==================================================\n",
      "Epoch 899\n",
      "train_loss : 0.09055509138852358 val_loss : 0.6519593000411987\n",
      "train_score : 5.052837877883576e-05 val_score : 2.969846173073165e-05\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 900\n",
      "train_loss : 0.09079531114548445 val_loss : 0.648155689239502\n",
      "train_score : 5.0388171075610444e-05 val_score : 2.9764229111606255e-05\n",
      "==================================================\n",
      "Epoch 901\n",
      "train_loss : 0.09018426015973091 val_loss : 0.6472364664077759\n",
      "train_score : 5.0511658628238365e-05 val_score : 2.970352943520993e-05\n",
      "==================================================\n",
      "Epoch 902\n",
      "train_loss : 0.09066851623356342 val_loss : 0.6425884962081909\n",
      "train_score : 5.043161945650354e-05 val_score : 2.973183654830791e-05\n",
      "==================================================\n",
      "Epoch 903\n",
      "train_loss : 0.09020247217267752 val_loss : 0.6485501527786255\n",
      "train_score : 5.042527845944278e-05 val_score : 3.000348442583345e-05\n",
      "==================================================\n",
      "Epoch 904\n",
      "train_loss : 0.09040556568652391 val_loss : 0.6571313142776489\n",
      "train_score : 5.0566985009936616e-05 val_score : 2.969500928884372e-05\n",
      "==================================================\n",
      "Epoch 905\n",
      "train_loss : 0.09047979488968849 val_loss : 0.655559778213501\n",
      "train_score : 5.04510935570579e-05 val_score : 3.009389365615789e-05\n",
      "==================================================\n",
      "Epoch 906\n",
      "train_loss : 0.09028396103531122 val_loss : 0.6546036601066589\n",
      "train_score : 5.048381717642769e-05 val_score : 3.0015506126801483e-05\n",
      "==================================================\n",
      "Epoch 907\n",
      "train_loss : 0.09063506405800581 val_loss : 0.6558086276054382\n",
      "train_score : 5.045848956797272e-05 val_score : 2.9976203222759068e-05\n",
      "==================================================\n",
      "Epoch 908\n",
      "train_loss : 0.0902306605130434 val_loss : 0.6415186524391174\n",
      "train_score : 5.044839053880423e-05 val_score : 2.989359745697584e-05\n",
      "==================================================\n",
      "Epoch 909\n",
      "train_loss : 0.09039494674652815 val_loss : 0.6481925249099731\n",
      "train_score : 5.042213888373226e-05 val_score : 2.9877970519009978e-05\n",
      "==================================================\n",
      "Epoch 910\n",
      "train_loss : 0.08942872658371925 val_loss : 0.6521561145782471\n",
      "train_score : 5.057561065768823e-05 val_score : 2.9525153877330013e-05\n",
      "==================================================\n",
      "Epoch 911\n",
      "train_loss : 0.08911838196218014 val_loss : 0.6545165181159973\n",
      "train_score : 5.055441943113692e-05 val_score : 2.9925358830951154e-05\n",
      "==================================================\n",
      "Epoch 912\n",
      "train_loss : 0.08918723929673433 val_loss : 0.6659092307090759\n",
      "train_score : 5.050570689490996e-05 val_score : 2.9932916731922887e-05\n",
      "==================================================\n",
      "Epoch 913\n",
      "train_loss : 0.08912425395101309 val_loss : 0.6602748036384583\n",
      "train_score : 5.0628794269869104e-05 val_score : 2.9957729566376656e-05\n",
      "==================================================\n",
      "Epoch 914\n",
      "train_loss : 0.0896193441003561 val_loss : 0.6561351418495178\n",
      "train_score : 5.052628694102168e-05 val_score : 2.986527579196263e-05\n",
      "==================================================\n",
      "Epoch 915\n",
      "train_loss : 0.08871458750218153 val_loss : 0.6733554601669312\n",
      "train_score : 5.049222818342969e-05 val_score : 2.9754057322861627e-05\n",
      "==================================================\n",
      "Epoch 916\n",
      "train_loss : 0.08934557903558016 val_loss : 0.659945011138916\n",
      "train_score : 5.0449900300009176e-05 val_score : 2.9660779546247795e-05\n",
      "==================================================\n",
      "Epoch 917\n",
      "train_loss : 0.08878592494875193 val_loss : 0.6500125527381897\n",
      "train_score : 5.064703509560786e-05 val_score : 3.0051995054236613e-05\n",
      "==================================================\n",
      "Epoch 918\n",
      "train_loss : 0.089497203938663 val_loss : 0.6565784811973572\n",
      "train_score : 5.053755012340844e-05 val_score : 2.9832062864443287e-05\n",
      "==================================================\n",
      "Epoch 919\n",
      "train_loss : 0.08897325396537781 val_loss : 0.6520900130271912\n",
      "train_score : 5.0605176511453465e-05 val_score : 2.9978185921208933e-05\n",
      "==================================================\n",
      "Epoch 920\n",
      "train_loss : 0.08939473424106836 val_loss : 0.6553370952606201\n",
      "train_score : 5.058712122263387e-05 val_score : 3.0058259653742425e-05\n",
      "==================================================\n",
      "Epoch 921\n",
      "train_loss : 0.08807798754423857 val_loss : 0.6649699211120605\n",
      "train_score : 5.066591620561667e-05 val_score : 2.9611746867885813e-05\n",
      "==================================================\n",
      "Epoch 922\n",
      "train_loss : 0.08876145631074905 val_loss : 0.6603552103042603\n",
      "train_score : 5.066471567261033e-05 val_score : 2.970495552290231e-05\n",
      "==================================================\n",
      "Epoch 923\n",
      "train_loss : 0.08785385172814131 val_loss : 0.6578855514526367\n",
      "train_score : 5.063521894044243e-05 val_score : 2.9934455596958287e-05\n",
      "==================================================\n",
      "Epoch 924\n",
      "train_loss : 0.08810621034353971 val_loss : 0.6708165407180786\n",
      "train_score : 5.065705408924259e-05 val_score : 2.9813896617270075e-05\n",
      "==================================================\n",
      "Epoch 925\n",
      "train_loss : 0.0886731157079339 val_loss : 0.6655454039573669\n",
      "train_score : 5.060391777078621e-05 val_score : 2.992540248669684e-05\n",
      "==================================================\n",
      "Epoch 926\n",
      "train_loss : 0.08882076665759087 val_loss : 0.6572923064231873\n",
      "train_score : 5.062601849203929e-05 val_score : 2.9814384106430225e-05\n",
      "==================================================\n",
      "Epoch 927\n",
      "train_loss : 0.08849922753870487 val_loss : 0.6553438305854797\n",
      "train_score : 5.0576229114085436e-05 val_score : 2.998580202984158e-05\n",
      "==================================================\n",
      "Epoch 928\n",
      "train_loss : 0.08891279436647892 val_loss : 0.6548858284950256\n",
      "train_score : 5.0651698984438553e-05 val_score : 2.9638675187015906e-05\n",
      "==================================================\n",
      "Epoch 929\n",
      "train_loss : 0.08887602388858795 val_loss : 0.659329354763031\n",
      "train_score : 5.054719440522604e-05 val_score : 2.997541923832614e-05\n",
      "==================================================\n",
      "Epoch 930\n",
      "train_loss : 0.08716483321040869 val_loss : 0.6638212203979492\n",
      "train_score : 5.0711001676972955e-05 val_score : 3.0033210350666195e-05\n",
      "==================================================\n",
      "Epoch 931\n",
      "train_loss : 0.08802443649619818 val_loss : 0.6678637266159058\n",
      "train_score : 5.0571939937071875e-05 val_score : 2.9995731892995536e-05\n",
      "==================================================\n",
      "Epoch 932\n",
      "train_loss : 0.0883252527564764 val_loss : 0.6580804586410522\n",
      "train_score : 5.0332779210293666e-05 val_score : 3.0246763344621286e-05\n",
      "==================================================\n",
      "Epoch 933\n",
      "train_loss : 0.08841169346123934 val_loss : 0.6668619513511658\n",
      "train_score : 5.064803917775862e-05 val_score : 2.9404396627796814e-05\n",
      "==================================================\n",
      "Epoch 934\n",
      "train_loss : 0.0883979108184576 val_loss : 0.660615861415863\n",
      "train_score : 5.044598947279155e-05 val_score : 2.994320311699994e-05\n",
      "==================================================\n",
      "Epoch 935\n",
      "train_loss : 0.08742301166057587 val_loss : 0.6653303503990173\n",
      "train_score : 5.062198397354223e-05 val_score : 2.9737861041212454e-05\n",
      "==================================================\n",
      "Epoch 936\n",
      "train_loss : 0.08727135602384806 val_loss : 0.6676543951034546\n",
      "train_score : 5.072593194199726e-05 val_score : 3.0165139833115973e-05\n",
      "==================================================\n",
      "Epoch 937\n",
      "train_loss : 0.08771514426916838 val_loss : 0.6630670428276062\n",
      "train_score : 5.063698336016387e-05 val_score : 3.0034938390599564e-05\n",
      "==================================================\n",
      "Epoch 938\n",
      "train_loss : 0.08643526863306761 val_loss : 0.6611872315406799\n",
      "train_score : 5.06120159116108e-05 val_score : 2.9885113690397702e-05\n",
      "==================================================\n",
      "Epoch 939\n",
      "train_loss : 0.08694789092987776 val_loss : 0.6675753593444824\n",
      "train_score : 5.0725437176879495e-05 val_score : 2.9687233109143563e-05\n",
      "==================================================\n",
      "Epoch 940\n",
      "train_loss : 0.08669241704046726 val_loss : 0.6609665155410767\n",
      "train_score : 5.071054692962207e-05 val_score : 2.992870577145368e-05\n",
      "==================================================\n",
      "Epoch 941\n",
      "train_loss : 0.08674944378435612 val_loss : 0.66154545545578\n",
      "train_score : 5.0784143240889534e-05 val_score : 2.967742329929024e-05\n",
      "==================================================\n",
      "Epoch 942\n",
      "train_loss : 0.0876278504729271 val_loss : 0.66609126329422\n",
      "train_score : 5.06902470078785e-05 val_score : 2.9687855203519575e-05\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 943\n",
      "train_loss : 0.0871814051643014 val_loss : 0.6686666011810303\n",
      "train_score : 5.073971988167614e-05 val_score : 2.9996625016792677e-05\n",
      "==================================================\n",
      "Epoch 944\n",
      "train_loss : 0.08718510251492262 val_loss : 0.6665049195289612\n",
      "train_score : 5.0627266318770126e-05 val_score : 2.979557029902935e-05\n",
      "==================================================\n",
      "Epoch 945\n",
      "train_loss : 0.08589908201247454 val_loss : 0.6665511131286621\n",
      "train_score : 5.074524960946292e-05 val_score : 2.9862147130188532e-05\n",
      "==================================================\n",
      "Epoch 946\n",
      "train_loss : 0.08648093417286873 val_loss : 0.6748400330543518\n",
      "train_score : 5.077337482362054e-05 val_score : 2.9758191885775886e-05\n",
      "==================================================\n",
      "Epoch 947\n",
      "train_loss : 0.08682616800069809 val_loss : 0.6645804643630981\n",
      "train_score : 5.061670526629314e-05 val_score : 2.9943030313006602e-05\n",
      "==================================================\n",
      "Epoch 948\n",
      "train_loss : 0.08647323027253151 val_loss : 0.6707425117492676\n",
      "train_score : 5.072670319350436e-05 val_score : 2.9686840207432397e-05\n",
      "==================================================\n",
      "Epoch 949\n",
      "train_loss : 0.08613854087889194 val_loss : 0.663973867893219\n",
      "train_score : 5.075232547824271e-05 val_score : 3.0142733521643095e-05\n",
      "==================================================\n",
      "Epoch 950\n",
      "train_loss : 0.08647398743778467 val_loss : 0.6626170873641968\n",
      "train_score : 5.076491288491525e-05 val_score : 2.9727036235271953e-05\n",
      "==================================================\n",
      "Epoch 951\n",
      "train_loss : 0.0860114824026823 val_loss : 0.6705691814422607\n",
      "train_score : 5.0833041314035654e-05 val_score : 2.9923015972599387e-05\n",
      "==================================================\n",
      "Epoch 952\n",
      "train_loss : 0.08572910074144602 val_loss : 0.6668501496315002\n",
      "train_score : 5.075881563243456e-05 val_score : 2.970608875330072e-05\n",
      "==================================================\n",
      "Epoch 953\n",
      "train_loss : 0.08630672935396433 val_loss : 0.6653062701225281\n",
      "train_score : 5.0660590204643086e-05 val_score : 2.9984219509060495e-05\n",
      "==================================================\n",
      "Epoch 954\n",
      "train_loss : 0.08564511872828007 val_loss : 0.6743596196174622\n",
      "train_score : 5.0722246669465676e-05 val_score : 2.977186522912234e-05\n",
      "==================================================\n",
      "Epoch 955\n",
      "train_loss : 0.08609972521662712 val_loss : 0.6713118553161621\n",
      "train_score : 5.082034840597771e-05 val_score : 2.986610161315184e-05\n",
      "==================================================\n",
      "Epoch 956\n",
      "train_loss : 0.0853679683059454 val_loss : 0.6820775866508484\n",
      "train_score : 5.079659604234621e-05 val_score : 2.9733475457760505e-05\n",
      "==================================================\n",
      "Epoch 957\n",
      "train_loss : 0.08565448131412268 val_loss : 0.6718677878379822\n",
      "train_score : 5.080401024315506e-05 val_score : 2.9903736503911205e-05\n",
      "==================================================\n",
      "Epoch 958\n",
      "train_loss : 0.08531926479190588 val_loss : 0.6691339612007141\n",
      "train_score : 5.071487612440251e-05 val_score : 2.9788350730086677e-05\n",
      "==================================================\n",
      "Epoch 959\n",
      "train_loss : 0.0859944662079215 val_loss : 0.6641747951507568\n",
      "train_score : 5.075569060863927e-05 val_score : 2.9732438633800484e-05\n",
      "==================================================\n",
      "Epoch 960\n",
      "train_loss : 0.08562912978231907 val_loss : 0.6698394417762756\n",
      "train_score : 5.0792823458323255e-05 val_score : 2.9256014386191964e-05\n",
      "==================================================\n",
      "Epoch 961\n",
      "train_loss : 0.08589200396090746 val_loss : 0.6671322584152222\n",
      "train_score : 5.066461380920373e-05 val_score : 2.9811100830556825e-05\n",
      "==================================================\n",
      "Epoch 962\n",
      "train_loss : 0.08573349472135305 val_loss : 0.6718199849128723\n",
      "train_score : 5.0856444431701675e-05 val_score : 3.0036508178454824e-05\n",
      "==================================================\n",
      "Epoch 963\n",
      "train_loss : 0.08516960497945547 val_loss : 0.6763076186180115\n",
      "train_score : 5.077360037830658e-05 val_score : 2.964120176329743e-05\n",
      "==================================================\n",
      "Epoch 964\n",
      "train_loss : 0.08465828560292721 val_loss : 0.6657848358154297\n",
      "train_score : 5.0905728130601346e-05 val_score : 3.0039929697522894e-05\n",
      "==================================================\n",
      "Epoch 965\n",
      "train_loss : 0.08529726881533861 val_loss : 0.6706461906433105\n",
      "train_score : 5.083431096863933e-05 val_score : 2.9899165383540094e-05\n",
      "==================================================\n",
      "Epoch 966\n",
      "train_loss : 0.0854225791990757 val_loss : 0.6762795448303223\n",
      "train_score : 5.070750194136053e-05 val_score : 2.960606616397854e-05\n",
      "==================================================\n",
      "Epoch 967\n",
      "train_loss : 0.08527101576328278 val_loss : 0.675764262676239\n",
      "train_score : 5.085176235297695e-05 val_score : 2.982133810291998e-05\n",
      "==================================================\n",
      "Epoch 968\n",
      "train_loss : 0.08488207869231701 val_loss : 0.6848926544189453\n",
      "train_score : 5.0793561968021095e-05 val_score : 2.9687345886486582e-05\n",
      "==================================================\n",
      "Epoch 969\n",
      "train_loss : 0.08456646744161844 val_loss : 0.6668760180473328\n",
      "train_score : 5.090491322334856e-05 val_score : 2.9815455491188914e-05\n",
      "==================================================\n",
      "Epoch 970\n",
      "train_loss : 0.08453414030373096 val_loss : 0.6760501265525818\n",
      "train_score : 5.079421680420637e-05 val_score : 2.986104300362058e-05\n",
      "==================================================\n",
      "Epoch 971\n",
      "train_loss : 0.08434208482503891 val_loss : 0.6797745823860168\n",
      "train_score : 5.077163950772956e-05 val_score : 2.973089794977568e-05\n",
      "==================================================\n",
      "Epoch 972\n",
      "train_loss : 0.08378694765269756 val_loss : 0.6867014765739441\n",
      "train_score : 5.0872404244728386e-05 val_score : 2.9733731935266405e-05\n",
      "==================================================\n",
      "Epoch 973\n",
      "train_loss : 0.08486145827919245 val_loss : 0.6731473207473755\n",
      "train_score : 5.0747152272379026e-05 val_score : 2.9949889722047374e-05\n",
      "==================================================\n",
      "Epoch 974\n",
      "train_loss : 0.08382427133619785 val_loss : 0.6738945841789246\n",
      "train_score : 5.0945734983542934e-05 val_score : 2.9769933462375775e-05\n",
      "==================================================\n",
      "Epoch 975\n",
      "train_loss : 0.08431893959641457 val_loss : 0.6794288158416748\n",
      "train_score : 5.0912658480228856e-05 val_score : 2.9623895898112096e-05\n",
      "==================================================\n",
      "Epoch 976\n",
      "train_loss : 0.08393768966197968 val_loss : 0.6770724058151245\n",
      "train_score : 5.094350126455538e-05 val_score : 2.9799439289490692e-05\n",
      "==================================================\n",
      "Epoch 977\n",
      "train_loss : 0.08396730478852987 val_loss : 0.6764772534370422\n",
      "train_score : 5.085089651402086e-05 val_score : 2.9898043067078106e-05\n",
      "==================================================\n",
      "Epoch 978\n",
      "train_loss : 0.08399251289665699 val_loss : 0.6783828139305115\n",
      "train_score : 5.083774885861203e-05 val_score : 2.9935092243249528e-05\n",
      "==================================================\n",
      "Epoch 979\n",
      "train_loss : 0.0830612201243639 val_loss : 0.6807698607444763\n",
      "train_score : 5.089346814202145e-05 val_score : 2.9730501410085708e-05\n",
      "==================================================\n",
      "Epoch 980\n",
      "train_loss : 0.08356938324868679 val_loss : 0.6716146469116211\n",
      "train_score : 5.086634701001458e-05 val_score : 2.989618769788649e-05\n",
      "==================================================\n",
      "Epoch 981\n",
      "train_loss : 0.08379031904041767 val_loss : 0.6830825209617615\n",
      "train_score : 5.087414683657698e-05 val_score : 2.9498167350539006e-05\n",
      "==================================================\n",
      "Epoch 982\n",
      "train_loss : 0.0838006567209959 val_loss : 0.6678743362426758\n",
      "train_score : 5.0869555707322434e-05 val_score : 2.9828654078301042e-05\n",
      "==================================================\n",
      "Epoch 983\n",
      "train_loss : 0.08366145566105843 val_loss : 0.6656160354614258\n",
      "train_score : 5.089574187877588e-05 val_score : 2.9758810342173092e-05\n",
      "==================================================\n",
      "Epoch 984\n",
      "train_loss : 0.08336745761334896 val_loss : 0.675295889377594\n",
      "train_score : 5.0907106924569234e-05 val_score : 2.9796752642141655e-05\n",
      "==================================================\n",
      "Epoch 985\n",
      "train_loss : 0.08376460522413254 val_loss : 0.6725757122039795\n",
      "train_score : 5.0893097068183124e-05 val_score : 2.9878476198064163e-05\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 986\n",
      "train_loss : 0.08284289110451937 val_loss : 0.6798085570335388\n",
      "train_score : 5.0960908993147314e-05 val_score : 2.994048008986283e-05\n",
      "==================================================\n",
      "Epoch 987\n",
      "train_loss : 0.08306619338691235 val_loss : 0.681439995765686\n",
      "train_score : 5.092439823783934e-05 val_score : 2.9831009669578634e-05\n",
      "==================================================\n",
      "Epoch 988\n",
      "train_loss : 0.08366607129573822 val_loss : 0.6722432971000671\n",
      "train_score : 5.085677548777312e-05 val_score : 2.9766229999950156e-05\n",
      "==================================================\n",
      "Epoch 989\n",
      "train_loss : 0.0834781788289547 val_loss : 0.6680362224578857\n",
      "train_score : 5.091173807159066e-05 val_score : 2.965648855024483e-05\n",
      "==================================================\n",
      "Epoch 990\n",
      "train_loss : 0.08298145607113838 val_loss : 0.6755893230438232\n",
      "train_score : 5.0956103223143145e-05 val_score : 2.9677994461962953e-05\n",
      "==================================================\n",
      "Epoch 991\n",
      "train_loss : 0.08298141695559025 val_loss : 0.6749622821807861\n",
      "train_score : 5.0957092753378674e-05 val_score : 2.9663460736628622e-05\n",
      "==================================================\n",
      "Epoch 992\n",
      "train_loss : 0.08247312344610691 val_loss : 0.6813260912895203\n",
      "train_score : 5.0928483688039705e-05 val_score : 2.997259252879303e-05\n",
      "==================================================\n",
      "Epoch 993\n",
      "train_loss : 0.08308972790837288 val_loss : 0.6817742586135864\n",
      "train_score : 5.095740925753489e-05 val_score : 2.9724806154263206e-05\n",
      "==================================================\n",
      "Epoch 994\n",
      "train_loss : 0.08255586214363575 val_loss : 0.6850857734680176\n",
      "train_score : 5.089274054626003e-05 val_score : 2.9801152777508833e-05\n",
      "==================================================\n",
      "Epoch 995\n",
      "train_loss : 0.08256741240620613 val_loss : 0.6879492998123169\n",
      "train_score : 5.099016198073514e-05 val_score : 2.9329799872357398e-05\n",
      "==================================================\n",
      "Epoch 996\n",
      "train_loss : 0.08293598238378763 val_loss : 0.6801461577415466\n",
      "train_score : 5.084248914499767e-05 val_score : 2.9442457162076607e-05\n",
      "==================================================\n",
      "Epoch 997\n",
      "train_loss : 0.08245751075446606 val_loss : 0.6897023320198059\n",
      "train_score : 5.097771645523608e-05 val_score : 2.9761909900116734e-05\n",
      "==================================================\n",
      "Epoch 998\n",
      "train_loss : 0.08257734309881926 val_loss : 0.6864442825317383\n",
      "train_score : 5.0929247663589194e-05 val_score : 2.9914233891759068e-05\n",
      "==================================================\n",
      "Epoch 999\n",
      "train_loss : 0.08189118281006813 val_loss : 0.6827307939529419\n",
      "train_score : 5.099899135529995e-05 val_score : 2.9958491722936742e-05\n",
      "==================================================\n",
      "Epoch 1000\n",
      "train_loss : 0.08182276505976915 val_loss : 0.6879967451095581\n",
      "train_score : 5.1016922952840105e-05 val_score : 2.981133366120048e-05\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "epochs = 1000\n",
    "valid_loss_min = np.Inf\n",
    "# train for some number of epochs\n",
    "epoch_tr_loss,epoch_vl_loss = [],[]\n",
    "epoch_tr_acc,epoch_vl_acc = [],[]\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_losses = []\n",
    "    train_acc = 0.0\n",
    "    model.train()\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "\n",
    "        inputs, labels = inputs.to(device), labels.to(device)   \n",
    "\n",
    "        model.zero_grad()\n",
    "        output = model(inputs.float())\n",
    "        #print(output.shape, labels.shape)\n",
    "        # calculate the loss and perform backprop\n",
    "        loss = criterion(output.squeeze(), labels.long())\n",
    "        loss.backward()\n",
    "        train_losses.append(loss.item())\n",
    "        # calculating mcc\n",
    "        accuracy = mcc(output,labels)\n",
    "        train_acc += accuracy\n",
    "        \n",
    "        optimizer.step()\n",
    " \n",
    "    \n",
    "    val_losses = []\n",
    "    val_acc = 0.0\n",
    "    model.eval()\n",
    "    for inputs, labels in valid_loader:\n",
    "\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            output = model(inputs.float())\n",
    "            val_loss = criterion(output.squeeze(), labels.long())\n",
    "\n",
    "            val_losses.append(val_loss.item())\n",
    "            \n",
    "            accuracy = mcc(output,labels)\n",
    "            val_acc += accuracy\n",
    "            \n",
    "    epoch_train_loss = np.mean(train_losses)\n",
    "    epoch_val_loss = np.mean(val_losses)\n",
    "    epoch_train_acc = train_acc/len(train_loader.dataset)\n",
    "    epoch_val_acc = val_acc/len(valid_loader.dataset)\n",
    "    epoch_tr_loss.append(epoch_train_loss)\n",
    "    epoch_vl_loss.append(epoch_val_loss)\n",
    "    epoch_tr_acc.append(epoch_train_acc)\n",
    "    epoch_vl_acc.append(epoch_val_acc)\n",
    "    print(f'Epoch {epoch+1}') \n",
    "    print(f'train_loss : {epoch_train_loss} val_loss : {epoch_val_loss}')\n",
    "    print(f'train_score : {epoch_train_acc} val_score : {epoch_val_acc}')\n",
    "    if epoch_val_loss <= valid_loss_min:\n",
    "        torch.save(model.state_dict(), '../../../state_dict.pt')\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,epoch_val_loss))\n",
    "        valid_loss_min = epoch_val_loss\n",
    "    print(25*'==')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize = (20, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epoch_tr_acc, label='Train Macro F1')\n",
    "plt.plot(epoch_vl_acc, label='Val Macro F1')\n",
    "plt.title(\"Macro F1\")\n",
    "plt.ylim([0,1])\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "    \n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epoch_tr_loss, label='Train loss')\n",
    "plt.plot(epoch_vl_loss, label='Validation loss')\n",
    "plt.title(\"Loss\")\n",
    "plt.ylim([0,1])\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "plt.savefig('../../../loss_f1score.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('../../../state_dict.pt'))\n",
    "test_loss, test_score = evaluate(model, valid_iterator, criterion)\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Score: {test_acc:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
